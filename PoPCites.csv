Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
121,"Luke Munn","The uselessness of AI ethics",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00209-w","",43,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00209-w","2730-5953","",3,3,869,877,121,40.33,121,1,3,"Abstract: As the awareness of AI’s power and danger has risen, the dominant response has been a turn to ethical principles. A flood of AI guidelines and codes of ethics have been released in both the public and private sector in the last several years. However, these are","https://link.springer.com/content/pdf/10.1007/s43681-022-00209-w.pdf",""
80,"Arslan Munir, Alexander Aved, Erik Blasch","Situational Awareness: Techniques, Challenges, and Prospects",2022,"AI","MDPI AG","https://doi.org/10.3390/ai3010005","",866,"2025-02-04 16:55:17","journal-article","10.3390/ai3010005","2673-2688","",3,1,55,77,80,26.67,27,3,3,"Situational awareness (SA) is defined as the perception of entities in the environment, comprehension of their meaning, and projection of their status in near future. From an Air Force perspective, SA refers to the capability to comprehend and project the current and future disposition of red and blue aircraft and surface threats within an airspace. In this article, we propose a model for SA and dynamic decision-making that incorporates artificial intelligence and dynamic data-driven application systems to adapt measurements and resources in accordance with changing situations. We discuss measurement of SA and the challenges associated with quantification of SA. We then elaborate a plethora of techniques and technologies that help improve SA ranging from different modes of intelligence gathering to artificial intelligence to automated vision systems. We then present different application domains of SA including battlefield, gray zone warfare, military- and air-base, homeland security and defense, and critical infrastructure. Finally, we conclude the article with insights into the future of SA.","https://www.mdpi.com/2673-2688/3/1/5/pdf",""
73,"Lorenzo Belenguer","AI bias: exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00138-8","",538,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00138-8","2730-5953","",2,4,771,787,73,24.33,73,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00138-8.pdf",""
70,"Golnar Karimian, Elena Petelos, Silvia M. A. A. Evers","The ethical issues of the application of artificial intelligence in healthcare: a systematic scoping review",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-021-00131-7","",864,"2025-02-04 16:55:17","journal-article","10.1007/s43681-021-00131-7","2730-5953","",2,4,539,551,70,23.33,23,3,3,"Abstract: Artificial intelligence (AI) is being increasingly applied in healthcare. The expansion of AI in healthcare necessitates AI-related ethical issues to be studied and addressed. This systematic scoping review was conducted to identify the ethical issues of AI application in healthcare, to highlight gaps, and to propose steps to move towards an evidence-informed approach for addressing them. A systematic search was conducted to retrieve all articles examining the ethical aspects of AI application in healthcare from Medline (PubMed) and Embase (OVID), published between 2010 and July 21, 2020. The search terms were “artificial intelligence” or “machine learning” or “deep learning” in combination with “ethics” or “bioethics”. The studies were selected utilizing a PRISMA flowchart and predefined inclusion criteria. Ethical principles of respect for human autonomy, prevention of harm, fairness, explicability, and privacy were charted. The search yielded 2166 articles, of which 18 articles were selected for data charting on the basis of the predefined inclusion criteria. The focus of many articles was a general discussion about ethics and AI. Nevertheless, there was limited examination of ethical principles in terms of consideration for design or deployment of AI in most retrieved studies. In the few instances where ethical principles were considered, fairness, preservation of human autonomy, explicability and privacy were equally discussed. The principle of prevention of harm was the least explored topic. Practical tools for testing and upholding ethical requirements across the lifecycle of AI-based technologies are largely absent from the body of reported evidence. In addition, the perspective of different stakeholders is largely missing.","https://link.springer.com/content/pdf/10.1007/s43681-021-00131-7.pdf",""
63,"Stuart McLennan, Amelia Fiske, Daniel Tigard, Ruth Müller, Sami Haddadin, Alena Buyx","Embedded ethics: a proposal for integrating ethics into the development of medical AI",2022,"BMC Medical Ethics","Springer Science and Business Media LLC","https://doi.org/10.1186/s12910-022-00746-3","",917,"2025-02-04 16:55:17","journal-article","10.1186/s12910-022-00746-3","1472-6939","",23,1,,,63,21.00,11,6,3,"Abstract: The emergence of ethical concerns surrounding artificial intelligence (AI) has led to an explosion of high-level ethical principles being published by a wide range of public and private organizations. However, there is a need to consider how AI developers can be practically assisted to anticipate, identify and address ethical issues regarding AI technologies. This is particularly important in the development of AI intended for healthcare settings, where applications will often interact directly with patients in various states of vulnerability. In this paper, we propose that an ‘embedded ethics’ approach, in which ethicists and developers together address ethical issues via an iterative and continuous process from the outset of development, could be an effective means of integrating robust ethical considerations into the practical development of medical AI.","https://link.springer.com/content/pdf/10.1186/s12910-022-00746-3.pdf",""
58,"Matti Mäntymäki, Matti Minkkinen, Teemu Birkstedt, Mika Viljanen","Defining organizational AI governance",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00143-x","",215,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00143-x","2730-5953","",2,4,603,609,58,19.33,15,4,3,"Abstract: Artificial intelligence (AI) governance is required to reap the benefits and manage the risks brought by AI systems. This means that ethical principles, such as fairness, need to be translated into practicable AI governance processes. A concise AI governance definition would allow researchers and practitioners to identify the constituent parts of the complex problem of translating AI ethics into practice. However, there have been few efforts to define AI governance thus far. To bridge this gap, this paper defines AI governance at the organizational level. Moreover, we delineate how AI governance enters into a governance landscape with numerous governance areas, such as corporate governance, information technology (IT) governance, and data governance. Therefore, we position AI governance as part of an organization’s governance structure in relation to these existing governance areas. Our definition and positioning of organizational AI governance paves the way for crafting AI governance frameworks and offers a stepping stone on the pathway toward governed AI.","https://link.springer.com/content/pdf/10.1007/s43681-022-00143-x.pdf",""
57,"Anastasia Chan","GPT-3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00148-6","",145,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00148-6","2730-5953","",3,1,53,64,57,19.00,57,1,3,"Abstract: This paper examines the ethical solutions raised in response to OpenAI’s language model Generative Pre-trained Transformer-3 (GPT-3) a year and a half from its release. I argue that hype and fear about GPT-3, even within the Natural Language Processing (NLP) industry and AI ethics, have often been underpinned by technologically deterministic perspectives. These perspectives emphasise the autonomy of the language model rather than the autonomy of human actors in AI systems. I highlight the existence of deterministic perspectives in the current AI discourse (which range from technological utopianism to dystopianism), with a specific focus on the two issues of: (1) GPT-3’s potential intentional misuse for manipulation and (2) unintentional harm caused by bias. In response, I find that a contextual approach to GPT-3, which is centred upon wider ecologies of societal harm and benefit, human autonomy, and human values, illuminates practical solutions to concerns about manipulation and bias. Additionally, although OpenAI’s newest 2022 language model InstructGPT represents a small step in reducing toxic language and aligning GPT-3 with user intent, it does not provide any compelling solutions to manipulation or bias. Therefore, I argue that solutions to address these issues must focus on organisational settings as a precondition for ethical decision-making in AI, and high-quality curated datasets as a precondition for less harmful language model outputs.","https://link.springer.com/content/pdf/10.1007/s43681-022-00148-6.pdf",""
53,"Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, Luciano Floridi","Auditing large language models: a three-layered approach",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00289-2","",627,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00289-2","2730-5953","",4,4,1085,1115,53,26.50,13,4,2,"Abstract: Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.","https://link.springer.com/content/pdf/10.1007/s43681-023-00289-2.pdf",""
48,"Fan Li, Nick Ruijs, Yuan Lu","Ethics &amp; AI: A Systematic Review on Ethical Concerns and Related Strategies for Designing with AI in Healthcare",2022,"AI","MDPI AG","https://doi.org/10.3390/ai4010003","",695,"2025-02-04 16:55:17","journal-article","10.3390/ai4010003","2673-2688","",4,1,28,53,48,16.00,16,3,3,"In modern life, the application of artificial intelligence (AI) has promoted the implementation of data-driven algorithms in high-stakes domains, such as healthcare. However, it is becoming increasingly challenging for humans to understand the working and reasoning of these complex and opaque algorithms. For AI to support essential decisions in these domains, specific ethical issues need to be addressed to prevent the misinterpretation of AI, which may have severe consequences for humans. However, little research has been published on guidelines that systematically addresses ethical issues when AI techniques are applied in healthcare. In this systematic literature review, we aimed to provide an overview of ethical concerns and related strategies that are currently identified when applying AI in healthcare. The review, which followed the PRISMA guidelines, revealed 12 main ethical issues: justice and fairness, freedom and autonomy, privacy, transparency, patient safety and cyber security, trust, beneficence, responsibility, solidarity, sustainability, dignity, and conflicts. In addition to these 12 main ethical issues, we derived 19 ethical sub-issues and associated strategies from the literature.","https://www.mdpi.com/2673-2688/4/1/3/pdf",""
46,"Daniel Vale, Ali El-Sharif, Muhammed Ali","Explainable artificial intelligence (XAI) post-hoc explainability methods: risks and limitations in non-discrimination law",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00142-y","",786,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00142-y","2730-5953","",2,4,815,826,46,15.33,15,3,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00142-y.pdf",""
45,"Hannah Bleher, Matthias Braun","Diffused responsibility: attributions of responsibility in the use of AI-driven clinical decision support systems",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00135-x","",409,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00135-x","2730-5953","",2,4,747,761,45,15.00,23,2,3,"Abstract: Good decision-making is a complex endeavor, and particularly so in a health context. The possibilities for day-to-day clinical practice opened up by AI-driven clinical decision support systems (AI-CDSS) give rise to fundamental questions around responsibility. In causal, moral and legal terms the application of AI-CDSS is challenging existing attributions of responsibility. In this context, responsibility gaps are often identified as main problem. Mapping out the changing dynamics and levels of attributing responsibility, we argue in this article that the application of AI-CDSS causes diffusions of responsibility with respect to a causal, moral, and legal dimension. Responsibility diffusion describes the situation where multiple options and several agents can be considered for attributing responsibility. Using the example of an AI-driven ‘digital tumor board’, we illustrate how clinical decision-making is changed and diffusions of responsibility take place. Not denying or attempting to bridge responsibility gaps, we argue that dynamics and ambivalences are inherent in responsibility, which is based on normative considerations such as avoiding experiences of disregard and vulnerability of human life, which are inherently accompanied by a moment of uncertainty, and is characterized by revision openness. Against this background and to avoid responsibility gaps, the article concludes with suggestions for managing responsibility diffusions in clinical decision-making with AI-CDSS.","https://link.springer.com/content/pdf/10.1007/s43681-022-00135-x.pdf",""
43,"Hazem Zohny, John McMillan, Mike King","Ethics of generative AI",2023,"Journal of Medical Ethics","BMJ","https://doi.org/10.1136/jme-2023-108909","",720,"2025-02-04 16:55:17","journal-article","10.1136/jme-2023-108909","0306-6800","",49,2,79,80,43,21.50,14,3,2,"","https://syndication.highwire.org/content/doi/10.1136/jme-2023-108909",""
42,"Zsófia Tóth, Robert Caruana, Thorsten Gruber, Claudia Loebbecke","The Dawn of the AI Robots: Towards a New Framework of AI Robot Accountability",2022,"Journal of Business Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s10551-022-05050-z","",863,"2025-02-04 16:55:17","journal-article","10.1007/s10551-022-05050-z","0167-4544","",178,4,895,916,42,14.00,11,4,3,"Abstract: Business, management, and business ethics literature pay little attention to the topic of AI robots. The broad spectrum of potential ethical issues pertains to using driverless cars, AI robots in care homes, and in the military, such as Lethal Autonomous Weapon Systems. However, there is a scarcity of in-depth theoretical, methodological, or empirical studies that address these ethical issues, for instance, the impact of morality and where accountability resides in AI robots’ use. To address this dearth, this study offers a conceptual framework that interpretively develops the ethical implications of AI robot applications, drawing on descriptive and normative ethical theory. The new framework elaborates on how the locus of morality (human to AI agency) and moral intensity combine within context-specific AI robot applications, and how this might influence accountability thinking. Our theorization indicates that in situations of escalating AI agency and situational moral intensity, accountability is widely dispersed between actors and institutions. ‘Accountability clusters’ are outlined to illustrate interrelationships between the locus of morality, moral intensity, and accountability and how these invoke different categorical responses: (i) illegal, (ii) immoral, (iii) permissible, and (iv) supererogatory pertaining to using AI robots. These enable discussion of the ethical implications of using AI robots, and associated accountability challenges for a constellation of actors—from designer, individual/organizational users to the normative and regulative approaches of industrial/governmental bodies and intergovernmental regimes.","https://link.springer.com/content/pdf/10.1007/s10551-022-05050-z.pdf",""
40,"Blair Attard-Frost, Andrés De los Ríos, Deneille R. Walters","The ethics of AI business practices: a review of 47 AI ethics guidelines",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00156-6","",70,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00156-6","2730-5953","",3,2,389,406,40,13.33,13,3,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00156-6.pdf",""
38,"Erich Prem","From ethical AI frameworks to tools: a review of approaches",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00258-9","",228,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00258-9","2730-5953","",3,3,699,716,38,19.00,38,1,2,"Abstract: In reaction to concerns about a broad range of potential ethical issues, dozens of proposals for addressing ethical aspects of artificial intelligence (AI) have been published. However, many of them are too abstract for being easily translated into concrete designs for AI systems. The various proposed ethical frameworks can be considered an instance of principlism that is similar to that found in medical ethics. Given their general nature, principles do not say how they should be applied in a particular context. Hence, a broad range of approaches, methods, and tools have been proposed for addressing ethical concerns of AI systems. This paper presents a systematic analysis of more than 100 frameworks, process models, and proposed remedies and tools for helping to make the necessary shift from principles to implementation, expanding on the work of Morley and colleagues. This analysis confirms a strong focus of proposed approaches on only a few ethical issues such as explicability, fairness, privacy, and accountability. These issues are often addressed with proposals for software and algorithms. Other, more general ethical issues are mainly addressed with conceptual frameworks, guidelines, or process models. This paper develops a structured list and definitions of approaches, presents a refined segmentation of the AI development process, and suggests areas that will require more attention from researchers and developers.","https://link.springer.com/content/pdf/10.1007/s43681-023-00258-9.pdf",""
38,"Tim Hulsen","Explainable Artificial Intelligence (XAI): Concepts and Challenges in Healthcare",2023,"AI","MDPI AG","https://doi.org/10.3390/ai4030034","",821,"2025-02-04 16:55:17","journal-article","10.3390/ai4030034","2673-2688","",4,3,652,666,38,19.00,38,1,2,"Artificial Intelligence (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (“clinical decision support”), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a “black box”, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable Artificial Intelligence (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.","https://www.mdpi.com/2673-2688/4/3/34/pdf",""
36,"Aurelien Teguede Keleko, Bernard Kamsu-Foguem, Raymond Houe Ngouna, Amèvi Tongne","Artificial intelligence and real-time predictive maintenance in industry 4.0: a bibliometric analysis",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-021-00132-6","",862,"2025-02-04 16:55:17","journal-article","10.1007/s43681-021-00132-6","2730-5953","",2,4,553,577,36,12.00,9,4,3,"","https://link.springer.com/content/pdf/10.1007/s43681-021-00132-6.pdf",""
36,"Matti Minkkinen, Anniina Niukkanen, Matti Mäntymäki","What about investors? ESG analyses as tools for ethics-based AI auditing",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01415-0","",977,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01415-0","0951-5666","",39,1,329,343,36,12.00,12,3,3,"Abstract: Artificial intelligence (AI) governance and auditing promise to bridge the gap between AI ethics principles and the responsible use of AI systems, but they require assessment mechanisms and metrics. Effective AI governance is not only about legal compliance; organizations can strive to go beyond legal requirements by proactively considering the risks inherent in their AI systems. In the past decade, investors have become increasingly active in advancing corporate social responsibility and sustainability practices. Including nonfinancial information related to environmental, social, and governance (ESG) issues in investment analyses has become mainstream practice among investors. However, the AI auditing literature is mostly silent on the role of investors. The current study addresses two research questions: (1) how companies’ responsible use of AI is included in ESG investment analyses and (2) what connections can be found between principles of responsible AI and ESG ranking criteria. We conducted a series of expert interviews and analyzed the data using thematic analysis. Awareness of AI issues, measuring AI impacts, and governing AI processes emerged as the three main themes in the analysis. The findings indicate that AI is still a relatively unknown topic for investors, and taking the responsible use of AI into account in ESG analyses is not an established practice. However, AI is recognized as a potentially material issue for various industries and companies, indicating that its incorporation into ESG evaluations may be justified. There is a need for standardized metrics for AI responsibility, while critical bottlenecks and asymmetrical knowledge relations must be tackled.","https://link.springer.com/content/pdf/10.1007/s00146-022-01415-0.pdf",""
33,"Pravik Solanki, John Grundy, Waqar Hussain","Operationalising ethics in artificial intelligence for healthcare: a framework for AI developers",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00195-z","",161,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00195-z","2730-5953","",3,1,223,240,33,11.00,11,3,3,"Abstract: Artificial intelligence (AI) offers much promise for improving healthcare. However, it runs the looming risk of causing individual and societal harms; for instance, exacerbating inequalities amongst minority groups, or enabling compromises in the confidentiality of patients’ sensitive data. As such, there is an expanding, unmet need for ensuring AI for healthcare is developed in concordance with human values and ethics. Augmenting “principle-based” guidance that highlight adherence to ethical ideals (without necessarily offering translation into actionable practices), we offer a solution-based framework for operationalising ethics in AI for healthcare. Our framework is built from a scoping review of existing solutions of ethical AI guidelines, frameworks and technical solutions to address human values such as self-direction in healthcare. Our view spans the entire length of the AI lifecycle: data management, model development, deployment and monitoring. Our focus in this paper is to collate actionable solutions (whether technical or non-technical in nature), which can be steps that enable and empower developers in their daily practice to ensuring ethical practices in the broader picture. Our framework is intended to be adopted by AI developers, with recommendations that are accessible and driven by the existing literature. We endorse the recognised need for ‘ethical AI checklists’ co-designed with health AI practitioners, which could further operationalise the technical solutions we have collated. Since the risks to health and wellbeing are so large, we believe a proactive approach is necessary for ensuring human values and ethics are appropriately respected in AI for healthcare.","https://link.springer.com/content/pdf/10.1007/s43681-022-00195-z.pdf",""
32,"Cathy Roche, P. J. Wall, Dave Lewis","Ethics and diversity in artificial intelligence policies, strategies and initiatives",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00218-9","",373,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00218-9","2730-5953","",3,4,1095,1115,32,10.67,11,3,3,"Abstract: A burgeoning of Artificial Intelligence (AI) technologies in recent years has led to increased discussion about its potential to address many issues considered otherwise intractable, including those highlighted by the United Nations 2030 Agenda for Sustainable Development and associated Sustainable Development Goals. In tandem with this growth in AI is an expanding body of documentation regarding how such advanced technologies should be governed and managed. Issued by a variety of sources and comprising frameworks, policies and guidelines, this body of work encompasses the legal, social, ethical and policy issues around AI. With at least 470 such documents identified, as of May 2021, in the Council of Europe’s tracker of AI initiatives, questions are emerging around the diversity of views expressed, especially regarding the influence of the Global North or Euro-American perspectives. Our previous analysis of a corpus of largely grey literature discovered blind spots regarding both gender representation and perspectives from the Global South. Expanding on that work, this paper examines a significantly extended corpus, with a focus on the role of underrepresented groups in the wider AI discourse. We find that voices from the Global South and consideration of alternative ethical approaches are largely absent from the conversation. In light of the prominence of social, cultural and ethical perspectives from the Global North, this paper explores implications for the development of standards for ethical AI. Concluding by offering approaches to incorporate more diverse ethical viewpoints and beliefs, we call for increased consideration of power structures when developing AI ethics policies and standards within these alternative socio-cultural and socio-economic contexts.","https://link.springer.com/content/pdf/10.1007/s43681-022-00218-9.pdf",""
32,"Ilina Georgieva, Claudio Lazo, Tjerk Timan, Anne Fleur van Veenstra","From AI ethics principles to data science practice: a reflection and a gap analysis based on recent frameworks and practical experience",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-021-00127-3","",461,"2025-02-04 16:55:17","journal-article","10.1007/s43681-021-00127-3","2730-5953","",2,4,697,711,32,10.67,8,4,3,"","https://link.springer.com/content/pdf/10.1007/s43681-021-00127-3.pdf",""
30,"Nicholas Tilmes","Disability, fairness, and algorithmic bias in AI recruitment",2022,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-022-09633-2","",967,"2025-02-04 16:55:17","journal-article","10.1007/s10676-022-09633-2","1388-1957","",24,2,,,30,10.00,30,1,3,"","https://link.springer.com/content/pdf/10.1007/s10676-022-09633-2.pdf",""
29,"Rifat Ara Shams, Didar Zowghi, Muneera Bano","AI and the quest for diversity and inclusion: a systematic literature review",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00362-w","",266,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00362-w","2730-5953","",,,,,29,14.50,10,3,2,"Abstract: The pervasive presence and wide-ranging variety of artificial intelligence (AI) systems underscore the necessity for inclusivity and diversity in their design and implementation, to effectively address critical issues of fairness, trust, bias, and transparency. However, diversity and inclusion (D&I) considerations are significantly neglected in AI systems design, development, and deployment. Ignoring D&I in AI systems can cause digital redlining, discrimination, and algorithmic oppression, leading to AI systems being perceived as untrustworthy and unfair. Therefore, we conducted a systematic literature review (SLR) to identify the challenges and their corresponding solutions (guidelines/ strategies/ approaches/ practices) about D&I in AI and about the applications of AI for D&I practices. Through a rigorous search and selection, 48 relevant academic papers published from 2017 to 2022 were identified. By applying open coding on the extracted data from the selected papers, we identified 55 unique challenges and 33 unique solutions in addressing D&I in AI. We also identified 24 unique challenges and 23 unique solutions for enhancing D&I practices by AI. The result of our analysis and synthesis of the selected studies contributes to a deeper understanding of diversity and inclusion issues and considerations in the design, development and deployment of the AI ecosystem. The findings would play an important role in enhancing awareness and attracting the attention of researchers and practitioners in their quest to embed D&I principles and practices in future AI systems. This study also identifies important gaps in the research literature that will inspire future direction for researchers.","https://link.springer.com/content/pdf/10.1007/s43681-023-00362-w.pdf",""
28,"Jakob Mökander, Luciano Floridi","Operationalising AI governance through ethics-based auditing: an industry case study",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00171-7","",106,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00171-7","2730-5953","",3,2,451,468,28,9.33,14,2,3,"Abstract: Ethics-based auditing (EBA) is a structured process whereby an entity’s past or present behaviour is assessed for consistency with moral principles or norms. Recently, EBA has attracted much attention as a governance mechanism that may help to bridge the gap between principles and practice in AI ethics. However, important aspects of EBA—such as the feasibility and effectiveness of different auditing procedures—have yet to be substantiated by empirical research. In this article, we address this knowledge gap by providing insights from a longitudinal industry case study. Over 12 months, we observed and analysed the internal activities of AstraZeneca, a biopharmaceutical company, as it prepared for and underwent an ethics-based AI audit. While previous literature concerning EBA has focussed on proposing or analysing evaluation metrics or visualisation techniques, our findings suggest that the main difficulties large multinational organisations face when conducting EBA mirror classical governance challenges. These include ensuring harmonised standards across decentralised organisations, demarcating the scope of the audit, driving internal communication and change management, and measuring actual outcomes. The case study presented in this article contributes to the existing literature by providing a detailed description of the organisational context in which EBA procedures must be integrated to be feasible and effective.","https://link.springer.com/content/pdf/10.1007/s43681-022-00171-7.pdf",""
26,"Emanuele Ratti, Mark Graves","Explainable machine learning practices: opening another black box for reliable medical AI",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00141-z","",334,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00141-z","2730-5953","",2,4,801,814,26,8.67,13,2,3,"Abstract: In the past few years, machine learning (ML) tools have been implemented with success in the medical context. However, several practitioners have raised concerns about the lack of transparency—at the algorithmic level—of many of these tools; and solutions from the field of explainable AI (XAI) have been seen as a way to open the ‘black box’ and make the tools more trustworthy. Recently, Alex London has argued that in the medical context we do not need machine learning tools to be interpretable at the algorithmic level to make them trustworthy, as long as they meet some strict empirical desiderata. In this paper, we analyse and develop London’s position. In particular, we make two claims. First, we claim that London’s solution to the problem of trust can potentially address another problem, which is how to evaluate the reliability of ML tools in medicine for regulatory purposes. Second, we claim that to deal with this problem, we need to develop London’s views by shifting the focus from the opacity of algorithmic details to the opacity of the way in which ML tools are trained and built. We claim that to regulate AI tools and evaluate their reliability, agencies need an explanation of how ML tools have been built, which requires documenting and justifying the technical choices that practitioners have made in designing such tools. This is because different algorithmic designs may lead to different outcomes, and to the realization of different purposes. However, given that technical choices underlying algorithmic design are shaped by value-laden considerations, opening the black box of the design process means also making transparent and motivating (technical and ethical) values and preferences behind such choices. Using tools from philosophy of technology and philosophy of science, we elaborate a framework showing how an explanation of the training processes of ML tools in medicine should look like.","https://link.springer.com/content/pdf/10.1007/s43681-022-00141-z.pdf",""
25,"Gijs van Maanen","AI Ethics, Ethics Washing, and the Need to Politicize Data Ethics",2022,"Digital Society","Springer Science and Business Media LLC","https://doi.org/10.1007/s44206-022-00013-3","",648,"2025-02-04 16:55:17","journal-article","10.1007/s44206-022-00013-3","2731-4650","",1,2,,,25,8.33,25,1,3,"Abstract: Many commercial actors in the tech sector publish ethics guidelines as a means to ‘wash away’ concerns raised about their policies. For some academics, this phenomenon is reason to replace ethics with other tools and methods in an attempt to make sure that the tech sector does not cross any moral Rubicons. Others warn against the tendency to reduce a criticism of ‘ethics washing’ into one of ethics simpliciter. In this essay, I argue firstly that the dominant focus on principles, dilemmas, and theory in conventional ethical theories and practices could be an explanation of it lacking resistance to abuse by dominant actors, and hence its rather disappointing capacity to stop, redirect, or at least slow down big tech’s course. Secondly, drawing from research on casuistry and political philosopher Raymond Geuss, this essay will make a case for a question, rather than theory or principle-based ethical data practice. The emphasis of this approach is placed on the acquisition of a thorough understanding of a social-political phenomenon like tech development. This approach should be replenished with one extra component to the picture of the repoliticized data ethics drawn so far: the importance of ‘exemplars,’ or stories. Precisely the fact that one should acquire an in-depth understanding of the problem in practice will also allow one to look in the past, present, or future for similar and comparable stories from which one can learn.","https://link.springer.com/content/pdf/10.1007/s44206-022-00013-3.pdf",""
25,"Marianna Anagnostou, Olga Karvounidou, Chrysovalantou Katritzidaki, Christina Kechagia, Kyriaki Melidou, Eleni Mpeza, Ioannis Konstantinidis, Eleni Kapantai, Christos Berberidis, Ioannis Magnisalis, Vassilios Peristeras","Characteristics and challenges in the industries towards responsible AI: a systematic literature review",2022,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-022-09634-1","",665,"2025-02-04 16:55:17","journal-article","10.1007/s10676-022-09634-1","1388-1957","",24,3,,,25,8.33,3,11,3,"","https://link.springer.com/content/pdf/10.1007/s10676-022-09634-1.pdf",""
24,"Thilo Hagendorff, Leonie N. Bossert, Yip Fai Tse, Peter Singer","Speciesist bias in AI: how AI applications perpetuate discrimination and unfair outcomes against animals",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00199-9","",381,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00199-9","2730-5953","",3,3,717,734,24,8.00,6,4,3,"Abstract: Massive efforts are made to reduce biases in both data and algorithms to render AI applications fair. These efforts are propelled by various high-profile cases where biased algorithmic decision-making caused harm to women, people of color, minorities, etc. However, the AI fairness field still succumbs to a blind spot, namely its insensitivity to discrimination against animals. This paper is a critical comment on current fairness research in AI. It is the first to describe the ‘speciesist bias’ and investigate it in several different AI systems by reflecting on the problem via a normative analysis and by probing, in several case studies, image recognition, word embedding, and language models with established methods for bias detection. We claim that animals matter morally and that discriminating against them is unethical. Furthermore, we provide evidence for speciesist biases in all the mentioned areas of AI. We find that speciesist biases are solidified by many mainstream AI applications, especially in the fields of computer vision as well as natural language processing. In both cases, this occurs because the models are trained on datasets in which speciesist patterns prevail. Therefore, AI technologies currently play a significant role in perpetuating and normalizing violence against animals. To change this, AI fairness frameworks must widen their scope and include mitigation measures for speciesist biases. This paper addresses the AI community in this regard and stresses the influence AI systems can have on either increasing or reducing the violence that is inflicted on animals, especially on farmed animals.","https://link.springer.com/content/pdf/10.1007/s43681-022-00199-9.pdf",""
23,"Mohamad Koohi-Moghadam, Kyongtae Ty Bae","Generative AI in Medical Imaging: Applications, Challenges, and Ethics",2023,"Journal of Medical Systems","Springer Science and Business Media LLC","https://doi.org/10.1007/s10916-023-01987-4","",197,"2025-02-04 16:55:17","journal-article","10.1007/s10916-023-01987-4","1573-689X","",47,1,,,23,11.50,12,2,2,"","https://link.springer.com/content/pdf/10.1007/s10916-023-01987-4.pdf",""
23,"Hannah Bleher, Matthias Braun","Reflections on Putting AI Ethics into Practice: How Three AI Ethics Approaches Conceptualize Theory and Practice",2023,"Science and Engineering Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s11948-023-00443-3","",307,"2025-02-04 16:55:17","journal-article","10.1007/s11948-023-00443-3","1353-3452","",29,3,,,23,11.50,12,2,2,"Abstract: Critics currently argue that applied ethics approaches to artificial intelligence (AI) are too principles-oriented and entail a theory–practice gap. Several applied ethical approaches try to prevent such a gap by conceptually translating ethical theory into practice. In this article, we explore how the currently most prominent approaches of AI ethics translate ethics into practice. Therefore, we examine three approaches to applied AI ethics: the embedded ethics approach, the ethically aligned approach, and the Value Sensitive Design (VSD) approach. We analyze each of these three approaches by asking how they understand and conceptualize theory and practice. We outline the conceptual strengths as well as their shortcomings: an embedded ethics approach is context-oriented but risks being biased by it; ethically aligned approaches are principles-oriented but lack justification theories to deal with trade-offs between competing principles; and the interdisciplinary Value Sensitive Design approach is based on stakeholder values but needs linkage to political, legal, or social governance aspects. Against this background, we develop a meta-framework for applied AI ethics conceptions with three dimensions. Based on critical theory, we suggest these dimensions as starting points to critically reflect on the conceptualization of theory and practice. We claim, first, that the inclusion of the dimension of","https://link.springer.com/content/pdf/10.1007/s11948-023-00443-3.pdf",""
23,"Jan-Christoph Heilinger","The Ethics of AI Ethics. A Constructive Critique",2022,"Philosophy &amp; Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s13347-022-00557-9","",891,"2025-02-04 16:55:17","journal-article","10.1007/s13347-022-00557-9","2210-5433","",35,3,,,23,7.67,23,1,3,"Abstract: The paper presents an ethical analysis and constructive critique of the current practice of AI ethics. It identifies conceptual substantive and procedural challenges and it outlines strategies to address them. The strategies include countering the hype and understanding AI as ubiquitous infrastructure including neglected issues of ethics and justice such as structural background injustices into the scope of AI ethics and making the procedures and fora of AI ethics more inclusive and better informed with regard to philosophical ethics. These measures integrate the perspective of AI justice into AI ethics, strengthening its capacity to provide comprehensive normative orientation and guidance for the development and use of AI that actually improves human lives and living together.","https://link.springer.com/content/pdf/10.1007/s13347-022-00557-9.pdf",""
22,"Catherine Stinson","Algorithms are not neutral",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00136-w","",391,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00136-w","2730-5953","",2,4,763,770,22,7.33,22,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00136-w.pdf",""
21,"Alvaro Fernandez-Quilez","Deep learning in radiology: ethics of data and on the value of algorithm transparency, interpretability and explainability",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00161-9","",450,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00161-9","2730-5953","",3,1,257,265,21,7.00,21,1,3,"Abstract: AI systems are quickly being adopted in radiology and, in general, in healthcare. A myriad of systems is being proposed and developed on a daily basis for high-stake decisions that can lead to unwelcome and negative consequences. AI systems trained under the supervised learning paradigm greatly depend on the quality and amount of data used to develop them. Nevertheless, barriers in data collection and sharing limit the data accessibility and potential ethical challenges might arise due to them leading, for instance, to systems that do not offer equity in their decisions and discriminate against certain patient populations or that are vulnerable to appropriation of intellectual property, among others. This paper provides an overview of some of the ethical issues both researchers and end-users might meet during data collection and development of AI systems, as well an introduction to the current state of transparency, interpretability and explainability of the systems in radiology applications. Furthermore, we aim to provide a comprehensive summary of currently open questions and identify key issues during the development and deployment of AI systems in healthcare, with a particular focus on the radiology area.","https://link.springer.com/content/pdf/10.1007/s43681-022-00161-9.pdf",""
21,"Christopher Burr, David Leslie","Ethical assurance: a practical approach to the responsible design, development, and deployment of data-driven technologies",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00178-0","",708,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00178-0","2730-5953","",3,1,73,98,21,7.00,11,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00178-0.pdf",""
20,"Philip Brey, Brandt Dainow","Ethics by design for artificial intelligence",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00330-4","",174,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00330-4","2730-5953","",4,4,1265,1277,20,10.00,10,2,2,"Abstract: In this paper, we present an approach for the systematic and comprehensive inclusion of ethical considerations in the design and development process of artificial intelligence systems, called Ethics by Design for AI (EbD-AI). The approach is the result of a three-year long research effort, and has recently be adopted by the European Commission as part of its ethics review procedure for AI projects. We describe and explain the approach and its different components and its application to the development of AI software and systems. We also compare it to other approaches in AI ethics, and we consider limitations of the approach as well as potential criticisms.","https://link.springer.com/content/pdf/10.1007/s43681-023-00330-4.pdf",""
19,"Richard Benjamins, Yaiza Rubio Viñuela, Chema Alonso","Social and ethical challenges of the metaverse",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00278-5","",17,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00278-5","2730-5953","",3,3,689,697,19,9.50,6,3,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00278-5.pdf",""
18,"Mark Coeckelbergh","Democracy, epistemic agency, and AI: political epistemology in times of artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00239-4","",267,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00239-4","2730-5953","",3,4,1341,1350,18,6.00,18,1,3,"Abstract: Democratic theories assume that citizens have some form of political knowledge in order to vote for representatives or to directly engage in democratic deliberation and participation. However, apart from widespread attention to the phenomenon of fake news and misinformation, less attention has been paid to","https://link.springer.com/content/pdf/10.1007/s43681-022-00239-4.pdf",""
18,"Anna Lena Hunkenschroer, Alexander Kriebitz","Is AI recruiting (un)ethical? A human rights perspective on the use of AI for hiring",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00166-4","",302,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00166-4","2730-5953","",3,1,199,213,18,6.00,9,2,3,"Abstract: The use of artificial intelligence (AI) technologies in organizations’ recruiting and selection procedures has become commonplace in business practice; accordingly, research on AI recruiting has increased substantially in recent years. But, though various articles have highlighted the potential opportunities and ethical risks of AI recruiting, the topic has not been normatively assessed yet. We aim to fill this gap by providing an ethical analysis of AI recruiting from a human rights perspective. In doing so, we elaborate on human rights’ theoretical implications for corporate use of AI-driven hiring solutions. Therefore, we analyze whether AI hiring practices inherently conflict with the concepts of validity, autonomy, nondiscrimination, privacy, and transparency, which represent the main human rights relevant in this context. Concluding that these concepts are not at odds, we then use existing legal and ethical implications to determine organizations’ responsibility to enforce and realize human rights standards in the context of AI recruiting.","https://link.springer.com/content/pdf/10.1007/s43681-022-00166-4.pdf",""
18,"Amelia Katirai","Ethical considerations in emotion recognition technologies: a review of the literature",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00307-3","",495,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00307-3","2730-5953","",4,4,927,948,18,9.00,18,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00307-3.pdf",""
18,"Alycia N. Carey, Xintao Wu","The statistical fairness field guide: perspectives from social and formal sciences",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00183-3","",602,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00183-3","2730-5953","",3,1,1,23,18,6.00,9,2,3,"Abstract: Over the past several years, a multitude of methods to measure the fairness of a machine learning model have been proposed. However, despite the growing number of publications and implementations, there is still a critical lack of literature that explains the interplay of fair machine learning with the social sciences of philosophy, sociology, and law. We hope to remedy this issue by accumulating and expounding upon the thoughts and discussions of fair machine learning produced by both social and formal (i.e., machine learning and statistics) sciences in this field guide. Specifically, in addition to giving the mathematical and algorithmic backgrounds of several popular statistics-based fair machine learning metrics used in fair machine learning, we explain the underlying philosophical and legal thoughts that support them. Furthermore, we explore several criticisms of the current approaches to fair machine learning from sociological, philosophical, and legal viewpoints. It is our hope that this field guide helps machine learning practitioners identify and remediate cases where algorithms violate human rights and values.","https://link.springer.com/content/pdf/10.1007/s43681-022-00183-3.pdf",""
17,"Karoline Reinhardt","Trust and trustworthiness in AI ethics",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00200-5","",51,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00200-5","2730-5953","",3,3,735,744,17,5.67,17,1,3,"Abstract: Due to the extensive progress of research in artificial intelligence (AI) as well as its deployment and application, the public debate on AI systems has also gained momentum in recent years. With the publication of the","https://link.springer.com/content/pdf/10.1007/s43681-022-00200-5.pdf",""
17,"Ekaterina Svetlova","AI ethics and systemic risks in finance",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-021-00129-1","",58,"2025-02-04 16:55:17","journal-article","10.1007/s43681-021-00129-1","2730-5953","",2,4,713,725,17,5.67,17,1,3,"Abstract: The paper suggests that AI ethics should pay attention to morally relevant systemic effects of AI use. It draws the attention of ethicists and practitioners to systemic risks that have been neglected so far in professional AI-related codes of conduct, industrial standards and ethical discussions more generally. The paper uses the financial industry as an example to ask: how can AI-enhanced systemic risks be ethically accounted for? Which specific issues does AI use raise for ethics that takes systemic effects into account? The paper (1) relates the literature about","https://link.springer.com/content/pdf/10.1007/s43681-021-00129-1.pdf",""
17,"Michael Pflanzer, Zachary Traylor, Joseph B. Lyons, Veljko Dubljević, Chang S. Nam","Ethics in human–AI teaming: principles and perspectives",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00214-z","",203,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00214-z","2730-5953","",3,3,917,935,17,5.67,3,5,3,"Abstract: Ethical considerations are the fabric of society, and they foster cooperation, help, and sacrifice for the greater good. Advances in AI create a greater need to examine ethical considerations involving the development and implementation of such systems. Integrating ethics into artificial intelligence-based programs is crucial for preventing negative outcomes, such as privacy breaches and biased decision making. Human–AI teaming (HAIT) presents additional challenges, as the ethical principles and moral theories that provide justification for them are not yet computable by machines. To that effect, models of human judgments and decision making, such as the agent-deed-consequence (ADC) model, will be crucial to inform the ethical guidance functions in AI team mates and to clarify how and why humans (dis)trust machines. The current paper will examine the ADC model as it is applied to the context of HAIT, and the challenges associated with the use of human-centric ethical considerations when applied to an AI context.","https://link.springer.com/content/pdf/10.1007/s43681-022-00214-z.pdf",""
17,"Sophia Falk, Aimee van Wynsberghe","Challenging AI for Sustainability: what ought it mean?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00323-3","",248,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00323-3","2730-5953","",4,4,1345,1355,17,8.50,9,2,2,"Abstract: This paper argues that the terms ‘Sustainable artificial intelligence (AI)’ in general and ‘Sustainability of AI’ in particular are overused to the extent that they have lost their meaning. The AI for (social) good movement is a manifestation of this trend in which almost any application used in the context of healthcare or agriculture can be classified as AI for good regardless of whether such applications have been evaluated from a broader perspective. In this paper, we aim to create a common understanding of what the ‘AI for Sustainability’ movement ought to mean. We distinguish between two possible AI for Sustainability applications, namely those that fulfill the necessary conditions and those that fulfill the sufficient conditions. The former are purely predictive systems that serve as information providers. The latter are directly involved in an activity that contributes to a sustainability goal. We argue that taking action is a key element in distinguishing between these two application groups, as inaction is the key bottleneck in effectively tackling climate change. Furthermore, we question how effective the use of AI applications can be for sustainability when the systems themselves are inherently unsustainable. Hence, AI for Sustainability should include both an action that contributes to a sustainable end goal as well as an investigation of the sustainability issues of the AI system itself. Following that, Sustainable AI research can be on a gradient: AI in an application domain, AI towards sustainability, and AI for Sustainability.","https://link.springer.com/content/pdf/10.1007/s43681-023-00323-3.pdf",""
17,"Zouhaier Slimi, Beatriz Villarejo Carballido","Navigating the Ethical Challenges of Artificial Intelligence in Higher Education: An Analysis of Seven Global AI Ethics Policies",2023,"TEM Journal","Association for Information Communication Technology Education and Science (UIKTEN)","https://doi.org/10.18421/tem122-02","",482,"2025-02-04 16:55:17","journal-article","10.18421/tem122-02","2217-8333","",,,590,602,17,8.50,9,2,2,"AI use in higher education raises ethical concerns that must be addressed. Biased algorithms pose a significant threat, especially if used in admission or grading processes, as they could have devastating effects on students. Another issue is the displacement of human educators by AI systems, and there are concerns about transparency and accountability as AI becomes more integrated into decision-making processes. This paper examined three AI objectives related higher education: biased algorithms, AI and decision-making, and human displacement. Discourse analysis of seven AI ethics policies was conducted, including those from UNESCO, China, the European Commission, Google, MIT, Sanford HAI, and Carnegie Mellon. The findings indicate that stakeholders must work together to address these challenges and ensure responsible AI deployment in higher education while maximizing its benefits. Fair use and protecting individuals, especially those with vulnerable characteristics, are crucial. Gender bias must be avoided in algorithm development, learning data sets, and AI decision-making. Data collection, labeling, and algorithm documentation must be of the highest quality to ensure traceability and openness. Universities must study the ethical, social, and policy implications of AI to ensure responsible development and deployment. The AI ethics policies stress responsible AI development and deployment, with a focus on transparency and accountability. Making AI systems more transparent and answerable may reduce the adverse effects of displacement. In conclusion, AI must be considered ethically in higher education, and stakeholders must ensure that AI is used responsibly, fairly, and in a way that maximizes its benefits while minimizing its risks.","https://www.temjournal.com/content/122/TEMJournalMay2023_590_602.pdf",""
17,"Cindy Friedman","Ethical concerns with replacing human relations with humanoid robots: an ubuntu perspective",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00186-0","",540,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00186-0","2730-5953","",3,2,527,538,17,5.67,17,1,3,"Abstract: This paper considers ethical concerns with regard to replacing human relations with humanoid robots. Many have written about the impact that certain types of relations with robots may have on us, and why we should be concerned about robots replacing human relations. There has, however, been no consideration of this issue from an African philosophical perspective. Ubuntu philosophy provides a novel perspective on how relations with robots may impact our own moral character and moral development. This paper first discusses what humanoid robots are, why and how humans tend to anthropomorphise them, and what the literature says about robots crowding out human relations. It then explains the ideal of becoming “fully human”, which pertains to being particularly moral in character. In ubuntu philosophy, we are not only biologically human, but must strive to become better, more moral versions of ourselves, to become fully human. We can become fully human by having other regarding traits or characteristics within the context of interdependent, or humane, relationships (such as by exhibiting human equality, reciprocity, or solidarity). This concept of becoming fully human is important in ubuntu philosophy. Having explained that idea, the main argument of the paper is then put forward: that treating humanoid robots as if they are human is morally concerning if they crowd out human relations, because such relations prevent us from becoming fully human. This is because we cannot experience human equality, solidarity, and reciprocity with robots, which can be seen to characterise interdependent, or humane, relations with human beings.","https://link.springer.com/content/pdf/10.1007/s43681-022-00186-0.pdf",""
17,"Md. Abdul Malek","Criminal courts’ artificial intelligence: the way it reinforces bias and discrimination",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00137-9","",541,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00137-9","2730-5953","",2,1,233,245,17,5.67,17,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00137-9.pdf",""
17,"Hyesun Choung, Prabu David, Arun Ross","Trust and ethics in AI",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01473-4","",669,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01473-4","0951-5666","",38,2,733,745,17,5.67,6,3,3,"","https://link.springer.com/content/pdf/10.1007/s00146-022-01473-4.pdf",""
17,"Avinash Agarwal, Harsh Agarwal, Nihaarika Agarwal","Fairness Score and process standardization: framework for fairness certification in artificial intelligence systems",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00147-7","",682,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00147-7","2730-5953","",3,1,267,279,17,5.67,6,3,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00147-7.pdf",""
16,"Angelo Trotta, Marta Ziosi, Vincenzo Lomonaco","The future of ethics in AI: challenges and opportunities",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01644-x","",27,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01644-x","0951-5666","",38,2,439,441,16,8.00,5,3,2,"","https://link.springer.com/content/pdf/10.1007/s00146-023-01644-x.pdf",""
16,"Peter Singer, Yip Fai Tse","AI ethics: the case for including animals",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00187-z","",77,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00187-z","2730-5953","",3,2,539,551,16,5.33,8,2,3,"Abstract: The ethics of artificial intelligence, or AI ethics, is a rapidly growing field, and rightly so. While the range of issues and groups of stakeholders concerned by the field of AI ethics is expanding, with speculation about whether it extends even to the machines themselves, there is a group of sentient beings who are also affected by AI, but are rarely mentioned within the field of AI ethics—the nonhuman animals. This paper seeks to explore the kinds of impact AI has on nonhuman animals, the severity of these impacts, and their moral implications. We hope that this paper will facilitate the development of a new field of philosophical and technical research regarding the impacts of AI on animals, namely, the ethics of AI as it affects nonhuman animals.","https://link.springer.com/content/pdf/10.1007/s43681-022-00187-z.pdf",""
16,"Abejide Ade-Ibijola, Chinedu Okonkwo","Artificial Intelligence in Africa: Emerging Challenges",2023,"Social and Cultural Studies of Robots and AI","Springer International Publishing","https://doi.org/10.1007/978-3-031-08215-3_5","",995,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-08215-3_5","2523-8523","",,,101,117,16,8.00,8,2,2,"Abstract: In the current African society, Artificial Intelligence (AI) is becoming more popular and seeking to cover all facets of human activity. The adoption and use of these modern technologies in the African context are currently low due to some emerging challenges. Consequently, these difficulties may have a direct influence on African economic development. In this paper, we highlight the challenges facing the adoption of AI technologies in Africa which include skills acquisition, lack of structured data ecosystem, ethics, government policies, insufficient infrastructure and network connectivity, uncertainty, and user attitude. Finally, various solutions to enhance AI adoption in Africa were then proposed.","https://link.springer.com/content/pdf/10.1007/978-3-031-08215-3_5",""
15,"Vasiliki Mollaki","Death of a reviewer or death of peer review integrity? the challenges of using AI tools in peer reviewing and the need to go beyond publishing policies",2024,"Research Ethics","SAGE Publications","https://doi.org/10.1177/17470161231224552","",380,"2025-02-04 16:55:17","journal-article","10.1177/17470161231224552","1747-0161","",20,2,239,250,15,15.00,15,1,1,"Peer review facilitates quality control and integrity of scientific research. Although publishing policies have adapted to include the use of Artificial Intelligence (AI) tools, such as Chat Generative Pre-trained Transformer (ChatGPT), in the preparation of manuscripts by authors, there is a lack of guidelines or policies on whether peer reviewers can use such tools. The present article highlights the lack of policies on the use of AI tools in the peer review process (PRP) and argues that we need to go beyond policies by creating transparent procedures that will enable journals to investigate allegations of non-compliance and take decisions that will protect the integrity of the peer review system. Reviewers found to violate relevant policies must be excluded from the process to safeguard the integrity of the peer review system.","https://journals.sagepub.com/doi/pdf/10.1177/17470161231224552",""
15,"Ren Bin Lee Dixon","A principled governance for emerging AI regimes: lessons from China, the European Union, and the United States",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00205-0","",465,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00205-0","2730-5953","",3,3,793,810,15,5.00,15,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00205-0.pdf",""
15,"Nancy S. Jecker, Eisuke Nakazawa","Bridging East-West Differences in Ethics Guidance for AI and Robotics",2022,"AI","MDPI AG","https://doi.org/10.3390/ai3030045","",604,"2025-02-04 16:55:17","journal-article","10.3390/ai3030045","2673-2688","",3,3,764,777,15,5.00,8,2,3,"Societies of the East are often contrasted with those of the West in their stances toward technology. This paper explores these perceived differences in the context of international ethics guidance for artificial intelligence (AI) and robotics. Japan serves as an example of the East, while Europe and North America serve as examples of the West. The paper’s principal aim is to demonstrate that Western values predominate in international ethics guidance and that Japanese values serve as a much-needed corrective. We recommend a hybrid approach that is more inclusive and truly ‘international’. Following an introduction, the paper examines distinct stances toward robots that emerged in the West and Japan, respectively, during the aftermath of the Second World War, reflecting history and popular culture, socio-economic conditions, and religious worldviews. It shows how international ethics guidelines reflect these disparate stances, drawing on a 2019 scoping review that examined 84 international AI ethics documents. These documents are heavily skewed toward precautionary values associated with the West and cite the optimistic values associated with Japan less frequently. Drawing insights from Japan’s so-called ‘moonshot goals’, the paper fleshes out Japanese values in greater detail and shows how to incorporate them more effectively in international ethics guidelines for AI and robotics.","https://www.mdpi.com/2673-2688/3/3/45/pdf",""
15,"Ryan Watkins","Guidance for researchers and peer-reviewers on the ethical use of Large Language Models (LLMs) in scientific research workflows",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00294-5","",754,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00294-5","2730-5953","",4,4,969,974,15,7.50,15,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00294-5.pdf",""
15,"Mirko Farina, Petr Zhdanov, Artur Karimov, Andrea Lavazza","AI and society: a virtue ethics approach",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01545-5","",823,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01545-5","0951-5666","",39,3,1127,1140,15,5.00,4,4,3,"","https://link.springer.com/content/pdf/10.1007/s00146-022-01545-5.pdf",""
15,"Bernd Carsten Stahl","Embedding responsibility in intelligent systems: from AI ethics to responsible AI ecosystems",2023,"Scientific Reports","Springer Science and Business Media LLC","https://doi.org/10.1038/s41598-023-34622-w","",852,"2025-02-04 16:55:17","journal-article","10.1038/s41598-023-34622-w","2045-2322","",13,1,,,15,7.50,15,1,2,"Abstract: Intelligent systems that are capable of making autonomous decisions based on input from their environment have great potential to do good, but they also raise significant social and ethical concerns. The discourse on ethics and artificial intelligence (AI) has covered these concerns in depth and developed an array of possible ways of addressing them. This article argues that a shortcoming of this discourse is that it concentrates on specific issues and their mitigation but neglects the nature of intelligent systems as socio-technical systems of systems that are often described as ecosystems. Building on the discussion of ethics and AI, the article suggests that it would be beneficial to come to an understanding of what would constitute responsible AI ecosystems. By introducing the concept of meta-responsibility or higher-level responsibility, the article proposes characteristics that an ecosystem would have to fulfil, in order to be considered a responsible ecosystem. This perspective is theoretically interesting because it extends the current AI ethics discourse. It furthermore offers a novel perspective for researchers and developers of intelligent system and helps them reflect on the way they relate to ethical issues.","https://www.nature.com/articles/s41598-023-34622-w.pdf",""
14,"Mario D. Schultz, Peter Seele","Towards AI ethics’ institutionalization: knowledge bridges from business ethics to advance organizational AI ethics",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00150-y","",34,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00150-y","2730-5953","",3,1,99,111,14,4.67,7,2,3,"Abstract: This paper proposes to generate awareness for developing Artificial intelligence (AI) ethics by transferring knowledge from other fields of applied ethics, particularly from business ethics, stressing the role of organizations and processes of institutionalization. With the rapid development of AI systems in recent years, a new and thriving discourse on AI ethics has (re-)emerged, dealing primarily with ethical concepts, theories, and application contexts. We argue that business ethics insights may generate positive knowledge spillovers for AI ethics, given that debates on ethical and social responsibilities have been adopted as voluntary or mandatory regulations for organizations in both national and transnational contexts. Thus, business ethics may transfer knowledge from five core topics and concepts researched and institutionalized to AI ethics: (1) stakeholder management, (2) standardized reporting, (3) corporate governance and regulation, (4) curriculum accreditation, and as a unified topic (5) AI ethics washing derived from greenwashing. In outlining each of these five knowledge bridges, we illustrate current challenges in AI ethics and potential insights from business ethics that may advance the current debate. At the same time, we hold that business ethics can learn from AI ethics in catching up with the digital transformation, allowing for cross-fertilization between the two fields. Future debates in both disciplines of applied ethics may benefit from dialog and cross-fertilization, meant to strengthen the ethical depth and prevent ethics washing or, even worse, ethics bashing.","https://link.springer.com/content/pdf/10.1007/s43681-022-00150-y.pdf",""
14,"Sanna J. Ali, Angèle Christin, Andrew Smart, Riitta Katila","Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs",2023,"2023 ACM Conference on Fairness, Accountability, and Transparency","ACM","https://doi.org/10.1145/3593013.3593990","",200,"2025-02-04 16:55:17","proceedings-article","10.1145/3593013.3593990","","",,,217,226,14,7.00,4,4,2,"","https://dl.acm.org/doi/pdf/10.1145/3593013.3593990",""
14,"Dinesh Kumar, Nidhi Suthar","Ethical and legal challenges of AI in marketing: an exploration of solutions",2024,"Journal of Information, Communication and Ethics in Society","Emerald","https://doi.org/10.1108/jices-05-2023-0068","",584,"2025-02-04 16:55:17","journal-article","10.1108/jices-05-2023-0068","1477-996X","",22,1,124,144,14,14.00,7,2,1,"Purpose: Artificial intelligence (AI) has sparked interest in various areas, including marketing. However, this exhilaration is being tempered by growing concerns about the moral and legal implications of using AI in marketing. Although previous research has revealed various ethical and legal issues, such as algorithmic discrimination and data privacy, there are no definitive answers. This paper aims to fill this gap by investigating AI’s ethical and legal concerns in marketing and suggesting feasible solutions. Design/methodology/approach: The paper synthesises information from academic articles, industry reports, case studies and legal documents through a thematic literature review. A qualitative analysis approach categorises and interprets ethical and legal challenges and proposes potential solutions. Findings: The findings of this paper raise concerns about ethical and legal challenges related to AI in the marketing area. Ethical concerns related to discrimination, bias, manipulation, job displacement, absence of social interaction, cybersecurity, unintended consequences, environmental impact, privacy and legal issues such as consumer security, responsibility, liability, brand protection, competition law, agreements, data protection, consumer protection and intellectual property rights are discussed in the paper, and their potential solutions are discussed. Research limitations/implications: Notwithstanding the interesting insights gathered from this investigation of the ethical and legal consequences of AI in marketing, it is important to recognise the limits of this research. Initially, the focus of this study is confined to a review of the most important ethical and legal issues pertaining to AI in marketing. Additional possible repercussions, such as those associated with intellectual property, contracts and licencing, should be investigated more deeply in future studies. Despite the fact that this study gives various answers and best practices for tackling the stated ethical and legal concerns, the viability and efficacy of these solutions may differ depending on the context and industry. Thus, more research and case studies are required to evaluate the applicability and efficacy of these solutions in other circumstances. This research is mostly based on a literature review and may not represent the experiences or opinions of all stakeholders engaged in AI-powered marketing. Further study might involve interviews or surveys with marketing professionals, customers and other key stakeholders to offer a full knowledge of the practical difficulties and solutions. Because of the rapid speed of technical progress, AI’s ethical and regulatory ramifications in marketing are continually increasing. Consequently, this work should be a springboard for more research and continuing conversations on this subject. Practical implications: This study’s findings have several practical implications for marketing professionals. Emphasising openness and explainability: Marketing professionals should prioritise transparency in their use of AI, ensuring that customers are fully informed about data collection and utilisation for targeted advertising. By promoting openness and explainability, marketers can foster customer trust and avoid the negative consequences of a lack of transparency. Establishing ethical guidelines: Marketing professionals need to develop ethical rules for the creation and implementation of AI-powered marketing strategies. Adhering to ethical principles ensures compliance with legal norms and aligns with the organisation’s values and ideals. Investing in bias detection tools and privacy-enhancing technology: To mitigate risks associated with AI in marketing, marketers should allocate resources to develop and implement bias detection tools and privacy-enhancing technology. These tools can identify and address biases in AI algorithms, safeguard consumer privacy and extract valuable insights from consumer data. Social implications: This study’s social implications emphasise the need for a comprehensive approach to address the ethical and legal challenges of AI in marketing. This includes adopting a responsible innovation framework, promoting ethical leadership, using ethical decision-making frameworks and conducting multidisciplinary research. By incorporating these approaches, marketers can navigate the complexities of AI in marketing responsibly, foster an ethical organisational culture, make informed ethical decisions and develop effective solutions. Such practices promote public trust, ensure equitable distribution of benefits and risk, and mitigate potential negative social consequences associated with AI in marketing. Originality/value: To the best of the authors’ knowledge, this paper is among the first to explore potential solutions comprehensively. This paper provides a nuanced understanding of the challenges by using a multidisciplinary framework and synthesising various sources. It contributes valuable insights for academia and industry.","https://www.emerald.com/insight/content/doi/10.1108/JICES-05-2023-0068/full/html",""
14,"Jianlong Zhou, Fang Chen","AI ethics: from principles to practice",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01602-z","",642,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01602-z","0951-5666","",38,6,2693,2703,14,4.67,7,2,3,"","https://link.springer.com/content/pdf/10.1007/s00146-022-01602-z.pdf",""
14,"Jaana Hallamaa, Taina Kalliokoski","AI Ethics as Applied Ethics",2022,"Frontiers in Computer Science","Frontiers Media SA","https://doi.org/10.3389/fcomp.2022.776837","",684,"2025-02-04 16:55:17","journal-article","10.3389/fcomp.2022.776837","2624-9898","",4,,,,14,4.67,7,2,3,"The need to design and develop artificial intelligence (AI) in a sustainable manner has motivated researchers, institutions, and organizations to formulate suggestions for AI ethics. Although these suggestions cover various topics and address diverse audiences, they share the presupposition that AI ethics provides a generalizable basis for designers that is applicable to their work. We propose that one of the reasons the influence of current ethical codes has remained modest, may be the conception of the applied ethics that they represent. We discuss bioethics as a point of reference for weighing the metaethical and methodological approaches adopted in AI ethics, and propose that AI ethics could be made more methodologically solid and substantively more influential if the resources were enriched by adopting tools from fields of study created to improve the quality of human action and safeguard its desired outcomes. The approaches we consider to be useful for this purpose are the systems theory, safety research, impact assessment approach, and theory of change.","https://www.frontiersin.org/articles/10.3389/fcomp.2022.776837/full",""
13,"Marietjie Botes","Autonomy and the social dilemma of online manipulative behavior",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00157-5","",420,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00157-5","2730-5953","",3,1,315,323,13,4.33,13,1,3,"Abstract: Persuasive online technologies were initially designed and used to gain insights into the online behavior of individuals to personalize advertising campaigns in an effort to influence people and convince them to buy certain products. But recently, these technologies have blurred the lines and morphed into technologies that covertly and gradually manipulate people into attaining a goal that is predetermined by the algorithm and disregards the decision-making rights of the individual. This may lead to people exercising decisions that do not align with their personal values and beliefs, and rob them of their autonomy—an ethical principle, in the absence of which the application of these technologies may be unethical. However, not all technologies that are persuasive are necessarily manipulative which require the careful consideration of a couple of elements to determine whether or not technologies are manipulative and ultimately whether their application is ethical or not. In this article, we analyze the ethical principle of autonomy and unpack the underlying elements of this ethical principle which must be considered to determine whether the application of a technology is ethical or not in the context of it being persuasive or manipulative.","https://link.springer.com/content/pdf/10.1007/s43681-022-00157-5.pdf",""
13,"Malak Sadek, Rafael A. Calvo, Céline Mougenot","Designing value-sensitive AI: a critical review and recommendations for socio-technical design processes",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00373-7","",466,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00373-7","2730-5953","",4,4,949,967,13,6.50,4,3,2,"Abstract: This paper presents a critical review of how different socio-technical design processes for AI-based systems, from scholarly works and industry, support the creation of value-sensitive AI (VSAI). The review contributes to the emerging field of human-centred AI, and the even more embryonic space of VSAI in four ways: (i) it introduces three criteria for the review of VSAI based on their contribution to design processes’ overall value-sensitivity, and as a response to criticisms that current interventions are lacking in these aspects: comprehensiveness, level of guidance offered, and methodological value-sensitivity, (ii) it provides a novel review of socio-technical design processes for AI-based systems, (iii) it assesses each process based on the mentioned criteria and synthesises the results into broader trends, and (iv) it offers a resulting set of recommendations for the design of VSAI. The objective of the paper is to help creators and followers of design processes—whether scholarly or industry-based—to understand the level of value-sensitivity offered by different socio-technical design processes and act accordingly based on their needs: to adopt or adapt existing processes or to create new ones.","https://link.springer.com/content/pdf/10.1007/s43681-023-00373-7.pdf",""
13,"Rosalie A. Waelen","The struggle for recognition in the age of facial recognition technology",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00146-8","",536,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00146-8","2730-5953","",3,1,215,222,13,4.33,13,1,3,"Abstract: Facial recognition is a promising emerging technology, but it sometimes fails to recognize people adequately. Facial recognition applications have been found to regularly misidentify certain demographics, misinterpret traits like gender, age, beliefs, or emotions, and categorize individuals in ways that do not resonate with their own sense of identity. In this paper, I argue that in each of these cases, the person who has their face analyzed is not merely misidentified or misunderstood, but misrecognized in an ethically relevant sense. Following Charles Taylor’s The Politics of Recognition (1992) and Axel Honneth’s The Struggle for Recognition (1996), I describe how those subjected to facial recognition systems on the one hand struggle to obtain adequate recognition on a universal level, as being equally important to others, and lack recognition for their individual uniqueness, on the other hand. These forms of misrecognition are of ethical concern, because they can harm a person’s identity formation. So, ironically, facial recognition technology can give rise to a struggle for recognition.","https://link.springer.com/content/pdf/10.1007/s43681-022-00146-8.pdf",""
13,"Hugo Cossette-Lefebvre, Jocelyn Maclure","AI’s fairness problem: understanding wrongful discrimination in the context of automated decision-making",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00233-w","",645,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00233-w","2730-5953","",3,4,1255,1269,13,4.33,7,2,3,"Abstract: The use of predictive machine learning algorithms is increasingly common to guide or even take decisions in both public and private settings. Their use is touted by some as a potentially useful method to avoid discriminatory decisions since they are, allegedly, neutral, objective, and can be evaluated in ways no human decisions can. By (fully or partly) outsourcing a decision process to an algorithm, it should allow human organizations to clearly define the parameters of the decision and to, in principle, remove human biases. Yet, in practice, the use of algorithms can still be the source of wrongful discriminatory decisions based on at least three of their features: the data-mining process and the categorizations they rely on can reconduct human biases, their automaticity and predictive design can lead them to rely on wrongful generalizations, and their opaque nature is at odds with democratic requirements. We highlight that the two latter aspects of algorithms and their significance for discrimination are too often overlooked in contemporary literature. Though these problems are not all insurmountable, we argue that it is necessary to clearly define the conditions under which a machine learning decision tool can be used. We identify and propose three main guidelines to properly constrain the deployment of machine learning algorithms in society: algorithms should be vetted to ensure that they do not unduly affect historically marginalized groups; they should not systematically override or replace human decision-making processes; and the decision reached using an algorithm should always be explainable and justifiable.","https://link.springer.com/content/pdf/10.1007/s43681-022-00233-w.pdf",""
13,"Nathalie de Marcellis-Warin, Frédéric Marty, Eva Thelisson, Thierry Warin","Artificial intelligence and consumer manipulations: from consumer's counter algorithms to firm's self-regulation tools",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00149-5","",856,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00149-5","2730-5953","",2,2,259,268,13,4.33,3,4,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00149-5.pdf",""
12,"Tim Gorichanaz","Being at home in the metaverse? Prospectus for a social imaginary",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00198-w","",496,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00198-w","2730-5953","",3,2,647,658,12,4.00,12,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00198-w.pdf",""
12,"Aastha Pant, Rashina Hoda, Simone V. Spiegler, Chakkrit Tantithamthavorn, Burak Turhan","Ethics in the Age of AI: An Analysis of AI Practitioners’ Awareness and Challenges",2024,"ACM Transactions on Software Engineering and Methodology","Association for Computing Machinery (ACM)","https://doi.org/10.1145/3635715","",505,"2025-02-04 16:55:17","journal-article","10.1145/3635715","1049-331X","",33,3,1,35,12,12.00,2,5,1,"Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI—AI practitioners—have to say about their understanding of AI ethics and the challenges associated with incorporating it into the AI-based systems they develop? Understanding AI practitioners’ views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners’","https://dl.acm.org/doi/pdf/10.1145/3635715",""
12,"Mark Theunissen, Jacob Browning","Putting explainable AI in context: institutional explanations for medical AI",2022,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-022-09649-8","",525,"2025-02-04 16:55:17","journal-article","10.1007/s10676-022-09649-8","1388-1957","",24,2,,,12,4.00,6,2,3,"Abstract: There is a current debate about if, and in what sense, machine learning systems used in the medical context need to be explainable. Those arguing in favor contend these systems require post hoc explanations for each individual decision to increase trust and ensure accurate diagnoses. Those arguing against suggest the high accuracy and reliability of the systems is sufficient for providing epistemic justified beliefs without the need for explaining each individual decision. But, as we show, both solutions have limitations—and it is unclear either address the epistemic worries of the medical professionals using these systems. We argue these systems do require an explanation, but an","https://link.springer.com/content/pdf/10.1007/s10676-022-09649-8.pdf",""
12,"Sven Nyholm","A new control problem? Humanoid robots, artificial intelligence, and the value of control",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00231-y","",552,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00231-y","2730-5953","",3,4,1229,1239,12,4.00,12,1,3,"Abstract: The control problem related to robots and AI usually discussed is that we might lose control over advanced technologies. When authors like Nick Bostrom and Stuart Russell discuss this control problem, they write in a way that suggests that having as much control as possible is good while losing control is bad. In life in general, however, not all forms of control are unambiguously positive and unproblematic. Some forms—e.g. control over other persons—are ethically problematic. Other forms of control are positive, and perhaps even intrinsically good. For example, one form of control that many philosophers have argued is intrinsically good and a virtue is self-control. In this paper, I relate these questions about control and its value to different forms of robots and AI more generally. I argue that the more robots are made to resemble human beings, the more problematic it becomes—at least symbolically speaking—to want to exercise full control over these robots. After all, it is unethical for one human being to want to fully control another human being. Accordingly, it might be seen as problematic—viz. as representing something intrinsically bad—to want to create humanoid robots that we exercise complete control over. In contrast, if there are forms of AI such that control over them can be seen as a form of self-control, then this might be seen as a virtuous form of control. The “new control problem”, as I call it, is the question of under what circumstances retaining and exercising complete control over robots and AI is unambiguously ethically good.","https://link.springer.com/content/pdf/10.1007/s43681-022-00231-y.pdf",""
12,"Sara Berger, Francesca Rossi","AI and Neurotechnology",2023,"Communications of the ACM","Association for Computing Machinery (ACM)","https://doi.org/10.1145/3529088","",647,"2025-02-04 16:55:17","journal-article","10.1145/3529088","0001-0782","",66,3,58,68,12,6.00,6,2,2,"The merging of machine, body, and psyche is on the horizon due to the technological advancements enabled by neuroscience and AI.","https://dl.acm.org/doi/pdf/10.1145/3529088",""
11,"Sara Kassir, Lewis Baker, Jackson Dolphin, Frida Polli","AI for hiring in context: a perspective on overcoming the unique challenges of employment research to mitigate disparate impact",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00208-x","",25,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00208-x","2730-5953","",3,3,845,868,11,3.67,3,4,3,"Abstract: Commentators interested in the societal implications of automated decision-making often overlook how decisions are made in the technology’s absence. For example, the benefits of ML and big data are often summarized as efficiency, objectivity, and consistency; the risks, meanwhile, include replicating historical discrimination and oversimplifying nuanced situations. While this perspective tracks when technology replaces capricious human judgements, it is ill-suited to contexts where standardized assessments already exist. In spaces like employment selection, the relevant question is how an ML model compares to a manually built test. In this paper, we explain that since the Civil Rights Act, industrial and organizational (I/O) psychologists have struggled to produce assessments without disparate impact. By examining the utility of ML for conducting exploratory analyses, coupled with the back-testing capability offered by advances in data science, we explain modern technology’s utility for hiring. We then empirically investigate a commercial hiring platform that applies several oft-cited benefits of ML to build custom job models for corporate employers. We focus on the disparate impact observed when models are deployed to evaluate real-world job candidates. Across a sample of 60 jobs built for 26 employers and used to evaluate approximately 400,00 candidates, minority-weighted impact ratios of 0.93 (Black–White), 0.97 (Hispanic–White), and 0.98 (Female–Male) are observed. We find similar results for candidates selecting disability-related accommodations within the platform versus unaccommodated users. We conclude by describing limitations, anticipating criticisms, and outlining further research.","https://link.springer.com/content/pdf/10.1007/s43681-022-00208-x.pdf",""
11,"Jamy Li, Mark Chignell","FMEA-AI: AI fairness impact assessment using failure mode and effects analysis",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00145-9","",179,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00145-9","2730-5953","",2,4,837,850,11,3.67,6,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00145-9.pdf",""
11,"Uwe Peters","Explainable AI lacks regulative reasons: why AI and human decision-making are not equally opaque",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00217-w","",213,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00217-w","2730-5953","",3,3,963,974,11,3.67,11,1,3,"Abstract: Many artificial intelligence (AI) systems currently used for decision-making are opaque, i.e., the internal factors that determine their decisions are not fully known to people due to the systems’ computational complexity. In response to this problem, several researchers have argued that human decision-making is equally opaque and since simplifying, reason-giving explanations (rather than exhaustive causal accounts) of a decision are typically viewed as sufficient in the human case, the same should hold for algorithmic decision-making. Here, I contend that this argument overlooks that human decision-making is sometimes significantly more transparent and trustworthy than algorithmic decision-making. This is because when people explain their decisions by giving reasons for them, this frequently prompts those giving the reasons to govern or regulate themselves so as to think and act in ways that confirm their reason reports. AI explanation systems lack this self-regulative feature. Overlooking it when comparing algorithmic and human decision-making can result in underestimations of the transparency of human decision-making and in the development of explainable AI that may mislead people by activating generally warranted beliefs about the regulative dimension of reason-giving.","https://link.springer.com/content/pdf/10.1007/s43681-022-00217-w.pdf",""
11,"Onur Bakiner","What do academics say about artificial intelligence ethics? An overview of the scholarship",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00182-4","",318,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00182-4","2730-5953","",3,2,513,525,11,3.67,11,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00182-4.pdf",""
11,"Ali Ladak","What would qualify an artificial intelligence for moral standing?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00260-1","",421,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00260-1","2730-5953","",4,2,213,228,11,5.50,11,1,2,"Abstract: What criteria must an artificial intelligence (AI) satisfy to qualify for moral standing? My starting point is that sentient AIs should qualify for moral standing. But future AIs may have unusual combinations of cognitive capacities, such as a high level of cognitive sophistication without sentience. This raises the question of whether sentience is a necessary criterion for moral standing, or merely sufficient. After reviewing nine criteria that have been proposed in the literature, I suggest that there is a strong case for thinking that some non-sentient AIs, such as those that are conscious and have non-valenced preferences and goals, and those that are non-conscious and have sufficiently cognitively complex preferences and goals, should qualify for moral standing. After responding to some challenges, I tentatively argue that taking into account uncertainty about which criteria an entity must satisfy to qualify for moral standing, and strategic considerations such as how such decisions will affect humans and other sentient entities, further supports granting moral standing to some non-sentient AIs. I highlight three implications: that the issue of AI moral standing may be more important, in terms of scale and urgency, than if either sentience or consciousness is necessary; that researchers working on policies designed to be inclusive of sentient AIs should broaden their scope to include all AIs with morally relevant interests; and even those who think AIs cannot be sentient or conscious should take the issue seriously. However, much uncertainty about these considerations remains, making this an important topic for future research.","https://link.springer.com/content/pdf/10.1007/s43681-023-00260-1.pdf",""
11,"Andrew McStay","Replika in the Metaverse: the moral problem with empathy in ‘It from Bit’",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00252-7","",553,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00252-7","2730-5953","",3,4,1433,1445,11,3.67,11,1,3,"Abstract: This paper assesses claims of computational empathy in relation to existing social open-ended chatbots and intention that these chatbots will feature in emergent mixed reality contexts, recently given prominence due to interest in the Metaverse. Against the background of increasing loneliness within society and use of chatbots as a potential remedy for this, the paper considers two leading current social chatbots,","https://link.springer.com/content/pdf/10.1007/s43681-022-00252-7.pdf",""
11,"Saleh Afroogh","A probabilistic theory of trust concerning artificial intelligence: can intelligent robots trust humans?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00174-4","",560,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00174-4","2730-5953","",3,2,469,484,11,3.67,11,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00174-4.pdf",""
11,"Petar Radanliev, David De Roure","Review of the state of the art in autonomous artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00176-2","",586,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00176-2","2730-5953","",3,2,497,504,11,3.67,6,2,3,"Abstract: This article presents a new design for autonomous artificial intelligence (AI), based on the state-of-the-art algorithms, and describes a new autonomous AI system called ‘AutoAI’. The methodology is used to assemble the design founded on self-improved algorithms that use new and emerging sources of data (NEFD). The objective of the article is to conceptualise the design of a novel AutoAI algorithm. The conceptual approach is used to advance into building new and improved algorithms. The article integrates and consolidates the findings from existing literature and advances the AutoAI design into (1) using new and emerging sources of data for teaching and training AI algorithms and (2) enabling AI algorithms to use automated tools for training new and improved algorithms. This approach is going beyond the state-of-the-art in AI algorithms and suggests a design that enables autonomous algorithms to self-optimise and self-adapt, and on a higher level, be capable to self-procreate.","https://link.springer.com/content/pdf/10.1007/s43681-022-00176-2.pdf",""
11,"Federica Russo, Eric Schliesser, Jean Wagemans","Connecting ethics and epistemology of AI",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01617-6","",698,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01617-6","0951-5666","",39,4,1585,1603,11,5.50,4,3,2,"Abstract: The need for fair and just AI is often related to the possibility of understanding AI itself, in other words, of turning an opaque box into a glass box, as inspectable as possible. Transparency and explainability, however, pertain to the technical domain and to philosophy of science, thus leaving the ethics and epistemology of AI largely disconnected. To remedy this, we propose an integrated approach premised on the idea that a glass-box epistemology should explicitly consider how to incorporate values and other normative considerations, such as intersectoral vulnerabilities, at critical stages of the whole process from design and implementation to use and assessment. To connect ethics and epistemology of AI, we perform a double shift of focus. First, we move from trusting the output of an AI system to trusting the process that leads to the outcome. Second, we move from expert assessment to more inclusive assessment strategies, aiming to facilitate expert and non-expert assessment. Together, these two moves yield a framework usable for experts and non-experts when they inquire into relevant epistemological and ethical aspects of AI systems. We dub our framework ‘epistemology-cum-ethics’ to signal the equal importance of both aspects. We develop it from the vantage point of the designers: how to create the conditions to internalize values into the whole process of design, implementation, use, and assessment of an AI system, in which values (epistemic and non-epistemic) are explicitly considered at each stage and inspectable by every salient actor involved at any moment.","https://link.springer.com/content/pdf/10.1007/s00146-022-01617-6.pdf",""
11,"James Bessen, Stephen Michael Impink, Robert Seamans","The Cost of Ethical AI Development for AI Startups",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3534195","",877,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3534195","","",,,92,106,11,3.67,4,3,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3534195",""
11,"Julian J. Koplin","Dual-use implications of AI text generation",2023,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-023-09703-z","",949,"2025-02-04 16:55:17","journal-article","10.1007/s10676-023-09703-z","1388-1957","",25,2,,,11,5.50,11,1,2,"Abstract: AI researchers have developed sophisticated language models capable of generating paragraphs of 'synthetic text' on topics specified by the user. While AI text generation has legitimate benefits, it could also be misused, potentially to grave effect. For example, AI text generators could be used to automate the production of convincing fake news, or to inundate social media platforms with machine-generated disinformation. This paper argues that AI text generators should be conceptualised as a dual-use technology, outlines some relevant lessons from earlier debates on dual-use life sciences research, and calls for closer collaboration between ethicists and the machine learning community to address AI language models’ dual-use implications.","https://link.springer.com/content/pdf/10.1007/s10676-023-09703-z.pdf",""
10,"Thilo Hagendorff, David Danks","Ethical and methodological challenges in building morally informed AI systems",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00188-y","",4,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00188-y","2730-5953","",3,2,553,566,10,3.33,5,2,3,"Abstract: Recent progress in large language models has led to applications that can (at least) simulate possession of full moral agency due to their capacity to report context-sensitive moral assessments in open-domain conversations. However, automating moral decision-making faces several methodological as well as ethical challenges. They arise in the fields of bias mitigation, missing ground truth for moral “correctness”, effects of bounded ethicality in machines, changes in moral norms over time, risks of using morally informed AI systems as actual advice, as well as societal implications an increasing importance of algorithmic moral decision-making would have. This paper comments on all these challenges and provides critical considerations for future research on full artificial moral agency. Importantly, some of the adduced challenges can be met by more careful technology design, but others necessarily require engagement with core problems of meta-ethics.","https://link.springer.com/content/pdf/10.1007/s43681-022-00188-y.pdf",""
10,"Anne Zimmerman, Joel Janhonen, Emily Beer","Human/AI relationships: challenges, downsides, and impacts on human/human relationships",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00348-8","",6,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00348-8","2730-5953","",4,4,1555,1567,10,5.00,3,3,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00348-8.pdf",""
10,"Shailendra Kumar, Sanghamitra Choudhury","Normative ethics, human rights, and artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00170-8","",207,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00170-8","2730-5953","",3,2,441,450,10,3.33,5,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00170-8.pdf",""
10,"Tahereh Saheb","“Ethically contentious aspects of artificial intelligence surveillance: a social science perspective”",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00196-y","",500,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00196-y","2730-5953","",3,2,369,379,10,3.33,10,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00196-y.pdf",""
10,"Melanie Smallman","Multi Scale Ethics—Why We Need  to Consider the Ethics of AI in Healthcare at Different Scales",2022,"Science and Engineering Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s11948-022-00396-z","",798,"2025-02-04 16:55:17","journal-article","10.1007/s11948-022-00396-z","1353-3452","",28,6,,,10,3.33,10,1,3,"Abstract: Many researchers have documented how AI and data driven technologies have the potential to have profound effects on our lives—in ways that make these technologies stand out from those that went before. Around the world, we are seeing a significant growth in interest and investment in AI in healthcare. This has been coupled with rising concerns about the ethical implications of these technologies and an array of ethical guidelines for the use of AI and data in healthcare has arisen. Nevertheless, the question of if and how AI and data technologies can be ethical remains open to debate. This paper aims to contribute to this debate by considering the wide range of implications that have been attributed to these technologies and asking whether current ethical guidelines take these factors into account. In particular, the paper argues that while current ethics guidelines for AI in healthcare effectively account for the four key issues identified in the ethics literature (transparency; fairness; responsibility and privacy), they have largely neglected wider issues relating to the way in which these technologies shape institutional and social arrangements. This, I argue, has given current ethics guidelines a strong focus on evaluating the impact of these technologies on the individual, while not accounting for the powerful social shaping effects of these technologies. To address this, the paper proposes a Multiscale Ethics Framework, which aims to help technology developers and ethical evaluations to consider the wider implications of these technologies.","https://link.springer.com/content/pdf/10.1007/s11948-022-00396-z.pdf",""
9,"Edward Hunter Christie, Amy Ertan, Laurynas Adomaitis, Matthias Klaus","Regulating lethal autonomous weapon systems: exploring the challenges of explainability and traceability",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00261-0","",29,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00261-0","2730-5953","",4,2,229,245,9,4.50,2,4,2,"Abstract: We explore existing political commitments by states regarding the development and use of lethal autonomous weapon systems. We carry out two background reviewing efforts, the first addressing ethical and legal framings and proposals from recent academic literature, the second addressing recent formal policy principles as endorsed by states, with a focus on the principles adopted by the United States Department of Defense and the North Atlantic Treaty Organization. We then develop two conceptual case studies. The first addresses the interrelated principles of explainability and traceability, leading to proposals for acceptable scope limitations to these principles. The second considers the topic of deception in warfare and how it may be viewed in the context of ethical principles for lethal autonomous weapon systems.","https://link.springer.com/content/pdf/10.1007/s43681-023-00261-0.pdf",""
9,"Thilo Hagendorff","AI ethics and its pitfalls: not living up to its own standards?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00173-5","",97,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00173-5","2730-5953","",3,1,329,336,9,3.00,9,1,3,"Abstract: AI ethics is deemed to be an essential ingredient in the quest for trustworthy AI. Hence, demands for implementing AI ethics and ethicists into AI organizations, especially corporations, are ubiquitous. However, the assumption that AI ethicists have particular epistemological advantages compared to non-ethicists as well as the idea that AI ethics automatically decreases the likelihood of unethical outcomes are both flawed. Therefore, this comment lists risks that either originate from AI ethicists themselves or from the consequences their embedding in AI organizations has. The compilation of risks comprises psychological considerations concerning the cognitive biases of AI ethicists themselves as well as biased reactions to their work, subject-specific and knowledge constraints AI ethicists often succumb to, negative side effects of ethics audits for AI applications, and many more. Ultimately, the aim of this comment is not to diminish or deny the importance of the discipline of AI ethics, but rather to increase its capacities for self-reflection and, ultimately, effectiveness.","https://link.springer.com/content/pdf/10.1007/s43681-022-00173-5.pdf",""
9,"Kevin LaGrandeur","The consequences of AI hype",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00352-y","",105,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00352-y","2730-5953","",4,3,653,656,9,4.50,9,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00352-y.pdf",""
9,"Ori Freiman","Making sense of the conceptual nonsense ‘trustworthy AI’",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00241-w","",167,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00241-w","2730-5953","",3,4,1351,1360,9,3.00,9,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00241-w.pdf",""
9,"Ramón Alvarado","What kind of trust does AI deserve, if any?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00224-x","",184,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00224-x","2730-5953","",3,4,1169,1183,9,3.00,9,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00224-x.pdf",""
9,"Päivi Kousa, Hannele Niemi","AI ethics and learning: EdTech companies’ challenges and solutions",2022,"Interactive Learning Environments","Informa UK Limited","https://doi.org/10.1080/10494820.2022.2043908","",287,"2025-02-04 16:55:17","journal-article","10.1080/10494820.2022.2043908","1049-4820","",31,10,6735,6746,9,3.00,5,2,3,"","https://www.tandfonline.com/doi/pdf/10.1080/10494820.2022.2043908",""
9,"Suzanne Kawamleh","Against explainability requirements for ethical artificial intelligence in health care",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00212-1","",447,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00212-1","2730-5953","",3,3,901,916,9,3.00,9,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00212-1.pdf",""
9,"Jorge Luis Morton Gutiérrez","On actor-network theory and algorithms: ChatGPT and the new power relationships in the age of AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00314-4","",471,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00314-4","2730-5953","",4,4,1071,1084,9,4.50,9,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00314-4.pdf",""
9,"Muhammad Usman Tariq","The Role of AI Ethics in Cost and Complexity Reduction",2024,"Advances in Business Information Systems and Analytics","IGI Global","https://doi.org/10.4018/979-8-3693-2643-5.ch004","",716,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-2643-5.ch004","2327-3275","",,,59,78,9,9.00,9,1,1,"This chapter underscores the intrinsic connection between AI ethics and contemporary business dynamics, elucidating how ethical considerations can serve as facilitators for cost control and operational simplification. It offers a comprehensive exploration of how corporations can harness ethical principles to augment efficiency, transparency, and sustainability. This exploration delves into the impediments, strategic methodologies, and tangible benefits associated with the ethical deployment of AI. Commencing with a succinct overview of AI ethics, the chapter delineates key concepts, notably responsibility, justice, and transparency, thereby establishing a foundational understanding of their pivotal significance across diverse business scenarios. The intricate correlation between AI implementation and cost management is accentuated, with a focal point on how ethical AI practices contribute to cost savings and operational streamlining.","https://www.igi-global.com/viewtitle.aspx?TitleId=347527",""
9,"Muhammad Zahid Iqbal, Abraham G. Campbell","Adopting smart glasses responsibly: potential benefits, ethical, and privacy concerns with Ray-Ban stories",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00155-7","",722,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00155-7","2730-5953","",3,1,325,327,9,3.00,5,2,3,"Abstract: The adoption of innovative wearable technologies is potentially increasing as a new trend. Jumping into the augmented reality (AR) and Metaverse, Facebook (now known as Meta) launched smart glasses partnering with Ray-Ban sunglasses brand’s parent company EssilorLuxottica. Ray-Ban stories has several technical features for entertainment and socializing; more importantly, these features can be adopted in the future for more advanced wearable. However, these smart glasses also came with many ethical and privacy concerns along with their potential benefits. Furthermore, the unbridled deployment of these smart glasses brought several challenging questions for public social interaction when we will have more such devices in our lives. This short article has discussed the Ray-Ban stories’ ethical and privacy issues for social interaction and public places.","https://link.springer.com/content/pdf/10.1007/s43681-022-00155-7.pdf",""
9,"Konstantinos Konstantis, Antonios Georgas, Antonis Faras, Konstantinos Georgas, Aristotle Tympas","Ethical considerations in working with ChatGPT on a questionnaire about the future of work with ChatGPT",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00312-6","",909,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00312-6","2730-5953","",4,4,1335,1344,9,4.50,2,5,2,"Abstract: The prospect of the use of Large Language Models, like ChatGPT, in work environments raises important questions regarding both the potential for a dramatic change in the quality of jobs and the risk of unemployment. The answers to these questions, but, also, the posing of questions to be answered, may involve the use of ChatGPT. This, in turn, may give rise to a series of ethical considerations. The article seeks to identify such considerations by presenting a research on a questionnaire that was developed by means of ChatGPT before it was answered, first, by a group of humans (H) and, then, through the use of a machine (M), ChatGPT. The language model was actually used to respond to the questionnaire twice. First, based on its data (M1), and, second, based on it being asked to imitate a human (M2). Based on the significant differences between the H and M answers, and, further, on the noticeable differences occurring within the M answers (the differences between the M1 and M2 answers), the article concludes by registering a cluster of three ethical considerations.","https://link.springer.com/content/pdf/10.1007/s43681-023-00312-6.pdf",""
8,"Lameck Mbangula Amugongo, Alexander Kriebitz, Auxane Boch, Christoph Lütge","Operationalising AI ethics through the agile software development lifecycle: a case study of AI-enabled mobile health applications",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00331-3","",111,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00331-3","2730-5953","",,,,,8,4.00,2,4,2,"Abstract: Although numerous ethical principles and guidelines have been proposed to guide the development of artificial intelligence (AI) systems, it has proven difficult to translate these principles into actionable practices beyond mere adherence to ethical ideas. This is particularly challenging in the context of AI systems for healthcare, which requires balancing the potential benefits of the solution against the risks to patients and the wider community, including minorities and underserved populations. To address this challenge, we propose a shift from one-size-fits-all ethical principles to contextualized case-based ethical frameworks. This study uses an AI-enabled mHealth application as a case study. Our framework is built on existing ethical guidelines and principles, including the AI4People framework, the EU High-Level Expert Group on trustworthy AI, and wider human rights considerations. Additionally, we incorporate relational perspectives to address human value concerns and moral tensions between individual rights and public health. Our approach is based on ”ethics by design,” where ethical principles are integrated throughout the entire AI development pipeline, ensuring that ethical considerations are not an afterthought but implemented from the beginning. For our case study, we identified 7 ethical principles: fairness, agility, precision, safeguarding humanity, respect for others, trust and accountability, and robustness and reproducibility. We believe that the best way to mitigate and address ethical consequences is by implementing ethical principles in the software development processes that developers commonly use. Finally, we provide examples of how our case-based framework can be applied in practice, using examples of AI-driven mobile applications in healthcare.","https://link.springer.com/content/pdf/10.1007/s43681-023-00331-3.pdf",""
8,"Andrea Aler Tubella, Marçal Mora-Cantallops, Juan Carlos Nieves","How to teach responsible AI in Higher Education: challenges and opportunities",2023,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-023-09733-7","",130,"2025-02-04 16:55:17","journal-article","10.1007/s10676-023-09733-7","1388-1957","",26,1,,,8,4.00,3,3,2,"Abstract: In recent years, the European Union has advanced towards responsible and sustainable Artificial Intelligence (AI) research, development and innovation. While the Ethics Guidelines for Trustworthy AI released in 2019 and the AI Act in 2021 set the starting point for a European Ethical AI, there are still several challenges to translate such advances into the public debate, education and practical learning. This paper contributes towards closing this gap by reviewing the approaches that can be found in the existing literature and by interviewing 11 experts across five countries to help define educational strategies, competencies and resources needed for the successful implementation of Trustworthy AI in Higher Education (HE) and to reach students from all disciplines. The findings are presented in the form of recommendations both for educators and policy incentives, translating the guidelines into HE teaching and practice, so that the next generation of young people can contribute to an ethical, safe and cutting-edge AI made in Europe.","https://link.springer.com/content/pdf/10.1007/s10676-023-09733-7.pdf",""
8,"Krishna Ravali Jammalamadaka, Srikanth Itapu","Responsible AI in automated credit scoring systems",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00175-3","",220,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00175-3","2730-5953","",3,2,485,495,8,2.67,4,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00175-3.pdf",""
8,"Jessica Rudd, Claudia Igbrude","A global perspective on data powering responsible AI solutions in health applications",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00302-8","",331,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00302-8","2730-5953","",4,4,1039,1049,8,4.00,4,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00302-8.pdf",""
8,"Tricia A. Griffin, Brian Patrick Green, Jos V. M. Welie","The ethical agency of AI developers",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00256-3","",332,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00256-3","2730-5953","",4,2,179,188,8,4.00,3,3,2,"Abstract: Public and academic discourse about the ethics of artificial intelligence, machine learning, and data science has largely focused on the algorithms and the companies deploying them. Little attention has been paid to the ethical agency of the developers. This study is the first of its kind that centers developers in the ethical environment. Semi-structured interviews with 40 developers about the ethics of being a developer revealed more than 20 themes, 3 of which are the subject of this paper: ethics in the occupational ecosystem, developer ethical agency, and the characteristics of an ethical developer. These themes reveal significant gaps between how developers perceive themselves and the reality of their work experiences. Their ethical agency is likewise variable. They have some authority to intervene for ethical reasons in systems they work on, but they often do not realize just how many ethical decisions they make. Nonetheless, this study reveals a growing ethical wisdom in this community, one that needs to be surfaced and nurtured by engaging with developers.","https://link.springer.com/content/pdf/10.1007/s43681-022-00256-3.pdf",""
8,"Mirko Farina, Xiao Yu, Andrea Lavazza","Ethical considerations and policy interventions concerning the impact of generative AI tools in the economy and in society",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00405-2","",407,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00405-2","2730-5953","",,,,,8,8.00,3,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00405-2.pdf",""
8,"Chinasa T. Okolo, Kehinde Aruleba, George Obaido","Responsible AI in Africa—Challenges and Opportunities",2023,"Social and Cultural Studies of Robots and AI","Springer International Publishing","https://doi.org/10.1007/978-3-031-08215-3_3","",714,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-08215-3_3","2523-8523","",,,35,64,8,4.00,3,3,2,"Abstract: This chapter provides an analysis into the factors that impact the effective adoption and successful implementation of artificial intelligence (AI) technologies in Africa. The study begins by defining the concept of “responsible AI” and what this means specifically for technologies developed and used within Africa. Further sections within the chapter present challenges including digital literacy, lack of local AI talent and governmental barriers that impede. The chapter also goes into an in-depth analysis of the AI startup and research landscape within the African continent, highlighting organisations of interest and concerning trends. ‘Challenges to effective AI adoption and Implementation in Africa’ concludes by envisioning what responsible AI could look like in the African context and provides actionable recommendations for making strides towards this goal.","https://link.springer.com/content/pdf/10.1007/978-3-031-08215-3_3",""
8,"Imdad Ali Shah, Noor Zaman Jhanjhi, Sarfraz Nawaz Brohi","Use of AI-Based Drones in Smart Cities",2024,"Advances in Information Security, Privacy, and Ethics","IGI Global","https://doi.org/10.4018/979-8-3693-0774-8.ch015","",809,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-0774-8.ch015","1948-9730","",,,362,380,8,8.00,3,3,1,"Autonomous drones, known as AI drones, have been working without human intervention. It is doing something like navigation, avoiding obstacles, and taking picture recognition without explicit human direction, thanks to AI technology. It's an advantage of AI-based drones that they are capable of flying without human intervention. This can be used for missions such as monitoring searches and rescue operations in remote areas where human lives are at risk. Because of their sophisticated cameras and detectors, AI-based drones are able to collect and analyze large amounts of data in real-time. With this data, comprehensive maps may be made, locations of interest can be found, and the situational awareness of human operators can be improved. These days, the concept of a “smart city” intrigues everyone. Advanced technologies such as AI, blockchain, the IoT, drones, and many more are integrated.","https://www.igi-global.com/viewtitle.aspx?TitleId=340084",""
8,"Georg Starke, Benedikt Schmidt, Eva De Clercq, Bernice Simone Elger","Explainability as fig leaf? An exploration of experts’ ethical expectations towards machine learning in psychiatry",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00177-1","",908,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00177-1","2730-5953","",3,1,303,314,8,2.67,2,4,3,"Abstract: The increasing implementation of programs supported by machine learning in medical contexts will affect psychiatry. It is crucial to accompany this development with careful ethical considerations informed by empirical research involving experts from the field, to identify existing problems, and to address them with fine-grained ethical reflection. We conducted semi-structured qualitative interviews with 15 experts from Germany and Switzerland with training in medicine and neuroscience on the assistive use of machine learning in psychiatry. We used reflexive thematic analysis to identify key ethical expectations and attitudes towards machine learning systems. Experts’ ethical expectations towards machine learning in psychiatry partially challenge orthodoxies from the field. We relate these challenges to three themes, namely (1) ethical challenges of machine learning research, (2) the role of explainability in research and clinical application, and (3) the relation of patients, physicians, and machine learning system. Participants were divided regarding the value of explainability, as promoted by recent guidelines for ethical artificial intelligence, and highlighted that explainability may be used as an ethical fig leaf to cover shortfalls in data acquisition. Experts recommended increased attention to machine learning methodology, and the education of physicians as first steps towards a potential use of machine learning systems in psychiatry. Our findings stress the need for domain-specific ethical research, scrutinizing the use of machine learning in different medical specialties. Critical ethical research should further examine the value of explainability for an ethical development of machine learning systems and strive towards an appropriate framework to communicate ML-based medical predictions.","https://link.springer.com/content/pdf/10.1007/s43681-022-00177-1.pdf",""
7,"Manuel Wörsdörfer","AI ethics and ordoliberalism 2.0: towards a ‘Digital Bill of Rights’",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00367-5","",48,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00367-5","2730-5953","",,,,,7,3.50,7,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00367-5.pdf",""
7,"Zoe Porter, Ibrahim Habli, John McDermid, Marten Kaas","A principles-based ethics assurance argument pattern for AI and autonomous systems",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00297-2","",208,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00297-2","2730-5953","",4,2,593,616,7,3.50,2,4,2,"Abstract: An assurance case is a structured argument, typically produced by safety engineers, to communicate confidence that a critical or complex system, such as an aircraft, will be","https://link.springer.com/content/pdf/10.1007/s43681-023-00297-2.pdf",""
7,"Paula Sweeney","Trusting social robots",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00165-5","",218,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00165-5","2730-5953","",3,2,419,426,7,2.33,7,1,3,"Abstract: In this paper, I argue that we need a more robust account of our ability and willingness to trust social robots. I motivate my argument by demonstrating that existing accounts of trust and of trusting social robots are inadequate. I identify that it is the feature of a façade or deception inherent in our engagement with social robots that both facilitates, and is in danger of undermining, trust. Finally, I utilise the fictional dualism model of social robots to clarify that trust in social robots, unlike trust in humans, must rely on an independent judgement of product reliability.","https://link.springer.com/content/pdf/10.1007/s43681-022-00165-5.pdf",""
7,"Connor Rees, Berndt Müller","All that glitters is not gold: trustworthy and ethical AI principles",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00232-x","",301,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00232-x","2730-5953","",3,4,1241,1254,7,2.33,4,2,3,"Abstract: Ethics of technology systems have become an area of interest in academic research as well as international policy in recent years. Several organisation have consequently published principles of ethical artificial intelligence (AI) in line with this trend. The documents identify principles, values, and other abstract requirements for AI development and deployment. Critics raise concerns about whether these documents are in fact constructive, or if they are produced as a higher form of virtue signalling. A theme that is beginning to become apparent in the academic literature regarding these documents is the inherent lack of effective and practical methods and processes for producing ethical AI. This article attempts a critical analysis which draws upon ethical AI documents from a range of contexts including company, organisational, governmental, and academic perspectives. Both the theoretical and practical components of AI guidelines are explored and analysed, consequently bringing to light the necessity of introducing a measurable component to such documents for the purpose of ensuring a positive outcome of deploying AI systems based on ethical principles. We propose a minimal framework for stakeholders to develop AI in an ethical and human-centred manner.","https://link.springer.com/content/pdf/10.1007/s43681-022-00232-x.pdf",""
7,"João Figueiredo Nobre Brito Cortese, Fabio Gagliardi Cozman, Marcos Paulo Lucca-Silveira, Adriano Figueiredo Bechara","Should explainability be a fifth ethical principle in AI ethics?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00152-w","",310,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00152-w","2730-5953","",3,1,123,134,7,2.33,2,4,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00152-w.pdf",""
7,"Declan Humphreys, Abigail Koay, Dennis Desmond, Erica Mealy","AI hype as a cyber security risk: the moral responsibility of implementing generative AI in business",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00443-4","",372,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00443-4","2730-5953","",4,3,791,804,7,7.00,2,4,1,"Abstract: This paper examines the ethical obligations companies have when implementing generative Artificial Intelligence (AI). We point to the potential cyber security risks companies are exposed to when rushing to adopt generative AI solutions or buying into “AI hype”. While the benefits of implementing generative AI solutions for business have been widely touted, the inherent risks associated have been less well publicised. There are growing concerns that the race to integrate generative AI is not being accompanied by adequate safety measures. The rush to buy into the hype of generative AI and not fall behind the competition is potentially exposing companies to broad and possibly catastrophic cyber-attacks or breaches. In this paper, we outline significant cyber security threats generative AI models pose, including potential ‘backdoors’ in AI models that could compromise user data or the risk of ‘poisoned’ AI models producing false results. In light of these the cyber security concerns, we discuss the moral obligations of implementing generative AI into business by considering the ethical principles of beneficence, non-maleficence, autonomy, justice, and explicability. We identify two examples of ethical concern,","https://link.springer.com/content/pdf/10.1007/s43681-024-00443-4.pdf",""
7,"Alesia Zhuk","Navigating the legal landscape of AI copyright: a comparative analysis of EU, US, and Chinese approaches",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00299-0","",377,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00299-0","2730-5953","",4,4,1299,1306,7,3.50,7,1,2,"Abstract: This paper compares AI copyright approaches in the EU, US, and China, evaluating their effectiveness and challenges. It examines the recognition of AI-generated works as copyrightable and the exclusive rights of copyright owners to reproduce, distribute, publicly display, and perform such works. Differences in approaches, such as recognizing AI as a sui generis right holder in the EU and the broad fair use doctrine in the US, are highlighted. This paper evaluates strengths and weaknesses of each approach, including enforcement and ownership of copyright in AI-generated works, and clarifies issues related to AI and copyright. While the EU and US have more developed legal frameworks for AI copyright than China, all three approaches face challenges that need addressing. This paper concludes by providing insight into the legal landscape of AI copyright and steps necessary for effective protection and use of AI-generated works.","https://link.springer.com/content/pdf/10.1007/s43681-023-00299-0.pdf",""
7,"Avinash Agarwal, Harsh Agarwal","A seven-layer model with checklists for standardising fairness assessment throughout the AI lifecycle",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00266-9","",387,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00266-9","2730-5953","",4,2,299,314,7,3.50,4,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00266-9.pdf",""
7,"Abraham Kuuku Sam, Philipp Olbrich","The Need for AI Ethics in Higher Education",2023,"SpringerBriefs in Ethics","Springer International Publishing","https://doi.org/10.1007/978-3-031-23035-6_1","",479,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-23035-6_1","2211-8101","",,,3,10,7,3.50,4,2,2,"Abstract: Business leaders, policymakers and technologists regularly portray Artificial Intelligence (AI) as an easy way to make sense of an increasingly complex world. Unsurprisingly, AI plays a central role in strategy papers, TED talks and speeches about the future of mobility.","https://link.springer.com/content/pdf/10.1007/978-3-031-23035-6_1",""
7,"Bernd Carsten Stahl, Tonii Leach","Assessing the ethical and social concerns of artificial intelligence in neuroinformatics research: an empirical test of the European Union Assessment List for Trustworthy AI (ALTAI)",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00201-4","",673,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00201-4","2730-5953","",3,3,745,767,7,2.33,4,2,3,"Abstract: Ethical and social concerns are a key obstacle to the adoption of artificial intelligence (AI) in the life sciences and beyond. The discussion of these issues has intensified in recent years and led to a number of approaches, tools and initiatives. Key amongst them is the idea of ex-ante impact assessments that aim to identify issues at the early stages of development. One prominent example of such ex-ante impact assessment is the European Union's (EU) Assessment list for Trustworthy AI (ALTAI). This article uses the findings of a large-scale application of the ALTAI to a large neuro-informatics project as an exemplar to demonstrate the effectiveness and limitations of the ALTAI in practice. The article shows that ex-ante impact assessment has the potential to help identify and address ethical and social issues. However, they need to be understood as part of a broader socio-technical ecosystem of AI. For ALTAI and related approaches to be useful in bio-medical research, they should be interpreted from a systems theory perspective which allows for their integration into the rich set of tools, legislation and approaches. The paper argues that ex-ante impact assessments have the best chance of being successful if seen applied in conjunction with other approaches in the context of the overall AI ecosystem.","https://link.springer.com/content/pdf/10.1007/s43681-022-00201-4.pdf",""
7,"Jack McGuire, David De Cremer","Algorithms, leadership, and morality: why a mere human effect drives the preference for human over algorithmic leadership",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00192-2","",787,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00192-2","2730-5953","",3,2,601,618,7,2.33,4,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00192-2.pdf",""
7,"Ilse Verdiesen, Virginia Dignum","Value elicitation on a scenario of autonomous weapon system deployment: a qualitative study based on the value deliberation process",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00211-2","",824,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00211-2","2730-5953","",3,3,887,900,7,2.33,4,2,3,"Abstract: Ethical concerns on autonomous weapon systems (AWS) call for a process of human oversight to ensure accountability over targeting decisions and the use of force. To align the behavior of autonomous systems with human values and norms, the Design for Values approach can be used to consciously embody values in the deployment of AWS. One instrument for the elicitation of values during the design is participative deliberation. In this paper, we describe a participative deliberation method and results of a value elicitation by means of the value deliberation process for which we organized two panels each consisting of a mixture of experts in the field of AWS working in military operations, foreign policy, NGO’s and industry. The results of our qualitative study indicate not only that value discussion leads to changes in perception of the acceptability of alternatives, or options, in a scenario of AWS deployment, it also gives insight in to which values are deemed important and highlights that trust in the decision-making of an AWS is crucial.","https://link.springer.com/content/pdf/10.1007/s43681-022-00211-2.pdf",""
7,"Ana Valdivia, Júlia Corbera Serrajòrdia, Aneta Swianiewicz","There is an elephant in the room: towards a critique on the use of fairness in biometrics",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00249-2","",861,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00249-2","2730-5953","",3,4,1407,1422,7,2.33,2,3,3,"Abstract: The proliferation of biometric systems in our societies is shaping public debates around its political, social and ethical implications. Yet, whilst concerns towards the racialised use of this technology have been on the rise, the field of biometrics remains unperturbed by these debates. Despite the lack of critical analysis, algorithmic fairness has recently been adopted by biometrics. Different studies have been published to understand and mitigate demographic bias in biometric systems, without analysing the political consequences. In this paper, we offer a critical reading of recent debates about biometric fairness and show its detachment from political debates. Building on previous fairness demonstrations, we prove that biometrics will be always biased. Yet, we claim algorithmic fairness cannot distribute justice in scenarios which are broken or whose intended purpose is to discriminate. By focusing on demographic biases rather than examine how these systems reproduce historical and political injustices, fairness has overshadowed the elephant in the room of biometrics.","https://link.springer.com/content/pdf/10.1007/s43681-022-00249-2.pdf",""
6,"Danilo Bruschi, Nicla Diomede","A framework for assessing AI ethics with applications to cybersecurity",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00162-8","",96,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00162-8","2730-5953","",3,1,65,72,6,2.00,3,2,3,"Abstract: In the last few years many scholars, public and private organizations have been involved in the definition of guidelines and frameworks for individuating the principles to adopt in the development and deployment of AI systems. Some authors, however, noted that the effectiveness of these guidelines or ethical codes on the developer’s community is very marginal. One of the obstacles that opposes to the effective implementation of ethical principles is the lack of an approach for solving tensions which arise when principles are applied. A possible solution to such an issue could be the adoption of a risk-based approach which is also advocated by many sources. To our knowledge, no concrete proposals have been presented in literature on how to perform a risk-based ethical assessment. In this paper we contribute to close this gap by introducing a framework based on a qualitative risk analysis approach for assessing the ethical impact underneath the introduction of an innovation either technological or organizational in a system. We will also show how the framework can be used for individuating suitable safeguards to adopt for balancing potential ethical infringements that the innovation may entail once implemented. Some case studies in the cybersecurity context are also described for showing the effectiveness of our approach.","https://link.springer.com/content/pdf/10.1007/s43681-022-00162-8.pdf",""
6,"Adriana Placani","Anthropomorphism in AI: hype and fallacy",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00419-4","",114,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00419-4","2730-5953","",4,3,691,698,6,6.00,6,1,1,"Abstract: This essay focuses on anthropomorphism as both a form of hype and fallacy. As a form of hype, anthropomorphism is shown to exaggerate AI capabilities and performance by attributing human-like traits to systems that do not possess them. As a fallacy, anthropomorphism is shown to distort moral judgments about AI, such as those concerning its moral character and status, as well as judgments of responsibility and trust. By focusing on these two dimensions of anthropomorphism in AI, the essay highlights negative ethical consequences of the phenomenon in this field.","https://link.springer.com/content/pdf/10.1007/s43681-024-00419-4.pdf",""
6,"Masike Malatji, Alaa Tolah","Artificial intelligence (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00427-4","",125,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00427-4","2730-5953","",,,,,6,6.00,3,2,1,"Abstract: As Artificial Intelligence (AI) rapidly advances and integrates into various domains, cybersecurity emerges as a critical field grappling with both the benefits and pitfalls of AI technologies. This paper explores the multifaceted dimensions of AI-driven cyberattacks, offering insights into their implications, mitigation strategies, underlying motivations, and profound societal impacts. The research centres on developing and presenting the AI Cybersecurity Dimensions (AICD) Framework, a comprehensive, multidimensional schema designed to guide academics, policymakers, and industry professionals in understanding and combating the evolving challenges posed by AI-driven cyber threats. The research unveils the complex dynamics of offensive AI, stressing the need for adaptive defences and ethical considerations. Concurrently, the study highlights adversarial AI threats, calling for proactive measures to address their potential ramifications. Through rigorous textual analyses and extensive literature reviews, the paper underscores the urgency for interdisciplinary approaches to bridge the technology-humanity chasm traditionally observed in cybersecurity discussions. By synthesising these diverse elements, the AICD Framework emerges as an instrumental tool for holistic understanding and practical interventions in the AI-infused cybersecurity landscape. The paper concludes with an urgent call for collaborative efforts in research and practice to navigate the intricate challenges and capitalise on the opportunities borne from the convergence of AI and cybersecurity.","https://link.springer.com/content/pdf/10.1007/s43681-024-00427-4.pdf",""
6,"Henrik Skaug Sætra, John Danaher","Resolving the battle of short- vs. long-term AI risks",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00336-y","",170,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00336-y","2730-5953","",,,,,6,3.00,3,2,2,"Abstract: AI poses both short- and long-term risks, but the AI ethics and regulatory communities are struggling to agree on how to think two thoughts at the same time. While disagreements over the exact probabilities and impacts of risks will remain, fostering a more productive dialogue will be important. This entails, for example, distinguishing between evaluations of particular risks and the politics of risk. Without proper discussions of AI risk, it will be difficult to properly manage them, and we could end up in a situation where neither short- nor long-term risks are managed and mitigated.","https://link.springer.com/content/pdf/10.1007/s43681-023-00336-y.pdf",""
6,"Maksim Karliuk","Proportionality principle for the ethics of artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00220-1","",176,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00220-1","2730-5953","",3,3,985,990,6,2.00,6,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00220-1.pdf",""
6,"Alessandra Buccella","“AI for all” is a matter of social justice",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00222-z","",192,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00222-z","2730-5953","",3,4,1143,1152,6,2.00,6,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00222-z.pdf",""
6,"Caitlin Curtis, Nicole Gillespie, Steven Lockey","AI-deploying organizations are key to addressing ‘perfect storm’ of AI risks",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00163-7","",231,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00163-7","2730-5953","",3,1,145,153,6,2.00,2,3,3,"Abstract: We argue that a perfect storm of five conditions heightens the risk of harm to society from artificial intelligence: (1) the powerful, invisible nature of AI, (2) low public awareness and AI literacy, (3) rapid scaled deployment of AI, (4) insufficient regulation, and (5) the gap between trustworthy AI principles and practices. To prevent harm, fit-for-purpose regulation and public AI literacy programs have been recommended, but education and government regulation will not be sufficient: AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and taking accountability to mitigate the risks.","https://link.springer.com/content/pdf/10.1007/s43681-022-00163-7.pdf",""
6,"Martin Gibert","The case for virtuous robots",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00185-1","",281,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00185-1","2730-5953","",3,1,135,144,6,2.00,6,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00185-1.pdf",""
6,"Leying Zou, Warut Khern-am-nuai","AI and housing discrimination: the case of mortgage applications",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00234-9","",290,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00234-9","2730-5953","",3,4,1271,1281,6,2.00,3,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00234-9.pdf",""
6,"Jeroen K. G. Hopster, Matthijs M. Maas","The technology triad: disruptive AI, regulatory gaps and value change",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00305-5","",348,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00305-5","2730-5953","",4,4,1051,1069,6,3.00,3,2,2,"Abstract: Disruptive technologies can have far-reaching impacts on society. They may challenge or destabilize cherished ethical values and disrupt legal systems. There is a convergent interest among ethicists and legal scholars in such “second-order disruptions” to norm systems. Thus far, however, ethical and legal approaches to technological norm-disruption have remained largely siloed. In this paper, we propose to integrate the existing ‘dyadic’ models of disruptive change in the ethical and legal spheres, and shift focus to the relations between and mutual shaping of values, technology, and law. We argue that a ‘triadic’ values-technology-regulation model—“the technology triad”—is more descriptively accurate, as it allows a better mapping of second-order impacts of technological changes (on values and norms, through changes in legal systems—or on legal systems, through changes in values and norms). Simultaneously, a triadic model serves to highlight a broader portfolio of ethical, technical, or regulatory interventions that can enable effective ethical triage of—and a more resilient response to—such Socially Disruptive Technologies. We illustrate the application of the triadic framework with two cases, one historical (how the adoption of the GDPR channeled and redirected the evolution of the ethical value of ‘privacy’ when that had been put under pressure by digital markets), and one anticipatory (looking at anticipated disruptions caused by the ongoing wave of generative AI systems).","https://link.springer.com/content/pdf/10.1007/s43681-023-00305-5.pdf",""
6,"Scott Robbins","The many meanings of meaningful human control",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00320-6","",359,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00320-6","2730-5953","",4,4,1377,1388,6,3.00,6,1,2,"Abstract: The concept of Meaningful Human Control (MHC) has gained prominence in the field of Artificial Intelligence ethics. MHC is discussed in relation to lethal autonomous weapons, autonomous cars, and more recently, AI systems in general. Little, however, has been done to analyze the concept. Those using MHC tend to look at it narrowly and intuitively—as if it is clear what it means. They fail to see the many issues concerning human control over machines. In this article, I break the concept into its three constitutive words (‘meaningful’, ‘human’, and, ‘control’) to outline the many meanings of MHC. While the intention is not to come to the","https://link.springer.com/content/pdf/10.1007/s43681-023-00320-6.pdf",""
6,"Merve Hickok, Nestor Maslej","A policy primer and roadmap on AI worker surveillance and productivity scoring tools",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00275-8","",369,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00275-8","2730-5953","",3,3,673,687,6,3.00,3,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00275-8.pdf",""
6,"Jan-Christoph Heilinger, Hendrik Kempt, Saskia Nagel","Beware of sustainable AI! Uses and abuses of a worthy goal",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00259-8","",388,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00259-8","2730-5953","",4,2,201,212,6,3.00,2,3,2,"Abstract: The ethical debate about technologies called artificial intelligence (AI) has recently turned towards the question whether and in which sense using AI can be sustainable, distinguishing possible contributions of AI to achieve the end of sustainability on the one hand from the sustainability of AI and its underlying technologies as means on the other hand. This important distinction is both applied in the context of environmental as well as social sustainability. However, further elaboration is necessary to capture the complexities of sustainability assessments in the context of AI. To this end, our analysis of the ends and means of “sustainable AI” in social and environmental contexts leads to a matrix of four dimensions reflecting its social and its environmental impact and costs. This matrix avoids overly narrow, one-dimensional assessments that too quickly label some AI-based technology as sustainable. While a selective assessment can, at best, warrant the narrower verdict of “thin” sustainability, only such a comprehensive assessment can warrant the verdict of what we call “thick” sustainability. In consequence, we recommend to broaden the normative scope in considering the ethics and justice of AI and to use the notion “sustainability” more carefully and sparingly, and to pursue the more ambitious goal of “thick” sustainability of AI-based technologies to meaningfully contribute to actual improvements of human lives and living together. Current conditions of an economy oriented towards permanent growth, however, may make it difficult or even impossible to realise sustainable AI.","https://link.springer.com/content/pdf/10.1007/s43681-023-00259-8.pdf",""
6,"Kristin Undheim, Truls Erikson, Bram Timmermans","True uncertainty and ethical AI: regulatory sandboxes as a policy tool for moral imagination",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00240-x","",439,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00240-x","2730-5953","",3,3,997,1002,6,2.00,2,3,3,"Abstract: We offer a complementary view to the ethical dilemmas discussed in the recent literature by pointing at the epistemological dilemma of true uncertainty, suggesting regulatory sandboxes as an apposite remedy. Using the exemplar case of the regulative sandbox for responsible artificial intelligence established by the Norwegian data protection authorities, we argue that regulative sandboxes have the potential of supporting the development of a more ethical AI through not only reducing uncertainty, but through nurturing moral imaginations.","https://link.springer.com/content/pdf/10.1007/s43681-022-00240-x.pdf",""
6,"Stefanie Meyer, Sarah Mandl, Dagmar Gesmann-Nuissl, Anja Strobel","Responsibility in Hybrid Societies: concepts and terms",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00184-2","",601,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00184-2","2730-5953","",3,1,25,48,6,2.00,2,4,3,"Abstract: With increased digitalization and new technologies, societies are expected to no longer only include human actors, but artificial actors as well. Such a future of societies raises new questions concerning the coexistence, tasks and responsibilities of different actors. Manifold disciplines are involved in the creation of these future societies. This requires a common understanding of responsibility, and of definitions of actors in Hybrid Societies. This review aims at clarifying aforementioned terms from a legal and psychological perspective. Building from this common ground, we identified seven capacities in total which need to be met by actors in societies to be considered fully responsible, in both a legal and moral sense. From a legal perspective, actors need to be autonomous, have capacity to act, legal capacity, and the ability to be held liable. From a psychological perspective, actors need to possess moral agency and can be trusted. Both disciplines agree that explainability is a pivotal capacity to be considered fully responsible. As of now, human beings are the only actors who can, with regard to these capacities, be considered morally and legally responsible. It is unclear whether and to which extent artificial entities will have these capacities, and subsequently, whether they can be responsible in the same sense as human beings are. However, on the basis of the conceptual clarification, further steps can now be taken to develop a concept of responsibility in Hybrid Societies.","https://link.springer.com/content/pdf/10.1007/s43681-022-00184-2.pdf",""
6,"Simon Friederich","Symbiosis, not alignment, as the goal for liberal democracies in the transition to artificial general intelligence",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00268-7","",646,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00268-7","2730-5953","",4,2,315,324,6,3.00,6,1,2,"Abstract: A transition to a world with artificial general intelligence (AGI) may occur within the next few decades. This transition may give rise to catastrophic risks from","https://link.springer.com/content/pdf/10.1007/s43681-023-00268-7.pdf",""
6,"Fouad Leboukh, Emmanuel Baba Aduku, Omar Ali","Balancing ChatGPT and Data Protection in Germany: Challenges and Opportunities for Policy Makers",2023,"Journal of Politics and Ethics in New Technologies and AI","National Documentation Centre (EKT)","https://doi.org/10.12681/jpentai.35166","",662,"2025-02-04 16:55:17","journal-article","10.12681/jpentai.35166","2944-9243","",2,1,,,6,3.00,2,3,2,"In the last few months there has been widespread discussion about the remarkable progress made in the field of artificial intelligence, specifically large language models such as ""ChatGPT"". The ethical implications of AI, particularly concerning data protection, have sparked discussions on the necessity of robust regulations. This article examines the intersection of data protection, ChatGPT, and the ethics of AI, it explores Germany's ongoing efforts to strike a balance between harnessing the potential of large language models as ChatGPT and ensuring responsible and transparent use of AI technology in the policy-making realm. The GDPR serves as a guiding framework, necessitating careful consideration of privacy rights and secure handling of personal data when deploying ChatGPT in Germany's policy-making processes. The study draws on analysis on the current laws and regulations of data protection in Germany while studying Germany's commitment to safeguarding personal information through the active presence of The German Federal Commissioner for Data Protection and Freedom of Information. The first section provides a context and presents the policy problem. The second section looks at the available policy options on the role of policymaking in establishing comprehensive regulations regarding the use of ChatGPT and generative AI. The third section provides recommendations on how Germany can ensure the responsible management of ChatGPT, through strengthening data protection laws and regulations, simultaneously, restricting ChatGPT usage to private users and government, also, embracing appropriate usage of generative AI while developing ethical guidelines and best practices to harness its benefits, fostering innovation and advancement.","https://ejournals.epublishing.ekt.gr/index.php/jpentai/article/download/35166/26620",""
6,"Maria Joseph Israel, Ahmed Amer","Rethinking data infrastructure and its ethical implications in the face of automated digital content generation",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00169-1","",707,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00169-1","2730-5953","",3,2,427,439,6,2.00,3,2,3,"Abstract: Recent AI developments have made it possible for AI to auto-generate content—text, image, and sound. Highly realistic auto-generated content raises the question of whether one can differentiate between what is AI-generated and human-generated, and assess its origin and authenticity. When it comes to the processes of digital scholarship and publication in the presence of automated content generation technology, the evolution of data storage and presentation technologies demand that we rethink basic processes, such as the nature of anonymity and the mechanisms of attribution. We propose to consider these issues in light of emerging digital storage technologies that may better support the mechanisms of attribution (and fulfilling broader goals of accountability, transparency, and trust). We discuss the scholarship review and publication process in a revised context, specifically the possibility of synthetically generated content and the availability of a digital storage infrastructure that can track data provenance while offering: immutability of stored data; accountability and attribution of authorship; and privacy-preserving authentication mechanisms. As an example, we consider the","https://link.springer.com/content/pdf/10.1007/s43681-022-00169-1.pdf",""
6,"Ramón Alvarado","AI as an Epistemic Technology",2023,"Science and Engineering Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s11948-023-00451-3","",761,"2025-02-04 16:55:17","journal-article","10.1007/s11948-023-00451-3","1353-3452","",29,5,,,6,3.00,6,1,2,"","https://link.springer.com/content/pdf/10.1007/s11948-023-00451-3.pdf",""
6,"Emre Kazim, Osman Güçlütürk, Denise Almeida, Charles Kerrigan, Elizabeth Lomas, Adriano Koshiyama, Airlie Hilliard, Markus Trengove","Proposed EU AI Act—Presidency compromise text: select overview and comment on the changes to the proposed regulation",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00179-z","",792,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00179-z","2730-5953","",3,2,381,387,6,2.00,1,8,3,"Abstract: With its proposed EU AI Act, the EU is aspiring to lead the world in admiral AI regulation (April 2021). In this brief, we summarise and comment on the ‘Presidency compromise text’, which is a revised version of the proposed act reflecting the consultation and deliberation by member states and actors (November 2021). The compromise text echoes the sentiment of the original text, much of which remains largely unchanged. However, there are important shifts and some significant changes. Our main comments focus on exemptions to the act with respect to national security; changes that seek to further protect research, development and innovation; and the attempt to clarify the draft legislation’s stance on algorithmic manipulation. Our target readership for this paper is those who are interested in tracking the evolution of the proposed EU AI act, such as policy-makers and those in the legal profession.","https://link.springer.com/content/pdf/10.1007/s43681-022-00179-z.pdf",""
6,"Imdad Ali Shah, Noor Zaman Jhanjhi, Raja Majid Ali Ujjan","Use of AI Applications for the Drone Industry",2024,"Advances in Information Security, Privacy, and Ethics","IGI Global","https://doi.org/10.4018/979-8-3693-0774-8.ch002","",927,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-0774-8.ch002","1948-9730","",,,27,41,6,6.00,2,3,1,"The unmanned aerial vehicle (UAV) industry, commonly referred to as the drone industry, has grown rapidly in recent years and changed many industries' operational procedures. Drones are adaptable AUs that have the ability to operate independently or remotely. The drone business has developed into a vibrant, diverse sector with applications in many other industries. Drone technology is set to grow and become more integrated into daily life and corporate operations as long as regulations keep up with technological advancements. Artificial intelligence (AI) technologies are increasingly used in various industries, notably drone companies. AI can improve drone technology's effectiveness, dependability, and efficiency, creating new opportunities for the drone industry to service multiple applications and sectors.","https://www.igi-global.com/viewtitle.aspx?TitleId=340071",""
6,"Davide Calvaresi, Rachele Carli, Jean-Gabriel Piguet, Victor H. Contreras, Gloria Luzzani, Amro Najjar, Jean-Paul Calbimonte, Michael Schumacher","Ethical and legal considerations for nutrition virtual coaches",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00237-6","",988,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00237-6","2730-5953","",3,4,1313,1340,6,2.00,1,8,3,"Abstract: Choices and preferences of individuals are nowadays increasingly influenced by countless inputs and recommendations provided by artificial intelligence-based systems. The accuracy of recommender systems (RS) has achieved remarkable results in several domains, from infotainment to marketing and lifestyle. However, in sensitive use-cases, such as nutrition, there is a need for more complex dynamics and responsibilities beyond conventional RS frameworks. On one hand, virtual coaching systems (VCS) are intended to support and educate the users about food, integrating additional dimensions w.r.t. the conventional RS (i.e., leveraging persuasion techniques, argumentation, informative systems, and recommendation paradigms) and show promising results. On the other hand, as of today, VCS raise unexplored ethical and legal concerns. This paper discusses the need for a clear understanding of the ethical/legal-technological entanglements, formalizing 21 ethical and ten legal challenges and the related mitigation strategies. Moreover, it elaborates on nutrition sustainability as a further nutrition virtual coaches dimension for a better society.","https://link.springer.com/content/pdf/10.1007/s43681-022-00237-6.pdf",""
6,"Juan M Durán, Martin Sand, Karin Jongsma","The ethics and epistemology of explanatory AI in medicine and healthcare",2022,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-022-09666-7","",998,"2025-02-04 16:55:17","journal-article","10.1007/s10676-022-09666-7","1388-1957","",24,4,,,6,2.00,2,3,3,"","https://link.springer.com/content/pdf/10.1007/s10676-022-09666-7.pdf",""
5,"Jess Whittlestone, Samuel Clarke","AI Challenges for Society and Ethics",2022,"The Oxford Handbook of AI Governance","Oxford University Press","https://doi.org/10.1093/oxfordhb/9780197579329.013.3","",14,"2025-02-04 16:55:17","book-chapter","10.1093/oxfordhb/9780197579329.013.3","","",,,45,64,5,1.67,3,2,3,"Abstract: Artificial intelligence is already being applied in and impacting many important sectors in society, including healthcare, finance, and policing. These applications will increase as AI capabilities continue to progress, which has the potential to be highly beneficial for society, or to cause serious harm. The role of AI governance is ultimately to take practical steps to mitigate this risk of harm while enabling the benefits of innovation in AI. This requires answering challenging empirical questions about current and potential risks and benefits of AI: assessing impacts that are often widely distributed and indirect, and making predictions about a highly uncertain future. It also requires thinking through the normative question of what beneficial use of AI in society looks like, which is equally challenging. Though different groups may agree on high-level principles that uses of AI should respect (e.g., privacy, fairness, and autonomy), challenges arise when putting these principles into practice. For example, it is straightforward to say that AI systems must protect individual privacy, but there is presumably some amount or type of privacy that most people would be willing to give up to develop life-saving medical treatments. Despite these challenges, research can and has made progress on these questions. The aim of this chapter will be to give readers an understanding of this progress, and of the challenges that remain.","https://academic.oup.com/edited-volume/chapter-pdf/57825637/book_41989_section_355436576.ag.pdf",""
5,"Vaishak Belle","Knowledge representation and acquisition for ethical AI: challenges and opportunities",2023,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-023-09692-z","",32,"2025-02-04 16:55:17","journal-article","10.1007/s10676-023-09692-z","1388-1957","",25,1,,,5,2.50,5,1,2,"Abstract: Machine learning (ML) techniques have become pervasive across a range of different applications, and are now widely used in areas as disparate as recidivism prediction, consumer credit-risk analysis, and insurance pricing. Likewise, in the physical world, ML models are critical components in autonomous agents such as robotic surgeons and self-driving cars. Among the many ethical dimensions that arise in the use of ML technology in such applications, analyzing morally permissible actions is both immediate and profound. For example, there is the potential for learned algorithms to become biased against certain groups. More generally, in so much that the decisions of ML models impact society, both virtually (e.g., denying a loan) and physically (e.g., driving into a pedestrian), notions of accountability, blame and responsibility need to be carefully considered. In this article, we advocate for a two-pronged approach ethical decision-making enabled using rich models of autonomous agency: on the one hand, we need to draw on philosophical notions of such as beliefs, causes, effects and intentions, and look to formalise them, as attempted by the knowledge representation community, but on the other, from a computational perspective, such theories need to also address the problems of tractable reasoning and (probabilistic) knowledge acquisition. As a concrete instance of this tradeoff, we report on a few preliminary results that apply (propositional) tractable probabilistic models to problems in fair ML and automated reasoning of moral principles. Such models are compilation targets for certain types of knowledge representation languages, and can effectively reason in service some computational tasks. They can also be learned from data. Concretely, current evidence suggests that they are attractive structures for jointly addressing three fundamental challenges: reasoning about possible worlds + tractable computation + knowledge acquisition. Thus, these seems like a good starting point for modelling reasoning robots as part of the larger ecosystem where accountability and responsibility is understood more broadly.","https://link.springer.com/content/pdf/10.1007/s10676-023-09692-z.pdf",""
5,"Emma Ruttkamp-Bloem","Epistemic Just and Dynamic AI Ethics in Africa",2023,"Social and Cultural Studies of Robots and AI","Springer International Publishing","https://doi.org/10.1007/978-3-031-08215-3_2","",45,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-08215-3_2","2523-8523","",,,13,34,5,2.50,5,1,2,"Abstract: This chapter considers the potential for actualising the ideal for responsible AI on the African continent, focusing on the AI ethics policy environment in Africa. I consider the impact of context and culture on successful adoption of AI technologies in general and on trust in AI technology and openness to AI regulation in particular. It concludes that actionable AI ethics in Africa should be driven by dynamic and epistemic just ethical systems.","https://link.springer.com/content/pdf/10.1007/978-3-031-08215-3_2",""
5,"Jonas Schuett, Ann-Katrin Reuel, Alexis Carlier","How to design an AI ethics board",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00409-y","",57,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00409-y","2730-5953","",,,,,5,5.00,2,3,1,"Abstract: The development and deployment of artificial intelligence (AI) systems poses significant risks to society. To reduce these risks to an acceptable level, AI companies need an effective risk management process and sound risk governance. In this paper, we explore a particular way in which AI companies can improve their risk governance: by setting up an AI ethics board. We identify five key design choices: (1) What responsibilities should the board have? (2) What should its legal structure be? (3) Who should sit on the board? (4) How should it make decisions? (5) And what resources does it need? We break each of these questions down into more specific sub-questions, list options, and discuss how different design choices affect the board’s ability to reduce societal risks from AI. Several failures have shown that designing an AI ethics board can be challenging. This paper provides a toolbox that can help AI companies to overcome these challenges.","https://link.springer.com/content/pdf/10.1007/s43681-023-00409-y.pdf",""
5,"Manuel Wörsdörfer","The E.U.’s artificial intelligence act: an ordoliberal assessment",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00337-x","",240,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00337-x","2730-5953","",,,,,5,2.50,5,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00337-x.pdf",""
5,"Travis LaCroix","Moral dilemmas for moral machines",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00134-y","",272,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00134-y","2730-5953","",2,4,737,746,5,1.67,5,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00134-y.pdf",""
5,"Dina Babushkina","Are we justified attributing a mistake in diagnosis to an AI diagnostic system?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00189-x","",295,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00189-x","2730-5953","",3,2,567,584,5,1.67,5,1,3,"Abstract: Responsible professional use of AI implies the readiness to respond to and address—in ethically appropriate manner—harm that may be associated with such use. This presupposes the ownership of mistakes. In this paper, I ask if a mistake in AI-enhanced decision making—such as AI-aided medical diagnosis—can be attributed to the AI system itself, and answer this question negatively. I will explore two options. If AI systems are merely tools, then we are never justified to attribute mistakes to them, because their failing does not meet rational constraints on being mistaken. If, for the sake of the argument, we assume that AI systems are not (mere) tools, then we are faced with certain challenges. The first is the burden to explain what this more-than-a-tool role of an AI system is, and to establish justificatory reasons for the AI system to be considered as such. The second is to prove that medical diagnosis can be reduced to the calculations by AI system without any significant loss to the purpose and quality of the diagnosis as a procedure. I will conclude that the problem of the ownership of mistakes in hybrid decision making necessitates new forms of epistemic responsibilities.","https://link.springer.com/content/pdf/10.1007/s43681-022-00189-x.pdf",""
5,"Nyu Wang, Michael Yuan Tian","“Intelligent Justice”: human-centered considerations in China’s legal AI transformation",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00202-3","",306,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00202-3","2730-5953","",3,2,349,354,5,1.67,3,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00202-3.pdf",""
5,"Edmund Terem Ugar, Ntsumi Malele","Designing AI for mental health diagnosis: challenges from sub-Saharan African value-laden judgements on mental health disorders",2024,"Journal of Medical Ethics","BMJ","https://doi.org/10.1136/jme-2023-109711","",309,"2025-02-04 16:55:17","journal-article","10.1136/jme-2023-109711","0306-6800","",50,9,592,595,5,5.00,3,2,1,"Recently clinicians have become more reliant on technologies such as artificial intelligence (AI) and machine learning (ML) for effective and accurate diagnosis and prognosis of diseases, especially mental health disorders. These remarks, however, apply primarily to Europe, the USA, China and other technologically developed nations. Africa is yet to leverage the potential applications of AI and ML within the medical space. Sub-Saharan African countries are currently disadvantaged economically and infrastructure-wise. Yet precisely, these circumstances create significant opportunities for the deployment of medical AI, which has already been deployed in some places in the continent. However, while AI and ML have come with enormous promises in Africa, there are still challenges when it comes to successfully applying AI and ML designed elsewhere within the African context, especially in diagnosing mental health disorders. We argue, in this paper, that there ought not to be a homogeneous/generic design of AI and ML used in diagnosing mental health disorders. Our claim is grounded on the premise that mental health disorders cannot be diagnosed solely on ‘factual evidence’ but on both factual evidence and value-laden judgements of what constitutes mental health disorders in sub-Saharan Africa. For ML to play a successful role in diagnosing mental health disorders in sub-Saharan African medical spaces, with a precise focus on South Africa, we allude that it ought to understand what sub-Saharan Africans consider as mental health disorders, that is, the value-laden judgements of some conditions.","https://syndication.highwire.org/content/doi/10.1136/jme-2023-109711",""
5,"J. Krijger, T. Thuis, M. de Ruiter, E. Ligthart, I. Broekman","The AI ethics maturity model: a holistic approach to advancing ethical data science in organizations",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00228-7","",317,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00228-7","2730-5953","",3,2,355,367,5,1.67,1,5,3,"Abstract: The field of AI ethics has advanced considerably over the past years, providing guidelines, principles, and technical solutions for enhancing the ethical development, deployment and usage of AI. However, there is still a clear need for research that facilitates the move from the ‘what’ of AI ethics to the ‘how’ of governance and operationalization. Although promising literature on the challenge of implementation is increasingly more common, so far no systemic analysis has been published that brings the various themes of operationalization together in a way that helps the gradual advancement of AI ethics procedures within organizations. In this opinion paper we therefore set out to provide a holistic maturity framework in the form of an AI ethics maturity model comprising six crucial dimensions for the operationalization of AI ethics within an organization. We contend that advancing AI ethics in practice is a multi-dimensional effort, as successful operationalization of ethics requires combined action on various dimensions. The model as presented is a preliminary result of literature analysis complemented with insights from several practical mutual learning sessions with some of the major public, private and research organizations of the Netherlands. The article contributes to the AI ethics literature and practice by synthesizing relevant aspects of operationalization and relating these to the praxis of AI in a maturity model that provides direction for organizations seeking to implement these ethical principles.","https://link.springer.com/content/pdf/10.1007/s43681-022-00228-7.pdf",""
5,"Stefan Buijsman","Navigating fairness measures and trade-offs",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00318-0","",324,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00318-0","2730-5953","",4,4,1323,1334,5,2.50,5,1,2,"Abstract: To monitor and prevent bias in AI systems, we can use a wide range of (statistical) fairness measures. However, it is mathematically impossible to optimize all of these measures at the same time. In addition, optimizing a fairness measure often greatly reduces the accuracy of the system (Kozodoi et al., Eur J Oper Res 297:1083–1094, 2022). As a result, we need a substantive theory that informs us how to make these decisions and for what reasons. I show that by using Rawls’ notion of justice as fairness, we can create a basis for navigating fairness measures and the accuracy trade-off. In particular, this leads to a principled choice focusing on both the most vulnerable groups and the type of fairness measure that has the biggest impact on that group. This also helps to close part of the gap between philosophical accounts of distributive justice and the fairness literature that has been observed by (Kuppler et al. Distributive justice and fairness metrics in automated decision-making: How much overlap is there? arXiv preprint","https://link.springer.com/content/pdf/10.1007/s43681-023-00318-0.pdf",""
5,"Yuko Ikkatai, Tilman Hartwig, Naohiro Takanashi, Hiromi M. Yokoyama","Segmentation of ethics, legal, and social issues (ELSI) related to AI in Japan, the United States, and Germany",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00207-y","",363,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00207-y","2730-5953","",3,3,827,843,5,1.67,1,4,3,"Abstract: Artificial intelligence (AI) is often accompanied by public concern. In this study, we quantitatively evaluated a source of public concern using the framework for ethics, legal, and social issues (ELSI). Concern was compared among people in Japan, the United States, and Germany using four different scenarios: (1) the use of AI to replicate the voice of a famous deceased singer, (2) the use of AI for customer service, (3) the use of AI for autonomous weapons, and (4) the use of AI for preventing criminal activities. The results show that the most striking difference was in the response to the “weapon” scenario. Respondents from Japan showed greater concern than those in the other two countries. Older respondents had more concerns, and respondents who had a deeper understanding of AI were more likely to have concerns related to the legal aspects of it. We also found that attitudes toward legal issues were the key to segmenting their attitudes toward ELSI related to AI: Positive, Less skeptical of laws, Skeptical of laws, and Negative.","https://link.springer.com/content/pdf/10.1007/s43681-022-00207-y.pdf",""
5,"Holli Sargeant","Algorithmic decision-making in financial services: economic and normative outcomes in consumer credit",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00236-7","",570,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00236-7","2730-5953","",3,4,1295,1311,5,1.67,5,1,3,"Abstract: Consider how much data is created and used based on our online behaviours and choices. Converging foundational technologies now enable analytics of the vast data required for machine learning. As a result, businesses now use algorithmic technologies to inform their processes, pricing and decisions. This article examines the implications of algorithmic decision-making in consumer credit markets from economic and normative perspectives. This article fills a gap in the literature to explore a multi-disciplinary approach to framing economic and normative issues for algorithmic decision-making in the private sector. This article identifies optimal and suboptimal outcomes in the relationships between companies and consumers. The economic approach of this article demonstrates that more data allows for more information which may result in better contracting outcomes. However, it also identifies potential risks of inaccuracy, bias and discrimination, and ‘gaming’ of algorithmic systems for personal benefit. Then, this article argues that these economic costs have normative implications. Connecting economic outcomes to a normative analysis contextualises the challenges in designing and regulating ML fairly. In particular, it identifies the normative implications of the process, as much as the outcome, concerning trust, privacy and autonomy and potential bias and discrimination in ML systems. Credit scoring, as a case study, elucidates the issues relating to private companies. Legal norms tend to mirror economic theory. Therefore, this article frames the critical economic and normative issues required for further regulatory work.","https://link.springer.com/content/pdf/10.1007/s43681-022-00236-7.pdf",""
5,"Hutan Ashrafian","Engineering a social contract: Rawlsian distributive justice through algorithmic game theory and artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00253-6","",594,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00253-6","2730-5953","",3,4,1447,1454,5,1.67,5,1,3,"Abstract: The potential for artificial intelligence algorithms and game theory concepts to offer prescriptive and decision-making capability for humankind is increasingly recognized. This derives from the increasing availability of granular, multivariable, well-curated data offering analytical insights for necessarily complex human behaviors and activities. Of the multitude of situations that this decision-making aptitude presents, the application to governmental policy offers a commanding case. This would allow decisions to be made for the benefit of societies and citizens based on rigorous objective information devoid of the traditional approach of choosing policies and societal values based on the opinion of a handful of selected representatives who may be exposed to a lack of comprehensive data analysis capacity and subject to personal biases. There would need to be a critical requirement of wider socially responsible data practices here, beyond those of technical considerations and the incorporation of wider societal fairness approaches. Amongst the schools of political thought particularly acquiescent to the application by this approach would be the egalitarian approach of John Rawls. Here an Original Position’s pre-determination tool of Veil of Ignorance and ensuing Difference Principal presents a method of distributive justice that can be clearly mathematically defined in economics theory through Wald’s Maximin principle. This offers an opportunity to apply algorithmic game theory and artificial intelligence computational approaches to implement Rawlsian distributive justice that are presented and discussed. The outputs from the algorithmic acquaintance of Rawlsian egalitarianism with applicable state data, protected with appropriate privacy, security, legal, ethical and social governance could in turn lead to automated direct governmental choices and an objective Social Contract for citizens of digitally literate nations.","https://link.springer.com/content/pdf/10.1007/s43681-022-00253-6.pdf",""
5,"James Oluwaseyi Hodonu-Wusu","The rise of artificial intelligence in libraries: the ethical and equitable methodologies, and prospects for empowering library users",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00432-7","",633,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00432-7","2730-5953","",,,,,5,5.00,5,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00432-7.pdf",""
5,"W. David Holford","‘Design-for-responsible’ algorithmic decision-making systems: a question of ethical judgement and human meaningful control",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00144-w","",667,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00144-w","2730-5953","",2,4,827,836,5,1.67,5,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00144-w.pdf",""
5,"Sebastian Knell, Markus Rüther","Artificial intelligence, superefficiency and the end of work: a humanistic perspective on meaning in life",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00273-w","",685,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00273-w","2730-5953","",4,2,363,373,5,2.50,3,2,2,"Abstract: How would it be assessed from an ethical point of view if human wage work were replaced by artificially intelligent systems (AI) in the course of an automation process? An answer to this question has been discussed above all under the aspects of individual well-being and social justice. Although these perspectives are important, in this article, we approach the question from a different perspective: that of leading a meaningful life, as understood in analytical ethics on the basis of the so-called meaning-in-life debate. Our thesis here is that a life without wage work loses specific sources of meaning, but can still be sufficiently meaningful in certain other ways. Our starting point is John Danaher’s claim that ubiquitous automation inevitably leads to an achievement gap. Although we share this diagnosis, we reject his provocative solution according to which game-like virtual realities could be an adequate substitute source of meaning. Subsequently, we outline our own systematic alternative which we regard as a decidedly humanistic perspective. It focuses both on different kinds of social work and on rather passive forms of being related to meaningful contents. Finally, we go into the limits and unresolved points of our argumentation as part of an outlook, but we also try to defend its fundamental persuasiveness against a potential objection.","https://link.springer.com/content/pdf/10.1007/s43681-023-00273-w.pdf",""
5,"Matthew J, Dennis, Evgeni Aizenberg","The Ethics of AI in Human Resources",2022,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-022-09653-y","",730,"2025-02-04 16:55:17","journal-article","10.1007/s10676-022-09653-y","1388-1957","",24,3,,,5,1.67,3,2,3,"","https://link.springer.com/content/pdf/10.1007/s10676-022-09653-y.pdf",""
5,"Tina Comes","AI for crisis decisions",2024,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-024-09750-0","",732,"2025-02-04 16:55:17","journal-article","10.1007/s10676-024-09750-0","1388-1957","",26,1,,,5,5.00,5,1,1,"Abstract: Increasingly, our cities are confronted with crises. Fuelled by climate change and a loss of biodiversity, increasing inequalities and fragmentation, challenges range from social unrest and outbursts of violence to heatwaves, torrential rainfall, or epidemics. As crises require rapid interventions that overwhelm human decision-making capacity, AI has been portrayed as a potential avenue to support or even automate decision-making. In this paper, I analyse the specific challenges of AI in urban crisis management as an example and test case for many","https://link.springer.com/content/pdf/10.1007/s10676-024-09750-0.pdf",""
5,"Yifan Xu","Dialogue Explanation With Reasoning for AI",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3539522","",737,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3539522","","",,,,,5,1.67,5,1,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3539522",""
5,"Carolina Villegas-Galaviz, Kirsten Martin","Moral distance, AI, and the ethics of care",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01642-z","",740,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01642-z","0951-5666","",39,4,1695,1706,5,2.50,3,2,2,"Abstract: This paper investigates how the introduction of AI to decision making increases moral distance and recommends the ethics of care to augment the ethical examination of AI decision making. With AI decision making, face-to-face interactions are minimized, and decisions are part of a more opaque process that humans do not always understand. Within decision-making research, the concept of moral distance is used to explain why individuals behave unethically towards those who are not seen. Moral distance abstracts those who are impacted by the decision and leads to less ethical decisions. The goal of this paper is to identify and analyze the moral distance created by AI through both proximity distance (in space, time, and culture) and bureaucratic distance (derived from hierarchy, complex processes, and principlism). We then propose the ethics of care as a moral framework to analyze the moral implications of AI. The ethics of care brings to the forefront circumstances and context, interdependence, and vulnerability in analyzing algorithmic decision making.","https://link.springer.com/content/pdf/10.1007/s00146-023-01642-z.pdf",""
5,"James Johnson","The AI Commander Problem: Ethical, Political, and Psychological Dilemmas of Human-Machine Interactions in AI-enabled Warfare",2022,"Journal of Military Ethics","Informa UK Limited","https://doi.org/10.1080/15027570.2023.2175887","",819,"2025-02-04 16:55:17","journal-article","10.1080/15027570.2023.2175887","1502-7570","",21,3,246,271,5,1.67,5,1,3,"","https://www.tandfonline.com/doi/pdf/10.1080/15027570.2023.2175887",""
5,"Ronny Bogani, Andreas Theodorou, Luca Arnaboldi, Robert H. Wortham","Garbage in, toxic data out: a proposal for ethical artificial intelligence sustainability impact statements",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00221-0","",825,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00221-0","2730-5953","",3,4,1135,1142,5,1.67,1,4,3,"Abstract: Data and autonomous systems are taking over our lives, from healthcare to smart homes very few aspects of our day to day are not permeated by them. The technological advances enabled by these technologies are limitless. However, with advantages so too come challenges. As these technologies encompass more and more aspects of our lives, we are forgetting the ethical, legal, safety and moral concerns that arise as an outcome of integrating our lives with technology. In this work, we study the lifecycle of artificial intelligence from data gathering to deployment, providing a structured analytical assessment of the potential ethical, safety and legal concerns. The paper then presents the foundations for the first ethical artificial intelligence sustainability statement to guide future development of AI in a safe and sustainable manner.","https://link.springer.com/content/pdf/10.1007/s43681-022-00221-0.pdf",""
5,"Simone Casiraghi","Anything new under the sun? Insights from a history of institutionalized AI ethics",2023,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-023-09702-0","",844,"2025-02-04 16:55:17","journal-article","10.1007/s10676-023-09702-0","1388-1957","",25,2,,,5,2.50,5,1,2,"Abstract: Scholars, policymakers and organizations in the EU, especially at the level of the European Commission, have turned their attention to the ethics of (trustworthy and human-centric) Artificial Intelligence (AI). However, there has been little reflexivity on (1) the history of the ethics of AI as an institutionalized phenomenon and (2) the comparison to similar episodes of “ethification” in other fields, to highlight common (unresolved) challenges.","https://link.springer.com/content/pdf/10.1007/s10676-023-09702-0.pdf",""
5,"Erez Firt","Calibrating machine behavior: a challenge for AI alignment",2023,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-023-09716-8","",952,"2025-02-04 16:55:17","journal-article","10.1007/s10676-023-09716-8","1388-1957","",25,3,,,5,2.50,5,1,2,"","https://link.springer.com/content/pdf/10.1007/s10676-023-09716-8.pdf",""
4,"Delaram Rezaeikhonakdar","AI Chatbots and Challenges of HIPAA Compliance for AI Developers and Vendors",2023,"Journal of Law, Medicine &amp; Ethics","Cambridge University Press (CUP)","https://doi.org/10.1017/jme.2024.15","",20,"2025-02-04 16:55:17","journal-article","10.1017/jme.2024.15","1073-1105","",51,4,988,995,4,2.00,4,1,2,"Abstract: Developers and vendors of large language models (“LLMs”) — such as ChatGPT, Google Bard, and Microsoft’s Bing at the forefront—can be subject to Health Insurance Portability and Accountability Act of 1996 (“HIPAA”) when they process protected health information (“PHI”) on behalf of the HIPAA covered entities. In doing so, they become business associates or subcontractors of a business associate under HIPAA.","https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S1073110524000159",""
4,"Didar Zowghi, Muneera Bano","AI for all: Diversity and Inclusion in AI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00485-8","",109,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00485-8","2730-5953","",4,4,873,876,4,4.00,2,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00485-8.pdf",""
4,"Ryan Watkins, Soheil Human","Needs-aware artificial intelligence: AI that ‘serves [human] needs’",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00181-5","",118,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00181-5","2730-5953","",,,,,4,1.33,2,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00181-5.pdf",""
4,"John Danaher, Sven Nyholm","The ethics of personalised digital duplicates: a minimally viable permissibility principle",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00513-7","",177,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00513-7","2730-5953","",,,,,4,4.00,2,2,1,"Abstract: With recent technological advances, it is possible to create personalised digital duplicates. These are partial, at least semi-autonomous, recreations of real people in digital form. Should such duplicates be created? When can they be used? This article develops a general framework for thinking about the ethics of digital duplicates. It starts by clarifying the object of inquiry– digital duplicates themselves– defining them, giving examples, and justifying the focus on them rather than other kinds of artificial being. It then identifies a set of generic harms and benefits associated with digital duplicates and uses this as the basis for formulating a minimally viable permissible principle (MVPP) that stipulates widely agreeable conditions that should be met in order for the creation and use of digital duplicates to be ethically permissible. It concludes by assessing whether it is possible for those conditions to be met in practice, and whether it is possible for the use of digital duplicates to be more or less permissible.","https://link.springer.com/content/pdf/10.1007/s43681-024-00513-7.pdf",""
4,"Guido Löhr","If conceptual engineering is a new method in the ethics of AI, what method is it exactly?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00295-4","",181,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00295-4","2730-5953","",4,2,575,585,4,2.00,4,1,2,"Abstract: Can a machine be a person? Can a robot think, be our friend or colleague? These familiar questions in the ethics of AI have recently become much more urgent than many philosophers anticipated. However, they also seem as intractable as ever. For this reason, several philosophers of AI have recently turned their attention to an arguably new method:","https://link.springer.com/content/pdf/10.1007/s43681-023-00295-4.pdf",""
4,"David Leslie","Does the sun rise for ChatGPT? Scientific discovery in the age of generative AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00315-3","",194,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00315-3","2730-5953","",,,,,4,2.00,4,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00315-3.pdf",""
4,"Aanchal Sethi, Tushar Tangri, Divyansh Puri, Abhinav Singh, Kashish Agrawal","Knowledge management and ethical vulnerability in AI",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00164-6","",227,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00164-6","2730-5953","",,,,,4,1.33,1,5,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00164-6.pdf",""
4,"Md. Asraful Haque, Shuai Li","Exploring ChatGPT and its impact on society",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00435-4","",280,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00435-4","2730-5953","",,,,,4,4.00,2,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00435-4.pdf",""
4,"Soheil Human, Ryan Watkins","Needs and artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00206-z","",311,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00206-z","2730-5953","",3,3,811,826,4,1.33,2,2,3,"Abstract: Throughout our history, we, Homo sapiens, have used technologies to better satisfy our","https://link.springer.com/content/pdf/10.1007/s43681-022-00206-z.pdf",""
4,"Shailendra Kumar, Sanghamitra Choudhury","Humans, super humans, and super humanoids: debating Stephen Hawking’s doomsday AI forecast",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00213-0","",327,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00213-0","2730-5953","",3,3,975,984,4,1.33,2,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00213-0.pdf",""
4,"Soraj Hongladarom, Jerd Bandasak","Non-western AI ethics guidelines: implications for intercultural ethics of technology",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01665-6","",328,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01665-6","0951-5666","",39,4,2019,2032,4,2.00,2,2,2,"","https://link.springer.com/content/pdf/10.1007/s00146-023-01665-6.pdf",""
4,"Rungpailin Songja, Iyakup Promboot, Bhavaris Haetanurak, Chutisant Kerdvibulvech","Deepfake AI images: should deepfakes be banned in Thailand?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00350-0","",371,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00350-0","2730-5953","",4,4,1519,1531,4,2.00,1,4,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00350-0.pdf",""
4,"Erez Firt","Ought we align the values of artificial moral agents?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00264-x","",415,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00264-x","2730-5953","",4,2,273,282,4,2.00,4,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00264-x.pdf",""
4,"Tina Nguyen","Merging public health and automated approaches to address online hate speech",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00281-w","",485,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00281-w","2730-5953","",4,2,441,450,4,2.00,4,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00281-w.pdf",""
4,"Katharina Simbeck","They shall be fair, transparent, and robust: auditing learning analytics systems",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00292-7","",497,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00292-7","2730-5953","",4,2,555,571,4,2.00,4,1,2,"Abstract: In the near future, systems, that use Artificial Intelligence (AI) methods, such as machine learning, are required to be certified or audited for fairness if used in ethically sensitive fields such as education. One example of those upcoming regulatory initiatives is the European Artificial Intelligence Act. Interconnected with fairness are the notions of system transparency (i.e. how understandable is the system) and system robustness (i.e. will similar inputs lead to similar results). Ensuring fairness, transparency, and robustness requires looking at data, models, system processes, and the use of systems as the ethical implications arise at the intersection between those. The potential societal consequences are domain specific, it is, therefore, necessary to discuss specifically for Learning Analytics (LA) what fairness, transparency, and robustness mean and how they can be certified. Approaches to certifying and auditing fairness in LA include assessing datasets, machine learning models, and the end-to-end LA process for fairness, transparency, and robustness. Based on Slade and Prinsloo’s six principals for ethical LA, relevant audit approaches will be deduced. Auditing AI applications in LA is a complex process that requires technical capabilities and needs to consider the perspectives of all stakeholders. This paper proposes a comprehensive framework for auditing AI applications in LA systems from the perspective of learners' autonomy, provides insights into different auditing methodologies, and emphasizes the importance of reflection and dialogue among providers, buyers, and users of these systems to ensure their ethical and responsible use.","https://link.springer.com/content/pdf/10.1007/s43681-023-00292-7.pdf",""
4,"Sam Hind, Fernando N. van der Vlist, Max Kanderske","Challenges as catalysts: how Waymo’s Open Dataset Challenges shape AI development",2024,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-01927-x","",498,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-01927-x","0951-5666","",,,,,4,4.00,1,3,1,"Abstract: Artificial intelligence (AI) and machine learning (ML) are becoming increasingly significant areas of research for scholars in science and technology studies (STS) and media studies. In March 2020, Waymo, Google/Alphabet’s autonomous vehicle project, introduced the ‘Open Dataset Virtual Challenge’, an annual competition leveraging their Waymo Open Dataset. This freely accessible dataset comprises annotated autonomous vehicle data from their own Waymo vehicles. Yearly, Waymo has continued to host iterations of this challenge, inviting teams of computer scientists to tackle evolving machine learning and vision problems using Google's data and tools. This article analyses these challenges, situating them within the context of the ‘Grand Challenges’ of artificial intelligence (AI), which aimed to foster accountable and commercially viable advancements in the late 1980s. Through two exploratory workshops, we adopted a ‘technographic’ approach to examine the pivotal role of challenges in the development and political economy of AI. Serving as an organising principle for the AI innovation ecosystem, the challenge connects companies and external collaborators, driving advancements in specific machine vision domains. By exploring six key themes—interface methods, incrementalism, metrics, AI vernacular, applied domains, and competitive advantages—the article illustrates the role of these challenges in shaping AI research and development. By unpacking the dynamic interaction between data, computation, and labour, these challenges serve as catalysts propelling advancements towards self-driving technologies. The study reveals how challenges have historically and presently shaped the evolving landscape of self-driving and AI technologies.","https://link.springer.com/content/pdf/10.1007/s00146-024-01927-x.pdf",""
4,"Paul Hayes, Ibo van de Poel, Marc Steen","Moral transparency of and concerning algorithmic tools",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00190-4","",547,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00190-4","2730-5953","",3,2,585,600,4,1.33,1,3,3,"Abstract: Algorithms and AI tools are becoming increasingly influential artefacts in commercial and governance contexts. Algorithms and AI tools are not value neutral; to some extent they must be rendered knowable and known as objects, and in their implementation and deployment, to see clearly and understand their implications for moral values, and what actions can be undertaken to optimise them in their design and use towards ethical goals, or whether they are even suitable for particular goals. Transparency is a term with variable uses and interpretations, a problem which can challenge its use in design and policy. Here, we attempt to further clarify transparency. We argue that transparency is the state of affairs that obtains when relevant and understandable information about some X is available and accessible to some target audience (A), so that this information is sufficient for A for the purpose (P). Moreover, we connect this conceptualisation with transparency’s moral value, where P is to provide an account about X’s supportive or conflicting relationship with relevant values and goals. Such teleological ends in our context here can be the ability to account for the degree to which an algorithm, process or organisation respects certain values and is conducive to (social) goals.","https://link.springer.com/content/pdf/10.1007/s43681-022-00190-4.pdf",""
4,"Wei Li, Yi Huang, Shichao Wang, Xuecai Xu","Safety criticism and ethical dilemma of autonomous vehicles",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-021-00128-2","",600,"2025-02-04 16:55:17","journal-article","10.1007/s43681-021-00128-2","2730-5953","",2,4,869,874,4,1.33,1,4,3,"","https://link.springer.com/content/pdf/10.1007/s43681-021-00128-2.pdf",""
4,"Rex Bringula","What do academics have to say about ChatGPT? A text mining analytics on the discussions regarding ChatGPT on research writing",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00354-w","",628,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00354-w","2730-5953","",,,,,4,2.00,4,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00354-w.pdf",""
4,"Anastasia Georgievskaya, Timur Tlyachev, Daniil Danko, Konstantin Chekanov, Hugo Corstjens","How artificial intelligence adopts human biases: the case of cosmetic skincare industry",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00378-2","",634,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00378-2","2730-5953","",,,,,4,2.00,1,5,2,"Abstract: The cosmetic skincare industry is a growing market that extends to different regions and customer groups. In addition to scientific advances and technological developments, state-of-the-art digital approaches, including machine learning and other artificial intelligence (AI)-based techniques, are being applied at different stages of the value chain. The objectives of these efforts include optimizing the supply chain, developing high-quality, effective and safe products and personalization at every step of the customer journey. However, the use of digital technologies comes with risks and undesirable effects. These include a lack of transparency and accountability, compromised fairness and a general deficiency in data governance, all of which are critical at every customer touchpoint. This dark side of digital transformation is recognized by both businesses and governments. In this paper, we explain the concept of bias leading to unfairness for beauty technology applications. Based on published data we identified potential sources of AI bias in the cosmetic skincare industry and/or beauty tech. They were classified by the stage of the AI lifecycle: biases related to target setting, to acquisition and annotation, to modeling, to validation and evaluation, and to deployment and monitoring. We aim to create awareness of such phenomena among readers, whether executives, managers, developers or potential end-users.","https://link.springer.com/content/pdf/10.1007/s43681-023-00378-2.pdf",""
4,"Sara Solarova, Juraj Podroužek, Matúš Mesarčík, Adrian Gavornik, Maria Bielikova","Reconsidering the regulation of facial recognition in public spaces",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00194-0","",677,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00194-0","2730-5953","",3,2,625,635,4,1.33,1,5,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00194-0.pdf",""
4,"José J. Cañas","AI and Ethics When Human Beings Collaborate With AI Agents",2022,"Frontiers in Psychology","Frontiers Media SA","https://doi.org/10.3389/fpsyg.2022.836650","",738,"2025-02-04 16:55:17","journal-article","10.3389/fpsyg.2022.836650","1664-1078","",13,,,,4,1.33,4,1,3,"The relationship between a human being and an AI system has to be considered as a collaborative process between two agents during the performance of an activity. When there is a collaboration between two people, a fundamental characteristic of that collaboration is that there is co-supervision, with each agent supervising the actions of the other. Such supervision ensures that the activity achieves its objectives, but it also means that responsibility for the consequences of the activity is shared. If there is no co-supervision, neither collaborator can be held co-responsible for the actions of the other. When the collaboration is between a person and an AI system, co-supervision is also necessary to ensure that the objectives of the activity are achieved, but this also means that there is co-responsibility for the consequences of the activities. Therefore, if each agent's responsibility for the consequences of the activity depends on the effectiveness and efficiency of the supervision that that agent performs over the other agent's actions, it will be necessary to take into account the way in which that supervision is carried out and the factors on which it depends. In the case of the human supervision of the actions of an AI system, there is a wealth of psychological research that can help us to establish cognitive and non-cognitive boundaries and their relationship to the responsibility of humans collaborating with AI systems. There is also psychological research on how an external observer supervises and evaluates human actions. This research can be used to programme AI systems in such a way that the boundaries of responsibility for AI systems can be established. In this article, we will describe some examples of how such research on the task of supervising the actions of another agent can be used to establish lines of shared responsibility between a human being and an AI system. The article will conclude by proposing that we should develop a methodology for assessing responsibility based on the results of the collaboration between a human being and an AI agent during the performance of one common activity.","https://www.frontiersin.org/articles/10.3389/fpsyg.2022.836650/full",""
4,"Junhua Zhu","AI ethics with Chinese characteristics? Concerns and preferred solutions in Chinese academia",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01578-w","",773,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01578-w","0951-5666","",39,3,1261,1274,4,1.33,4,1,3,"Abstract: Since Chinese scholars are playing an increasingly important role in shaping the national landscape of discussion on AI ethics, understanding their ethical concerns and preferred solutions is essential for global cooperation on governance of AI. This article, therefore, provides the first elaborated analysis on the discourse on AI ethics in Chinese academia, via a systematic literature review. This article has three main objectives. (1) to identify the most discussed ethical issues of AI in Chinese academia and those being left out (the question of “what”); (2) to analyze the solutions proposed and preferred by Chinese scholars (the question of “how”); and (3) to map out whose voices are dominating and whose are in the marginal (the question of “who”). Findings suggest that in terms of short-term implications, Chinese scholars’ concerns over AI resemble predominantly the content of international ethical guidelines. Yet in terms of long-term implications, there are some significant differences needed to be further addressed in a cultural context. Further, among a wide range of solution proposals, Chinese scholars seem to prefer strong-binding regulations to those weak ethical guidelines. In addition, this article also found that the Chinese academic discourse was dominated by male scholars and those who are from elite universities, which arguably is not a unique phenomenon in China.","https://link.springer.com/content/pdf/10.1007/s00146-022-01578-w.pdf",""
4,"Jonathan Moreno, Michael L. Gross, Jack Becker, Blake Hereth, Neil D. Shortland, Nicholas G. Evans","The ethics of AI-assisted warfighter enhancement research and experimentation: Historical perspectives and ethical challenges",2022,"Frontiers in Big Data","Frontiers Media SA","https://doi.org/10.3389/fdata.2022.978734","",805,"2025-02-04 16:55:17","journal-article","10.3389/fdata.2022.978734","2624-909X","",5,,,,4,1.33,1,6,3,"The military applications of AI raise myriad ethical challenges. Critical among them is how AI integrates with human decision making to enhance cognitive performance on the battlefield. AI applications range from augmented reality devices to assist learning and improve training to implantable Brain-Computer Interfaces (BCI) to create bionic “super soldiers.” As these technologies mature, AI-wired warfighters face potential affronts to cognitive liberty, psychological and physiological health risks and obstacles to integrating into military and civil society during their service and upon discharge. Before coming online and operational, however, AI-assisted technologies and neural interfaces require extensive research and human experimentation. Each endeavor raises additional ethical concerns that have been historically ignored thereby leaving military and medical scientists without a cogent ethics protocol for sustainable research. In this way, this paper is a “prequel” to the current debate over enhancement which largely considers neuro-technologies once they are already out the door and operational. To lay the ethics foundation for AI-assisted warfighter enhancement research, we present an historical overview of its technological development followed by a presentation of salient ethics research issues (ICRC,","https://www.frontiersin.org/articles/10.3389/fdata.2022.978734/full",""
4,"Jimiama M. Mase, Natalie Leesakul, Grazziela P. Figueredo, Mercedes Torres Torres","Facial identity protection using deep learning technologies: an application in affective computing",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00215-y","",822,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00215-y","2730-5953","",3,3,937,946,4,1.33,1,4,3,"Abstract: Automatic prediction of human attributions of valence and arousal using facial recognition technologies can improve human–computer and human–robot interaction. However, data protection has become an issue of great concern in affect recognition using facial images, as the facial identities of people (i.e. recognising who a person is) could be exposed in the process. For instance, malicious individuals could exploit facial images of users to assume their identities and infiltrate biometric authentication systems. Possible solutions to protect the facial identity of users are to: (1) extract anonymised facial features in users’ local machines, namely action units (AU) of facial images, discard their facial images and send the AUs to the developer for processing, and (2) employ a federated learning approach i.e. process users’ facial images in their local machines and only send their locally trained models back to the developer’s machine for augmenting the final model. In this paper, we implement and compare the performance of these privacy-preserving strategies for affect recognition. Results on the popular RECOLA affective datasets show promising affect recognition performance in adopting a federated learning approach to protect users’ identities, with Concordance Correlation Coefficient of 0.426 for valence and 0.390 for arousal.","https://link.springer.com/content/pdf/10.1007/s43681-022-00215-y.pdf",""
4,"Carolina Villegas-Galaviz","Ethics of Care as Moral Grounding for AI *",2022,"Ethics of Data and Analytics","Auerbach Publications","https://doi.org/10.1201/9781003278290-13","",834,"2025-02-04 16:55:17","book-chapter","10.1201/9781003278290-13","","",,,78,83,4,1.33,4,1,3,"","",""
4,"Andrea Ferrario, Sophie Gloeckler, Nikola Biller-Andorno","AI knows best? Avoiding the traps of paternalism and other pitfalls of AI-based patient preference prediction",2023,"Journal of Medical Ethics","BMJ","https://doi.org/10.1136/jme-2023-108945","",911,"2025-02-04 16:55:17","journal-article","10.1136/jme-2023-108945","0306-6800","",49,3,185,186,4,2.00,1,3,2,"","https://syndication.highwire.org/content/doi/10.1136/jme-2023-108945",""
3,"Jeremy Davis, Duncan Purves, Juan Gilbert, Schuyler Sturm","Five ethical challenges facing data-driven policing",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-021-00105-9","",15,"2025-02-04 16:55:17","journal-article","10.1007/s43681-021-00105-9","2730-5953","",2,1,185,198,3,1.00,1,4,3,"","https://link.springer.com/content/pdf/10.1007/s43681-021-00105-9.pdf",""
3,"Roman V. Yampolskiy","On monitorability of AI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00420-x","",52,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00420-x","2730-5953","",,,,,3,3.00,3,1,1,"Abstract: Artificially intelligent (AI) systems have ushered in a transformative era across various domains, yet their inherent traits of unpredictability, unexplainability, and uncontrollability have given rise to concerns surrounding AI safety. This paper aims to demonstrate the infeasibility of accurately monitoring advanced AI systems to predict the emergence of certain capabilities prior to their manifestation. Through an analysis of the intricacies of AI systems, the boundaries of human comprehension, and the elusive nature of emergent behaviors, we argue for the impossibility of reliably foreseeing some capabilities. By investigating these impossibility results, we shed light on their potential implications for AI safety research and propose potential strategies to overcome these limitations.","https://link.springer.com/content/pdf/10.1007/s43681-024-00420-x.pdf",""
3,"Peter Singer, Yip Fai Tse","Correction: AI ethics: the case for including animals",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00243-8","",85,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00243-8","2730-5953","",3,1,347,347,3,1.00,2,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00243-8.pdf",""
3,"Pascale Fung, Hubert Etienne","Confucius, cyberpunk and Mr. Science: comparing AI ethics principles between China and the EU",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00180-6","",154,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00180-6","2730-5953","",3,2,505,511,3,1.00,2,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00180-6.pdf",""
3,"Eva Thelisson, Himanshu Verma","Conformity assessment under the EU AI act general approach",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00402-5","",242,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00402-5","2730-5953","",4,1,113,121,3,3.00,2,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00402-5.pdf",""
3,"Federico Cugurullo","The obscure politics of artificial intelligence: a Marxian socio-technical critique of the AI alignment problem thesis",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00476-9","",260,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00476-9","2730-5953","",,,,,3,3.00,3,1,1,"Abstract: There is a growing feeling that artificial intelligence (AI) is getting out of control. Many AI experts worldwide stress that great care must be taken on the so-called","https://link.springer.com/content/pdf/10.1007/s43681-024-00476-9.pdf",""
3,"Rosalie A. Waelen","The ethics of computer vision: an overview in terms of power",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00272-x","",276,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00272-x","2730-5953","",4,2,353,362,3,1.50,3,1,2,"Abstract: Computer vision is a subfield of artificial intelligence, aimed at making computers see. Computer vision tools enable a system or device to automatically analyze, interpret, and respond to images and videos. Computer vision tasks range from object detection and tracking, to the recognition of people’s faces and emotional states. While the ethics of AI in general has received significant attention, and the ethics of facial recognition (a computer vision application) too, little of the AI ethics literature focuses specifically on the ethics of computer vision. In this chapter, I create an overview of ethical, social, and political issues related to computer vision, using a critical approach. This means that I identify issues in terms of power and evaluate them in function of their impact on the value of autonomy and the normative goal of emancipatory progress. The aim of this chapter is first and foremost to offer an overview of potential normative implications of computer vision. Additionally, the chapter functions as an example for the use of a critical approach to AI ethics.","https://link.springer.com/content/pdf/10.1007/s43681-023-00272-x.pdf",""
3,"Sivan Tamir","Artificial intelligence in human reproduction: charting the ethical debate over AI in IVF",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00216-x","",294,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00216-x","2730-5953","",3,3,947,961,3,1.00,3,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00216-x.pdf",""
3,"Fabio Tollon","Responsibility gaps and the reactive attitudes",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00172-6","",319,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00172-6","2730-5953","",3,1,295,302,3,1.00,3,1,3,"Abstract: Artificial Intelligence (AI) systems are ubiquitous. From social media timelines, video recommendations on YouTube, and the kinds of adverts we see online, AI, in a very real sense, filters the world we see. More than that, AI is being embedded in agent-like systems, which might prompt certain reactions from users. Specifically, we might find ourselves feeling frustrated if these systems do not meet our expectations. In normal situations, this might be fine, but with the ever increasing sophistication of AI-systems, this might become a problem. While it seems unproblematic to realize that being angry at your car for breaking down is unfitting, can the same be said for AI-systems? In this paper, therefore, I will investigate the so-called “reactive attitudes”, and their important link to our responsibility practices. I then show how within this framework there exist exemption and excuse conditions, and test whether our adopting the “objective attitude” toward agential AI is justified. I argue that such an attitude is appropriate in the context of three distinct senses of responsibility (answerability, attributability, and accountability), and that, therefore, AI-systems do not undermine our responsibility ascriptions.","https://link.springer.com/content/pdf/10.1007/s43681-022-00172-6.pdf",""
3,"Simon Courtenage","Intelligent machines, collectives, and moral responsibility",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00285-6","",323,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00285-6","2730-5953","",4,2,485,498,3,1.50,3,1,2,"Abstract: Collectives, such as companies, are generally thought to be moral agents and hence capable of being held responsible for what they do. If collectives, being non-human, can be ascribed moral responsibility, then can we do the same for machines? Is it equally the case that machines, particularly intelligent machines, can be held morally responsible for what they choose to do? I consider the conditions required for moral responsibility, and argue that, in terms of the agency condition, artificial, non-human entities in general are excused from being responsible because, although they may choose their actions, the beliefs and desires that form the basis of their choices are predetermined by their designers, placing them in an analogous position to persons suffering covert manipulation. This creates a problem for collective responsibility, but I argue that collectives, through their supervention on human persons, represent an exception. Finally, I consider that the design of future machines may be sufficiently abstract and high-level as to fall below some threshold of influence, allowing machines enough freedom for us to hold them responsible.","https://link.springer.com/content/pdf/10.1007/s43681-023-00285-6.pdf",""
3,"Désirée Martin, Michael W. Schmidt, Rafaela Hillerbrand","Implementing AI Ethics in the Design of AI-assisted Rescue Robots",2023,"2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology (ETHICS)","IEEE","https://doi.org/10.1109/ethics57328.2023.10155062","",345,"2025-02-04 16:55:17","proceedings-article","10.1109/ethics57328.2023.10155062","","",,,,,3,1.50,1,3,2,"","http://xplorestaging.ieee.org/ielx7/10154894/10154907/10155062.pdf?arnumber=10155062",""
3,"Kudzayi Savious Tarisayi","Lustre and shadows: unveiling the gaps in South African University plagiarism policies amidst the emergence of AI-generated content",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00333-1","",367,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00333-1","2730-5953","",,,,,3,1.50,3,1,2,"Abstract: In recent years, artificial intelligence (AI) has become a key technology in the field of academic integrity. However, there is a lack of a comprehensive understanding of the legal dimensions of plagiarism in the context of AI. In this study, a theoretical framework that combines the social construction of technology and the legal dimension of plagiarism was used to explore the current construction of plagiarism in South African university plagiarism policies. This study aims to highlight the inadequacy of current plagiarism policies, which primarily focus on the act of copying from others and emphasize the need for a broader perspective that addresses the challenges posed by artificial intelligence in academic integrity in the era of AI-generated content. The author used confirming sampling and data saturation was reached with a sample of ten university plagiarism policies. The findings revealed an inadequacy of the policies on the coverage of AI-generated content and therefore justifying the need to redefine plagiarism in the context of the artificial intelligence revolution. The author concludes by redefining plagiarism and justifying the utility of the recommended definition.","https://link.springer.com/content/pdf/10.1007/s43681-023-00333-1.pdf",""
3,"John W. Murphy, Randon R. Taylor","To democratize or not to democratize AI? That is the question",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00313-5","",370,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00313-5","2730-5953","",4,4,1357,1363,3,1.50,2,2,2,"Abstract: This paper advances the debate surrounding whether to democratize AI and explores some of the challenges and benefits of democratization through community-based work and direct democracy. We contend that community-based strategies can incorporate local knowledge and control, thereby providing more effective AI solutions that are human-centric and less harmful. However, democratization needs to be approached with caution and care, since this process requires a deeper understanding of who participates, the decision domain, and the different realities at stake. Moreover, we highlight the importance of participation in AI development to ensure its legitimacy, considering the capacity of this technology to shape reality. We emphasize that participation should be more than just involving stakeholders or seeking input from users. Rather, participation should involve local narratives that generate knowledge and shape information landscapes, thereby producing a different, anti-Cartesian scene. We conclude by underscoring that the success of democratizing AI hinges on the careful delineation of the boundaries of participation, which should include the specific needs of the immediate context, the decision domain, and the various participants involved.","https://link.springer.com/content/pdf/10.1007/s43681-023-00313-5.pdf",""
3,"Åsbjørn Melkevik","The internal morality of markets and artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00151-x","",384,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00151-x","2730-5953","",3,1,113,122,3,1.00,3,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00151-x.pdf",""
3,"Joyce Nakatumba-Nabende, Conrad Suuna, Engineer Bainomugisha","AI Ethics in Higher Education: Research Experiences from Practical Development and Deployment of AI Systems",2023,"SpringerBriefs in Ethics","Springer International Publishing","https://doi.org/10.1007/978-3-031-23035-6_4","",410,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-23035-6_4","2211-8101","",,,39,55,3,1.50,1,3,2,"Abstract: Artificial Intelligence (AI) offers tangible benefits in several application domains like disease diagnosis in health.","https://link.springer.com/content/pdf/10.1007/978-3-031-23035-6_4",""
3,"Bryce Goodman","Privacy without persons: a Buddhist critique of surveillance capitalism",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00204-1","",430,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00204-1","2730-5953","",3,3,781,792,3,1.00,3,1,3,"Abstract: Much has been written about artificial intelligence (AI) perpetuating social inequity and disenfranchising marginalized groups (Barocas in SSRN J, 2016; Goodman in Law and Ethics of AI, 2017; Buolamwini and Gebru in Conference on Fairness, Accountability and Transparency, 2018). It is a sad irony that virtually all of these critiques are exclusively couched in concepts and theories from the Western philosophical tradition (Algorithm Watch in AI ethics guidelines global inventory, 2021; Goffi in Sapiens, 2021). In particular, Buddhist philosophy is, with a few notable exceptions (Hongladarom in A Buddhist Theory of Privacy, Springer, Singapore, 2016; Hongladarom in The Ethics of AI and Robotics A Buddhist Viewpoint, Lexington Book, Maryland, 2020; Hongladarom in MIT Technology Review, 2021; Lin et al. in Robot Ethics: The Ethical and Social Implications fo Robotics, MIT, Cambridge, 2012; Promta and Einar Himma in J Inf Commun Ethics Soc 6(2):172–187, 2008), completely ignored. This inattention to non-Western philosophy perpetuates a pernicious form of intellectual imperialism (Alatas in Southeast Asian J Soc Sci 28(1):23–45, 2000), and deprives the field of vital intellectual resources. The aim of this article is twofold: to introduce Buddhist concepts and arguments to an unfamiliar audience and to demonstrate how those concepts can be fruitfully deployed within the field of AI ethics. In part one, I develop a Buddhist inspired critique of two propositions about privacy: that the scope of privacy is defined by an essential connection between certain types of information and personal identity (i.e., what makes a person who they are), and that privacy is intrinsically valuable as a part of human dignity (Council of the European Union in Position of the Council on General Data Protection Regulation, 2016). The Buddhist doctrine of not self (","https://link.springer.com/content/pdf/10.1007/s43681-022-00204-1.pdf",""
3,"Larissa Schlicht, Miriam Räker","A context-specific analysis of ethical principles relevant for AI-assisted decision-making in health care",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00324-2","",436,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00324-2","2730-5953","",4,4,1251,1263,3,1.50,2,2,2,"Abstract: Artificial intelligence (AI)-assisted technologies may exert a profound impact on social structures and practices in care contexts. Our study aimed to complement ethical principles considered relevant for the design of AI-assisted technology in health care with a context-specific conceptualization of the principles from the perspectives of individuals potentially affected by the implementation of AI technologies in nursing care. We conducted scenario-based semistructured interviews focusing on situations involving moral decision-making occurring in everyday nursing practice with nurses (","https://link.springer.com/content/pdf/10.1007/s43681-023-00324-2.pdf",""
3,"David B. Resnik, Suzanne L. Andrews","A precautionary approach to autonomous vehicles",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00277-6","",458,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00277-6","2730-5953","",4,2,403,418,3,1.50,2,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00277-6.pdf",""
3,"Anaïs Resseguier, Fabienne Ufert","AI research ethics is in its infancy: the EU’s AI Act can make it a grown-up",2023,"Research Ethics","SAGE Publications","https://doi.org/10.1177/17470161231220946","",463,"2025-02-04 16:55:17","journal-article","10.1177/17470161231220946","1747-0161","",20,2,143,155,3,1.50,2,2,2,"As the artificial intelligence (AI) ethics field is currently working towards its operationalisation, ethics review as carried out by research ethics committees (RECs) constitutes a powerful, but so far underdeveloped, framework to make AI ethics effective in practice at the research level. This article contributes to the elaboration of research ethics frameworks for research projects developing and/or using AI. It highlights that these frameworks are still in their infancy and in need of a structure and criteria to ensure AI research projects advance in a way that respects norms and principles. This article proposes to draw from the European Union’s AI Act currently in development to shape these frameworks. Although, in the current form of the draft (as of August 2023), the obligations of the AI Act do not apply to scientific research, it is most likely that they will still have a strong impact on AI research considering the need to anticipate market placement or to test new tools in real world conditions. This article investigates what the risk-based approach in the AI Act implies for research ethics and highlights some AI Act obligations of particular value to implement for ethics review processes.","https://journals.sagepub.com/doi/pdf/10.1177/17470161231220946",""
3,"Jan-Hendrik Heinrichs","Responsibility assignment won’t solve the moral issues of artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00133-z","",477,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00133-z","2730-5953","",2,4,727,736,3,1.00,3,1,3,"Abstract: Who is responsible for the events and consequences caused by using artificially intelligent tools, and is there a gap between what human agents can be responsible for and what is being done using artificial intelligence? Both questions presuppose that the term ‘responsibility’ is a good tool for analysing the moral issues surrounding artificial intelligence. This article will draw this presupposition into doubt and show how reference to responsibility obscures the complexity of moral situations and moral agency, which can be analysed with a more differentiated toolset of moral terminology. It suggests that the impression of responsibility gaps only occurs if we gloss over the complexity of the moral situation in which artificial intelligent tools are employed and if—counterfactually—we ascribe them some kind of pseudo-agential status.","https://link.springer.com/content/pdf/10.1007/s43681-022-00133-z.pdf",""
3,"Ghanim Al-Sulaiti, Mohammad Amin Sadeghi, Lokendra Chauhan, Ji Lucas, Sanjay Chawla, Ahmed Elmagarmid","A pragmatic perspective on AI transparency at workplace",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00257-w","",489,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00257-w","2730-5953","",4,2,189,200,3,1.50,1,6,2,"Abstract: Recently, artificial intelligence (AI) systems have been widely used in different contexts and professions. However, with these systems developing and becoming more complex, they have transformed into black boxes that are difficult to interpret and explain. Therefore, urged by the wide media coverage of negative incidents involving AI, many scholars and practitioners have called for AI systems to be transparent and explainable. In this study, we examine transparency in AI-augmented settings, such as in workplaces, and perform a novel analysis of the different jobs and tasks that can be augmented by AI. Using more than 1000 job descriptions and 20,000 tasks from the O*NET database, we analyze the level of transparency required to augment these tasks by AI. Our findings indicate that the transparency requirements differ depending on the augmentation score and perceived risk category of each task. Furthermore, they suggest that it is important to be pragmatic about transparency, and they support the growing viewpoint regarding the impracticality of the notion of full transparency.","https://link.springer.com/content/pdf/10.1007/s43681-023-00257-w.pdf",""
3,"Pouyan Esmaeilzadeh","The role of ChatGPT in disrupting concepts, changing values, and challenging ethical norms: a qualitative study",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00338-w","",501,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00338-w","2730-5953","",,,,,3,1.50,3,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00338-w.pdf",""
3,"Ugo Pagallo, Eleonora Bassi, Massimo Durante","The Normative Challenges of AI in Outer Space: Law, Ethics, and the Realignment of Terrestrial Standards",2023,"Philosophy &amp; Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s13347-023-00626-7","",578,"2025-02-04 16:55:17","journal-article","10.1007/s13347-023-00626-7","2210-5433","",36,2,,,3,1.50,1,3,2,"Abstract: The paper examines the open problems that experts of space law shall increasingly address over the next few years, according to four different sets of legal issues. Such differentiation sheds light on what is old and what is new with today’s troubles of space law, e.g., the privatization of space, vis-à-vis the challenges that AI raises in this field. Some AI challenges depend on its unique features, e.g., autonomy and opacity, and how they affect pillars of the law, whether on Earth or in space missions. The paper insists on a further class of legal issues that AI systems raise, however, only in outer space. We shall never overlook the constraints of a hazardous and hostile environment, such as on a mission between Mars and the Moon. The aim of this paper is to illustrate what is still mostly unexplored or in its infancy in this kind of research, namely, the fourfold ways in which the uniqueness of AI and that of outer space impact both ethical and legal standards. Such standards shall provide for thresholds of evaluation according to which courts and legislators evaluate the pros and cons of technology. Our claim is that a new generation of sui generis standards of space law, stricter or more flexible standards for AI systems in outer space, down to the “principle of equality” between human standards and robotic standards, will follow as a result of this twofold uniqueness of AI and of outer space.","https://link.springer.com/content/pdf/10.1007/s13347-023-00626-7.pdf",""
3,"Mois Navon","Let us make man in our image-a Jewish ethical perspective on creating conscious robots",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00328-y","",620,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00328-y","2730-5953","",4,4,1235,1250,3,1.50,3,1,2,"Abstract: The dream of making conscious humanoid robots is one that has long tantalized humanity, yet today it seems closer than ever before. Assuming that science can make it happen, the question becomes: should we make it happen? Is it morally permissible to create synthetic beings with consciousness? While a consequentialist approach may seem logical, attempting to assess the potential positive and negative consequences of such a revolutionary technology is highly speculative and raises more questions than it answers. Accordingly, some turn to ancient and not-so-ancient stories of “automata” for direction. Of the many automata conjured throughout history, if not in matter then in mind, the Golem stands out as one of the most persistent paradigms employed to discuss technology in general and technologically engendered life forms in particular. In this essay, I introduce a novel reading of the Golem paradigm to argue not from consequentialism, but from a deep-seated two-thousand-year-old tradition, the ethical implications of which are wholly deontological.","https://link.springer.com/content/pdf/10.1007/s43681-023-00328-y.pdf",""
3,"Yoshija Walter","The rapid competitive economy of machine learning development: a discussion on the social risks and benefits",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00276-7","",654,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00276-7","2730-5953","",4,2,635,648,3,1.50,3,1,2,"Abstract: Research in artificial intelligence (AI) has started in the twentieth century but it was not until 2012 that modern models of artificial neural networks aided the machine learning process considerably so that in the past ten years, both computer vision as well as natural language processing have become increasingly better. AI developments have accelerated rapidly, leaving open questions about the potential benefits and risks of these dynamics and how the latter might be managed. This paper discusses three major risks, all lying in the domain of AI safety engineering: the problem of AI alignment, the problem of AI abuse, and the problem of information control. The discussion goes through a short history of AI development, briefly touching on the benefits and risks, and eventually making the case that the risks might potentially be mitigated through strong collaborations and awareness concerning trustworthy AI. Implications for the (digital) humanities are discussed.","https://link.springer.com/content/pdf/10.1007/s43681-023-00276-7.pdf",""
3,"Michal Pruski","Ethics framework for predictive clinical AI model updating",2023,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-023-09721-x","",656,"2025-02-04 16:55:17","journal-article","10.1007/s10676-023-09721-x","1388-1957","",25,3,,,3,1.50,3,1,2,"","https://link.springer.com/content/pdf/10.1007/s10676-023-09721-x.pdf",""
3,"Karamjit S. Gill","Actionable ethics",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01387-1","",686,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01387-1","0951-5666","",37,1,1,7,3,1.00,3,1,3,"","https://link.springer.com/content/pdf/10.1007/s00146-022-01387-1.pdf",""
3,"Umair Rehman, Farkhund Iqbal, Muhammad Umair Shah","Exploring differences in ethical decision-making processes between humans and ChatGPT-3 model: a study of trade-offs",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00335-z","",706,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00335-z","2730-5953","",,,,,3,1.50,1,3,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00335-z.pdf",""
3,"Simon Coghlan, Thomas Quinn","Ethics of using artificial intelligence (AI) in veterinary medicine",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01686-1","",744,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01686-1","0951-5666","",39,5,2337,2348,3,1.50,2,2,2,"Abstract: This paper provides the first comprehensive analysis of ethical issues raised by artificial intelligence (AI) in veterinary medicine for companion animals. Veterinary medicine is a socially valued service, which, like human medicine, will likely be significantly affected by AI. Veterinary AI raises some unique ethical issues because of the nature of the client–patient–practitioner relationship, society’s relatively minimal valuation and protection of nonhuman animals and differences in opinion about responsibilities to animal patients and human clients. The paper examines how these distinctive features influence the ethics of AI systems that might benefit clients, veterinarians and animal patients—but also harm them. It offers practical ethical guidance that should interest ethicists, veterinarians, clinic owners, veterinary bodies and regulators, clients, technology developers and AI researchers.","https://link.springer.com/content/pdf/10.1007/s00146-023-01686-1.pdf",""
3,"Ajay Vishwanath, Einar Duenger Bøhn, Ole-Christoffer Granmo, Charl Maree, Christian Omlin","Towards artificial virtuous agents: games, dilemmas and machine learning",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00251-8","",748,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00251-8","2730-5953","",3,3,663,672,3,1.00,1,5,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00251-8.pdf",""
3,"James Brusseau","From the ground truth up: doing AI ethics from practice to principles",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-021-01336-4","",769,"2025-02-04 16:55:17","journal-article","10.1007/s00146-021-01336-4","0951-5666","",38,4,1651,1657,3,1.00,3,1,3,"","https://link.springer.com/content/pdf/10.1007/s00146-021-01336-4.pdf",""
3,"Theodore M. Lechterman","The Concept of Accountability in AI Ethics and Governance",2022,"The Oxford Handbook of AI Governance","Oxford University Press","https://doi.org/10.1093/oxfordhb/9780197579329.013.10","",795,"2025-02-04 16:55:17","book-chapter","10.1093/oxfordhb/9780197579329.013.10","","",,,164,182,3,1.00,3,1,3,"Abstract: Calls to hold artificial intelligence to account are intensifying. Activists and researchers alike warn of an “accountability gap” or even a “crisis of accountability” in AI. Meanwhile, several prominent scholars maintain that accountability holds the key to governing AI. But usage of the term varies widely in discussions of AI ethics and governance. This chapter begins by disambiguating some different senses and dimensions of accountability, distinguishing it from neighboring concepts, and identifying sources of confusion. It proceeds to explore the idea that AI operates within an accountability gap arising from technical features of AI as well as the social context in which it is deployed. The chapter also evaluates various proposals for closing this gap. It concludes that the role of accountability in AI ethics and governance is vital but also more limited than some suggest. Accountability’s primary job description is to verify compliance with substantive normative principles—once those principles are settled. Theories of accountability cannot ultimately tell us what substantive standards to account for, especially when norms are contested or still emerging. Nonetheless, formal mechanisms of accountability provide a way of diagnosing and discouraging egregious wrongdoing even in the absence of normative agreement. Providing accounts can also be an important first step toward the development of more comprehensive regulatory standards for AI.","https://academic.oup.com/edited-volume/chapter-pdf/57889531/book_41989_section_386768252.ag.pdf",""
3,"Bernd Carsten Stahl, Doris Schroeder, Rowena Rodrigues","AI for Good and the SDGs",2022,"SpringerBriefs in Research and Innovation Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-17040-9_8","",804,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-17040-9_8","2452-0519","",,,95,106,3,1.00,1,3,3,"Abstract: In 2015, 193 nations came together to agree Agenda 2030: 17 goals ranging from the elimination of poverty to the building of partnerships to achieve those goals. The spirit of the UN Sustainable Development Goals (SDGs) is to leave no one behind. Artificial intelligence (AI) has a great potential to assist in reaching the SDGs. For instance, using algorithms on new and vast agricultural data sets can improve the efficiency of agriculture practices and thereby contribute to SDG 1, “Zero hunger”. However, the high energy consumption, computational resources and levels of expertise required for AI can exacerbate existing inequalities. At the same time, potentially useful AI applications such as seasonal climate forecasting have led to the accelerated laying off of workers in Peru and credit denial to poor farmers in Zimbabweand Brazil. If AI for Good is to be truly realised, AI’s potential to worsen inequality, to overexploit resources, to be undertaken through “helicopter research” and to focus on SDG issues relevant mainly to high-income countries must be overcome, ideally in close collaboration and engagement with potential beneficiaries in resource-limited settings.","https://link.springer.com/content/pdf/10.1007/978-3-031-17040-9_8",""
3,"Matthew J Dennis, Evgeni Aizenberg","Correction to: the Ethics of AI in Human Resources",2023,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-022-09671-w","",808,"2025-02-04 16:55:17","journal-article","10.1007/s10676-022-09671-w","1388-1957","",25,1,,,3,1.50,2,2,2,"","https://link.springer.com/content/pdf/10.1007/s10676-022-09671-w.pdf",""
3,"Joshua C. Gellers","AI ethics discourse: a call to embrace complexity, interdisciplinarity, and epistemic humility",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01708-y","",831,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01708-y","0951-5666","",39,5,2593,2594,3,1.50,3,1,2,"","https://link.springer.com/content/pdf/10.1007/s00146-023-01708-y.pdf",""
3,"Anastasia Siapka","Towards a Feminist Metaethics of AI",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3534197","",853,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3534197","","",,,665,674,3,1.00,3,1,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3534197",""
3,"Nataliya Nedzhvetskaya, J. S. Tan","The Role of Workers in AI Ethics and Governance",2022,"The Oxford Handbook of AI Governance","Oxford University Press","https://doi.org/10.1093/oxfordhb/9780197579329.013.68","",889,"2025-02-04 16:55:17","book-chapter","10.1093/oxfordhb/9780197579329.013.68","","",,,584,603,3,1.00,2,2,3,"Abstract: While the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention. This chapter looks at the role that workers play in identifying and mitigating harms from AI technologies. Harms are the causally assessed “impacts” of technologies. They arise despite technical reliability and are not a result of technical negligence but rather of normative uncertainty around questions of safety and fairness in complex social systems. There is high consensus in the AI ethics community on the benefits of reducing harms but less consensus on mechanisms for determining or addressing harms. This lack of consensus has led to numerous collective actions by workers protesting how harms are identified and addressed in their workplace. This chapter theorizes the role of workers within AI governance and constructs a model of harm reporting processes in AI workplaces. The harm reporting process involves three steps: identification, the governance decision, and the response. Workers draw upon three types of claims to argue for jurisdiction over questions of AI governance: subjection, control over the product of one’s labor, and proximate knowledge of systems. Examining the past decade of AI-related worker activism allows us to understand how different types of workers are positioned within a workplace that produces AI systems, how their position informs their claims, and the place of collective action in staking their claims. This chapter argues that workers occupy a unique role in identifying and mitigating harms caused by AI systems.","https://academic.oup.com/edited-volume/chapter-pdf/57890036/book_41989_section_371702524.ag.pdf",""
3,"Gabriele Griffin, Elisabeth Wennerström, Anna Foka","AI and Swedish Heritage Organisations: challenges and opportunities",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01689-y","",983,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01689-y","0951-5666","",39,5,2359,2372,3,1.50,1,3,2,"Abstract: This article examines the challenges and opportunities that arise with artificial intelligence (AI) and machine learning (ML) methods and tools when implemented within cultural heritage institutions (CHIs), focusing on three selected Swedish case studies. The article centres on the perspectives of the CHI professionals who deliver that implementation. Its purpose is to elucidate how CHI professionals respond to the opportunities and challenges AI/ML provides. The three Swedish CHIs discussed here represent different organizational frameworks and have different types of collections, while sharing, to some extent, a similar position in terms of the use of AI/ML tools and methodologies. The overarching question of this article is what is the state of knowledge about AI/ML among Swedish CHI professionals, and what are the related issues? To answer this question, we draw on (1) semi-structured interviews with CHI professionals, (2) individual CHI website information, and (3) CHI-internal digitization protocols and digitalization strategies, to provide a nuanced analysis of both professional and organisational processes concerning the implementation of AI/ML methods and tools. Our study indicates that AI/ML implementation is in many ways at the very early stages of implementation in Swedish CHIs. The CHI professionals are affected in their AI/ML engagement by four key issues that emerged in the interviews: their institutional and professional knowledge regarding AI/ML; the specificities of their collections and associated digitization and digitalization issues; issues around personnel; and issues around AI/ML resources. The article suggests that a national CHI strategy for AI/ML might be helpful as would be knowledge-, expertise-, and potentially personnel- and resource-sharing to move beyond the constraints that the CHIs face in implementing AI/ML.","https://link.springer.com/content/pdf/10.1007/s00146-023-01689-y.pdf",""
2,"Max Tretter, Tabea Ott, Peter Dabrock","AI-produced certainties in health care: current and future challenges",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00374-6","",2,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00374-6","2730-5953","",,,,,2,1.00,1,3,2,"Abstract: Since uncertainty is a major challenge in medicine and bears the risk of causing incorrect diagnoses and harmful treatment, there are many efforts to tackle it. For some time, AI technologies have been increasingly implemented in medicine and used to reduce medical uncertainties. What initially seems desirable, however, poses challenges. We use a multimethod approach that combines philosophical inquiry, conceptual analysis, and ethical considerations to identify key challenges that arise when AI is used for medical certainty purposes. We identify several challenges. Where AI is used to reduce medical uncertainties, it is likely to result in (a) patients being stripped down to their measurable data points, and being made disambiguous. Additionally, the widespread use of AI technologies in health care bears the risk of (b) human physicians being pushed out of the medical decision-making process, and patient participation being more and more limited. Further, the successful use of AI requires extensive and invasive monitoring of patients, which raises (c) questions about surveillance as well as privacy and security issues. We outline these several challenges and show that they are immediate consequences of AI-driven security efforts. If not addressed, they could entail unfavorable consequences. We contend that diminishing medical uncertainties through AI involves a tradeoff. The advantages, including enhanced precision, personalization, and overall improvement in medicine, are accompanied by several novel challenges. This paper addresses them and gives suggestions about how to use AI for certainty purposes without causing harm to patients.","https://link.springer.com/content/pdf/10.1007/s43681-023-00374-6.pdf",""
2,"Muhammad Salar Khan","A multidimensional approach towards addressing existing and emerging challenges in the use of ChatGPT",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00360-y","",9,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00360-y","2730-5953","",,,,,2,1.00,2,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00360-y.pdf",""
2,"Brian Hutler, Travis N. Rieder, Debra J. H. Mathews, David A. Handelman, Ariel M. Greenberg","Designing robots that do no harm: understanding the challenges of Ethics for Robots",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00283-8","",26,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00283-8","2730-5953","",4,2,463,471,2,1.00,0,5,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00283-8.pdf",""
2,"Mattis Jacobs, Judith Simon","Reexamining computer ethics in light of AI systems and AI regulation",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00229-6","",65,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00229-6","2730-5953","",3,4,1203,1213,2,0.67,1,2,3,"Abstract: This article argues that the emergence of AI systems and AI regulation showcases developments that have significant implications for computer ethics and make it necessary to reexamine some key assumptions of the discipline. Focusing on design- and policy-oriented computer ethics, the article investigates new challenges and opportunities that occur in this context. The main challenges concern how an AI system’s technical, social, political, and economic features can hinder a successful application of computer ethics. Yet, the article demonstrates that features of AI systems that potentially interfere with successfully applying some approaches to computer ethics are (often) only contingent, and that computer ethics can influence them. Furthermore, it shows how computer ethics can make use of how power manifests in an AI system’s technical, social, political, and economic features to achieve its goals. Lastly, the article outlines new interdependencies between policy- and design-oriented computer ethics, manifesting as either conflicts or synergies.","https://link.springer.com/content/pdf/10.1007/s43681-022-00229-6.pdf",""
2,"Alesia Zhuk","Ethical implications of AI in the Metaverse",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00450-5","",73,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00450-5","2730-5953","",,,,,2,2.00,2,1,1,"Abstract: This paper delves into the ethical implications of AI in the Metaverse through the analysis of real-world case studies, including Horizon Worlds, Decentraland, Roblox, Sansar, and Rec Room. The examination reveals recurring concerns related to content moderation, emphasising the need for a human-AI hybrid approach to strike a balance between creative freedom and user safety. Privacy and data protection emerge as crucial considerations, highlighting the importance of transparent communication and user data control for responsible AI implementation. Additionally, promoting inclusivity and diversity is emphasised, calling for transparent governance, diverse representation, and collaboration with ethics experts to ensure equitable AI practices. By addressing these specific ethical challenges, we can pave the way towards a responsible and user-centric Metaverse, maximising its potential while safeguarding user well-being and rights.","https://link.springer.com/content/pdf/10.1007/s43681-024-00450-5.pdf",""
2,"Jeff Sebo, Robert Long","Moral consideration for AI systems by 2030",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00379-1","",87,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00379-1","2730-5953","",,,,,2,1.00,1,2,2,"Abstract: This paper makes a simple case for extending moral consideration to some AI systems by 2030. It involves a normative premise and a descriptive premise. The normative premise is that humans have a duty to extend moral consideration to beings that have a non-negligible chance, given the evidence, of being conscious. The descriptive premise is that some AI systems do in fact have a non-negligible chance, given the evidence, of being conscious by 2030. The upshot is that humans have a duty to extend moral consideration to some AI systems by 2030. And if we have a duty to do that, then we plausibly also have a duty to start preparing now, so that we can be ready to treat AI systems with respect and compassion when the time comes.","https://link.springer.com/content/pdf/10.1007/s43681-023-00379-1.pdf",""
2,"Anna Katrine Jørgensen, Anders Søgaard","Rawlsian AI fairness loopholes",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00226-9","",133,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00226-9","2730-5953","",3,4,1185,1192,2,0.67,1,2,3,"Abstract: Researchers and industry developers in artificial intelligence (AI) and natural language processing (NLP) have uniformly adopted a Rawlsian definition of fairness. On this definition, a technology is fair if performance is maximized for the least advantaged. We argue this definition has considerable loopholes, which can be used to legitimize common practices in AI/NLP research that actively contributes to social and economic inequalities. Such practices include what we shall refer to as Subgroup Test Ballooning and Snapshot-Representative Evaluation. Subgroup Test Ballooning refers to the practice of initially tailoring a technology to a specific target group of technology-ready early adopters to collect feedback faster. Snapshot-Representative Evaluation refers to the practice of evaluating a technology on a representative sample of current end users. Both strategies may contribute to social and economic inequalities but are commonly justified using arguments familiar from political economics and grounded in Rawlsian fairness. We discuss an egalitarian alternative to Rawlsian fairness, as well as, more generally, the roadblocks on the path toward globally and socially fair AI/NLP research and development.","https://link.springer.com/content/pdf/10.1007/s43681-022-00226-9.pdf",""
2,"Sun Sun Lim, Roland Bouffanais","‘Data dregs’ and its implications for AI ethics: Revelations from the pandemic",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-021-00130-8","",143,"2025-02-04 16:55:17","journal-article","10.1007/s43681-021-00130-8","2730-5953","",2,4,595,597,2,0.67,1,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-021-00130-8.pdf",""
2,"Bernd Carsten Stahl, Tonii Leach, Oluyinka Oyeniji, George Ogoh","AI Policy as a Response to AI Ethics? Addressing Ethical Issues in the Development of AI Policies in North Africa",2023,"Social and Cultural Studies of Robots and AI","Springer International Publishing","https://doi.org/10.1007/978-3-031-08215-3_7","",166,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-08215-3_7","2523-8523","",,,141,167,2,1.00,1,4,2,"Abstract: The recent exponential rate of AI development has led to a proliferation of AI national policies and strategies as global power blocs have sought to consolidate positions of strategic dominance. These policies have sought to promote the benefits and mitigate the risks of AI—and address ever more serious ethical concerns about these technologies. This raises the question of how countries less influential in the sphere of AI might seek to address such ethical issues themselves, and whether lessons can be learned from existing policies in addressing issues of ethics and human rights. From a consideration of the current AI ethics discourse and existing AI policies, the chapter goes on to explore how ethical concerns are addressed in the North African AI strategies and which gaps and opportunities in terms of coverage of ethical issues arise from the current state of these policies. We suggest that ethical issues should be addressed clearly in policy at the earliest possible stage to ensure that ethical standards are internally produced in line with social and cultural values, rather than being de facto applied by external actors.","https://link.springer.com/content/pdf/10.1007/978-3-031-08215-3_7",""
2,"Raquel Iniesta","The human role to guarantee an ethical AI in healthcare: a five-facts approach",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00353-x","",188,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00353-x","2730-5953","",,,,,2,1.00,2,1,2,"Abstract: With the emergence of AI systems to assist clinical decision-making, several ethical dilemmas are brought to the general attention. AI systems are claimed to be the solution for many high-skilled medical tasks where machines can potentially surpass human ability as for example in identifying normal and abnormal chest X-rays. However, there are also warns that AI tools could be the basis for a human replacement that can risk dehumanisation in medicine. In recent years, important proposals in the domain of AI ethics in healthcare have identified main ethical issues, as for example fairness, autonomy, transparency, and responsibility. The human warranty, which implies human evaluation of the AI procedures, has been described to lower the ethical risks. However, as relevant these works have been, translating principles into action has proved challenging as existing codes were mostly a description of principles. There is a great need to produce","https://link.springer.com/content/pdf/10.1007/s43681-023-00353-x.pdf",""
2,"Kjell Jørgen Hole","Tools with general AI and no existential risk",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00271-y","",189,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00271-y","2730-5953","",4,2,345,352,2,1.00,2,1,2,"Abstract: According to philosophers and scientists in artificial intelligence (AI), future autonomous agents with general AI constitute an existential risk to humanity. This paper leverages results from neuroscience to propose tools with general AI and no existential risk. Tools answering questions in different domains enable the safe exploration of general AI’s enormous potential.","https://link.springer.com/content/pdf/10.1007/s43681-023-00271-y.pdf",""
2,"Mustafa Ali Khalaf","Does attitude towards plagiarism predict aigiarism using ChatGPT?",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00426-5","",254,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00426-5","2730-5953","",,,,,2,2.00,2,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00426-5.pdf",""
2,"Eduardo Vyhmeister, Gabriel Castane, P.-O. Östberg, Simon Thevenin","A responsible AI framework: pipeline contextualisation",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00154-8","",292,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00154-8","2730-5953","",3,1,175,197,2,0.67,1,4,3,"Abstract: Incorporating ethics and values within the life cycle of an AI asset means securing its development, deployment, use, and decommission under these perspectives. These approaches depend on the","https://link.springer.com/content/pdf/10.1007/s43681-022-00154-8.pdf",""
2,"David De Cremer, Devesh Narayanan","On educating ethics in the AI era: why business schools need to move beyond digital upskilling, towards ethical upskilling",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00306-4","",297,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00306-4","2730-5953","",3,4,1037,1041,2,1.00,1,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00306-4.pdf",""
2,"Dominik Vrabič Dežman","Promising the future, encoding the past: AI hype and public media imagery",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00474-x","",305,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00474-x","2730-5953","",4,3,743,756,2,2.00,2,1,1,"Abstract: In recent years, “AI hype” has taken over public media, oscillating between sensationalism and concerns about the societal implications of AI growth. The latest historical wave of AI hype indexes a period of increased research, investment, and speculation on machine learning, centred around generative AI, a novel class of machine learning that can generate original media from textual prompts. In this paper, I dive into the production of AI hype in online media, with the aim of prioritising the normative and political dimension of AI hype. Formulating AI as a promise reframes it as a normative project, centrally involving the formation of public and institutional confidence in the technology. The production and dissemination of images, in this context, plays a pivotal role in reinforcing these normative commitments to the public. My argument is divided into four sections. First, I examine the political relevance of stock images as the dominant imagery used to convey AI concepts to the public. These stock images encode specific readings of AI and circulate through public media, significantly influencing perceptions. Second, I look at the dominant images of AI as matters of political concern. Third, as generative AI increasingly contributes to the production of stock imagery, I compare the epistemic work performed by AI-generated outputs and stock images, as both encode style, content, and taxonomic structures of the world. I employ an entity relationship diagram (ERD) to investigate the political economy of AI imagery in digital media, providing a snapshot of how AI hype is materialised and amplified online. With this study, I reaffirm AI’s normative character at the forefront of its political and ethical discourse.","https://link.springer.com/content/pdf/10.1007/s43681-024-00474-x.pdf",""
2,"Jorge Luis Morton","On inscription and bias: data, actor network theory, and the social problems of text-to-image AI models",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00431-8","",326,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00431-8","2730-5953","",,,,,2,2.00,2,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00431-8.pdf",""
2,"David B. Resnik, Mohammad Hosseini","The ethics of using artificial intelligence in scientific research: new guidance needed for a new tool",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00493-8","",344,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00493-8","2730-5953","",,,,,2,2.00,1,2,1,"Abstract: Using artificial intelligence (AI) in research offers many important benefits for science and society but also creates novel and complex ethical issues. While these ethical issues do not necessitate changing established ethical norms of science, they require the scientific community to develop new guidance for the appropriate use of AI. In this article, we briefly introduce AI and explain how it can be used in research, examine some of the ethical issues raised when using it, and offer nine recommendations for responsible use, including: (1) Researchers are responsible for identifying, describing, reducing, and controlling AI-related biases and random errors; (2) Researchers should disclose, describe, and explain their use of AI in research, including its limitations, in language that can be understood by non-experts; (3) Researchers should engage with impacted communities, populations, and other stakeholders concerning the use of AI in research to obtain their advice and assistance and address their interests and concerns, such as issues related to bias; (4) Researchers who use synthetic data should (a) indicate which parts of the data are synthetic; (b) clearly label the synthetic data; (c) describe how the data were generated; and (d) explain how and why the data were used; (5) AI systems should not be named as authors, inventors, or copyright holders but their contributions to research should be disclosed and described; (6) Education and mentoring in responsible conduct of research should include discussion of ethical use of AI.","https://link.springer.com/content/pdf/10.1007/s43681-024-00493-8.pdf",""
2,"Fallon J. Cochlin, Charles D. Curran, Cason D. Schmit","<b>Unlocking Public Health Data:</b> Navigating New Legal Guardrails and Emerging AI Challenges",2024,"Journal of Law, Medicine &amp; Ethics","Cambridge University Press (CUP)","https://doi.org/10.1017/jme.2024.40","",378,"2025-02-04 16:55:17","journal-article","10.1017/jme.2024.40","1073-1105","",52,,70,74,2,2.00,1,3,1,"Abstract: Here, we analyze the public health implications of recent legal developments — including privacy legislation, intergovernmental data exchange, and artificial intelligence governance — with a view toward the future of public health informatics and the potential of diverse data to inform public health actions and drive population health outcomes.","https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S1073110524000408",""
2,"Arunima Chakraborty, Nisigandha Bhuyan","Can artificial intelligence be a Kantian moral agent? On moral autonomy of AI system",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00269-6","",386,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00269-6","2730-5953","",4,2,325,331,2,1.00,1,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00269-6.pdf",""
2,"Koki Arai, Masakazu Matsumoto","Public perceptions of autonomous lethal weapons systems",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00282-9","",417,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00282-9","2730-5953","",4,2,451,462,2,1.00,1,2,2,"Abstract: This study attempts to bridge the gap in empirical and philosophical research on lethal autonomous weapons systems (LAWS), through a survey of attitudes using experimental methods. “LAWS” refer to “fully autonomous weapons” that can set attack targets without human involvement and are lethal. Based on previous research, we conducted a randomized controlled experiment to create, present, and collect responses to scenarios describing military operations and outcomes that are likely to express awareness of the ethical issues raised by LAWS. First, our hypothesis that LAWS are less likely to be used was rejected, and the opposite trend was observed. Second, the hypothesis that civilian casualties rather than combatant casualties would influence LAWS use was strongly and significantly confirmed. Third, the hypothesis that remote weapons are more likely to be used than LAWS was rejected. Fourth, there was some support for the hypothesis that LAWS are more likely to be used in homeland defense. Fifth, the hypothesis that male and younger individuals are more willing to use LAWS was strongly and significantly confirmed for male, but not on the basis of age. This study highlights the need for further discussion based on these findings.","https://link.springer.com/content/pdf/10.1007/s43681-023-00282-9.pdf",""
2,"Simisola Johnson","Racing into the fourth industrial revolution: exploring the ethical dimensions of medical AI and rights-based regulatory framework",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00153-9","",440,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00153-9","2730-5953","",2,1,227,232,2,0.67,2,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00153-9.pdf",""
2,"David M. Douglas, Justine Lacey, David Howard","Ethical risks of AI-designed products: bespoke surgical tools as a case study",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00219-8","",441,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00219-8","2730-5953","",3,4,1117,1133,2,0.67,1,3,3,"Abstract: An emerging use of machine learning (ML) is creating products optimised using computational design for individual users and produced using 3D printing. One potential application is bespoke surgical tools optimised for specific patients. While optimised tool designs benefit patients and surgeons, there is the risk that computational design may also create unexpected designs that are unsuitable for use with potentially harmful consequences. We interviewed potential stakeholders to identify both established and unique technical risks associated with the use of computational design for surgical tool design and applied ethical risk analysis (eRA) to identify how stakeholders might be exposed to ethical risk within this process. The main findings of this research are twofold. First, distinguishing between unique and established risks for new medical technologies helps identify where existing methods of risk mitigation may be applicable to a surgical innovation, and where new means of mitigating risks may be needed. Second, the value of distinguishing between technical and ethical risks in such a system is that it identifies the key responsibilities for managing these risks and allows for any potential interdependencies between stakeholders in managing these risks to be made explicit. The approach demonstrated in this paper may be applied to understanding the implications of new AI and ML applications in healthcare and other high consequence domains.","https://link.springer.com/content/pdf/10.1007/s43681-022-00219-8.pdf",""
2,"Tobias Flattery","The Kant-inspired indirect argument for non-sentient robot rights",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00304-6","",449,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00304-6","2730-5953","",4,4,997,1011,2,1.00,2,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00304-6.pdf",""
2,"Joshua L. M. Brand","Why reciprocity prohibits autonomous weapons systems in war",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00193-1","",452,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00193-1","2730-5953","",3,2,619,624,2,0.67,2,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00193-1.pdf",""
2,"Inyoung Cheong, Aylin Caliskan, Tadayoshi Kohno","Safeguarding human values: rethinking US law for generative AI’s societal impacts",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00451-4","",460,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00451-4","2730-5953","",,,,,2,2.00,1,3,1,"Abstract: Our interdisciplinary study examines the effectiveness of US law in addressing the complex challenges posed by generative AI systems to fundamental human values, including physical and mental well-being, privacy, autonomy, diversity, and equity. Through the analysis of diverse hypothetical scenarios developed in collaboration with experts, we identified significant shortcomings and ambiguities within the existing legal protections. Constitutional and civil rights law currently struggles to hold AI companies responsible for AI-assisted discriminatory outputs. Moreover, even without considering the liability shield provided by Section 230, existing liability laws may not effectively remedy unintentional and intangible harms caused by AI systems. Demonstrating causal links for liability claims such as defamation or product liability proves exceptionally difficult due to the intricate and opaque nature of these systems. To effectively address these unique and evolving risks posed by generative AI, we propose a “Responsible AI Legal Framework” that adapts to recognize new threats and utilizes a multi-pronged approach. This framework would enshrine fundamental values in legal frameworks, establish comprehensive safety guidelines, and implement liability models tailored to the complexities of human-AI interactions. By proactively mitigating unforeseen harms like mental health impacts and privacy breaches, this framework aims to create a legal landscape capable of navigating the exciting yet precarious future brought forth by generative AI technologies.","https://link.springer.com/content/pdf/10.1007/s43681-024-00451-4.pdf",""
2,"","EUROPEAN UNION’S ETHICS GUIDELINES FOR AI",2023,"Trustworthy AI Alone Is Not Enough.","Dykinson","https://doi.org/10.2307/jj.8500773.5","",472,"2025-02-04 16:55:17","book-chapter","10.2307/jj.8500773.5","","",,,17,21,2,1.00,0,0,2,"","",""
2,"Jeff J. H. Kim, Richard S. Um, James W. Y. Lee, Olusola Ajilore","Generative AI can fabricate advanced scientific visualizations: ethical implications and strategic mitigation framework",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00439-0","",473,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00439-0","2730-5953","",,,,,2,2.00,1,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00439-0.pdf",""
2,"Danton Char","Challenges of Local Ethics Review in a Global Healthcare AI Market",2022,"The American Journal of Bioethics","Informa UK Limited","https://doi.org/10.1080/15265161.2022.2055214","",499,"2025-02-04 16:55:17","journal-article","10.1080/15265161.2022.2055214","1526-5161","",22,5,39,41,2,0.67,2,1,3,"","https://www.tandfonline.com/doi/pdf/10.1080/15265161.2022.2055214",""
2,"Mahdi Khalili","Against the opacity, and for a qualitative understanding, of artificially intelligent technologies",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00332-2","",513,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00332-2","2730-5953","",4,4,1013,1021,2,1.00,2,1,2,"Abstract: This paper aims, first, to argue against using opaque AI technologies in decision making processes, and second to suggest that we need to possess a qualitative form of understanding about them. It first argues that opaque artificially intelligent technologies are suitable for users who remain indifferent to the understanding of decisions made by means of these technologies. According to virtue ethics, this implies that these technologies are not well-suited for those who care about realizing their moral capacity. The paper then draws on discussions on scientific understanding to suggest that an AI technology becomes understandable to its users when they are provided with a qualitative account of the consequences of using it. As a result, explainable AI methods can render an AI technology understandable to its users by presenting the qualitative implications of employing the technology for their lives.","https://link.springer.com/content/pdf/10.1007/s43681-023-00332-2.pdf",""
2,"Federico Sabbatini, Roberta Calegari","On the evaluation of the symbolic knowledge extracted from black boxes",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00406-1","",554,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00406-1","2730-5953","",4,1,65,74,2,2.00,1,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00406-1.pdf",""
2,"Seng W. Loke","Designed to cooperate: a Kant-inspired ethic of machine-to-machine cooperation",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00238-5","",559,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00238-5","2730-5953","",3,3,991,996,2,0.67,2,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00238-5.pdf",""
2,"Germaine Tchuente Foguem, Aurelien Teguede Keleko","Artificial intelligence applied in pulmonary hypertension: a bibliometric analysis",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00267-8","",561,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00267-8","2730-5953","",3,4,1063,1093,2,1.00,1,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00267-8.pdf",""
2,"Giuseppe Placidi","Ethical issues deriving from the delayed adoption of artificial intelligence in medical imaging",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00139-7","",566,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00139-7","2730-5953","",2,4,599,602,2,0.67,2,1,3,"Abstract: Medical imaging (MI) has assumed a central role in medicine. Artificial intelligence (AI) has revolutionized computer vision and it is also approaching to impact deeply MI. Fundamental ethical matters have raised and teams of experts around the world are involved in defining ethical borders for AI in MI. However, reading the extremely detailed proposals, it is clear that the treated ethical arguments have been completely redefined and specifically structured for AI in MI. Instead, many of them should be inherited from other technologies already in use in MI. The complete re-definition of ethical principles could produce contradictions and delays for AI adoption in MI, thus arising important ethical concerns. In this paper, potential ethical issues related to AI delay are presented: the objective is to contribute to reuse some concepts from other technologies to streamline the arguments and avoid these concerns.","https://link.springer.com/content/pdf/10.1007/s43681-022-00139-7.pdf",""
2,"James Steinhoff","AI ethics as subordinated innovation network",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01658-5","",581,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01658-5","0951-5666","",39,4,1995,2007,2,1.00,2,1,2,"Abstract: AI ethics is proposed, by the Big Tech companies which lead AI research and development, as the cure for diverse social problems posed by the commercialization of data-intensive technologies. It aims to reconcile capitalist AI production with ethics. However, AI ethics is itself now the subject of wide criticism; most notably, it is accused of being no more than “ethics washing” a cynical means of dissimulation for Big Tech, while it continues its business operations unchanged. This paper aims to critically assess, and go beyond the ethics washing thesis. I argue that AI ethics is indeed ethics washing, but not only that. It has a more significant economic function for Big Tech. To make this argument I draw on the theory of intellectual monopoly capital. I argue that ethics washing is better understood as a subordinated innovation network: a dispersed network of contributors beyond Big Tech’s formal employment whose research is indirectly planned by Big Tech, which also appropriates its results. These results are not intended to render AI more ethical, but rather to advance the business processes of data-intensive capital. Because the parameters of AI ethics are indirectly set in advance by Big tech, the ostensible goal that AI ethics sets for itself—to resolve the contradiction between business and ethics—is in fact insoluble. I demonstrate this via an analysis of the latest trend in AI ethics: the operationalization of ethical principles.","https://link.springer.com/content/pdf/10.1007/s00146-023-01658-5.pdf",""
2,"Manisha Kusuma, Vikram Mohanty, Marx Wang, Kurt Luther","Civil War Twin",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3534141","",582,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3534141","","",,,,,2,0.67,1,4,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3534141",""
2,"Christian Sieberichs, Simon Geerkens, Alexander Braun, Thomas Waschulzik","ECS: an interactive tool for data quality assurance",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00393-3","",585,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00393-3","2730-5953","",4,1,131,139,2,2.00,1,4,1,"Abstract: With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper, we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.","https://link.springer.com/content/pdf/10.1007/s43681-023-00393-3.pdf",""
2,"Blair Attard-Frost, Andrés De los Ríos, Deneille R. Walters","The Ethics of AI Business Practices: A Review of 47 AI Ethics Guidelines",2022,"SSRN Electronic Journal","Elsevier BV","https://doi.org/10.2139/ssrn.4034804","",590,"2025-02-04 16:55:17","journal-article","10.2139/ssrn.4034804","1556-5068","",,,,,2,0.67,1,3,3,"","",""
2,"Blanca Rodríguez-López, Jon Rueda","Artificial moral experts: asking for ethical advice to artificial intelligent assistants",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00246-5","",597,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00246-5","2730-5953","",3,4,1371,1379,2,1.00,1,2,2,"Abstract: In most domains of human life, we are willing to accept that there are experts with greater knowledge and competencies that distinguish them from non-experts or laypeople. Despite this fact, the very recognition of expertise curiously becomes more controversial in the case of “moral experts”. Do moral experts exist? And, if they indeed do, are there ethical reasons for us to follow their advice? Likewise, can emerging technological developments broaden our very concept of moral expertise? In this article, we begin by arguing that the objections that have tried to deny the existence (and convenience) of moral expertise are unsatisfactory. After that, we show that people have ethical reasons to ask for a piece of moral advice in daily life situations. Then, we argue that some Artificial Intelligence (AI) systems can play an increasing role in human morality by becoming moral experts. Some AI-based moral assistants can qualify as artificial moral experts and we would have good ethical reasons to use them.","https://link.springer.com/content/pdf/10.1007/s43681-022-00246-5.pdf",""
2,"Ehtesham Hashmi, Muhammad Mudassar Yamin, Sule Yildirim Yayilgan","Securing tomorrow: a comprehensive survey on the synergy of Artificial Intelligence and information security",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00529-z","",617,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00529-z","2730-5953","",,,,,2,2.00,1,3,1,"Abstract: This survey paper explores the transformative role of Artificial Intelligence (AI) in information security. Traditional methods, especially rule-based approaches, faced significant challenges in protecting sensitive data from ever-changing cyber threats, particularly with the rapid increase in data volume. This study thoroughly evaluates AI’s application in information security, discussing its strengths and weaknesses. It provides a detailed review of AI’s impact on information security, examining various AI algorithms used in this field, such as supervised, unsupervised, and reinforcement learning, and highlighting their respective strengths and limitations. The study identifies key areas for future AI research in information security, focusing on improving algorithms, strengthening information security, addressing ethical issues, and exploring safety and security-related concerns. It emphasizes significant security risks, including vulnerability to adversarial attacks, and aims to enhance the robustness and reliability of AI systems in protecting sensitive information by proposing solutions for potential threats. The findings aim to benefit cybersecurity professionals and researchers by offering insights into the intricate relationship between AI, information security, and emerging technologies.","https://link.springer.com/content/pdf/10.1007/s43681-024-00529-z.pdf",""
2,"Christoph Bartneck, Kumar Yogeeswaran, Chris G. Sibley","Personality and demographic correlates of support for regulating artificial intelligence",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00279-4","",624,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00279-4","2730-5953","",4,2,419,426,2,1.00,1,3,2,"Abstract: The arrival of artificial intelligence (AI) in our society has sparked many hopes and fears, with people having diverging views on the need to strictly regulate AI. The current study investigates how demographic and personality traits are associated with a desire to strictly regulate AI using a representative sample of adults from New Zealand (","https://link.springer.com/content/pdf/10.1007/s43681-023-00279-4.pdf",""
2,"Robin Chan, Radin Dardashti, Meike Osinski, Matthias Rottmann, Dominik Brüggemann, Cilia Rücker, Peter Schlicht, Fabian Hüger, Nikol Rummel, Hanno Gottschalk","What should AI see? Using the public’s opinion to determine the perception of an AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00248-3","",636,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00248-3","2730-5953","",3,4,1381,1405,2,1.00,0,10,2,"Abstract: Deep neural networks (DNN) have made impressive progress in the interpretation of image data so that it is conceivable and to some degree realistic to use them in safety critical applications like automated driving. From an ethical standpoint, the AI algorithm should take into account the vulnerability of objects or subjects on the street that ranges from “not at all”, e.g. the road itself, to “high vulnerability” of pedestrians. One way to take this into account is to define the cost of confusion of one semantic category with another and use cost-based decision rules for the interpretation of probabilities, which are the output of DNNs. However, it is an open problem how to define the cost structure, who should be in charge to do that, and thereby define what AI-algorithms will actually “see”. As one possible answer, we follow a participatory approach and set up an online survey to ask the public to define the cost structure. We present the survey design and the data acquired along with an evaluation that also distinguishes between perspective (car passenger vs. external traffic participant) and gender. Using simulation based","https://link.springer.com/content/pdf/10.1007/s43681-022-00248-3.pdf",""
2,"Enrico Barbierato, Maria Enrica Zamponi","Shifting Perspectives on AI Evaluation: The Increasing Role of Ethics in Cooperation",2022,"AI","MDPI AG","https://doi.org/10.3390/ai3020021","",640,"2025-02-04 16:55:17","journal-article","10.3390/ai3020021","2673-2688","",3,2,331,352,2,0.67,1,2,3,"Evaluating AI is a challenging task, as it requires an operative definition of intelligence and the metrics to quantify it, including amongst other factors economic drivers, depending on specific domains. From the viewpoint of AI basic research, the ability to play a game against a human has historically been adopted as a criterion of evaluation, as competition can be characterized by an algorithmic approach. Starting from the end of the 1990s, the deployment of sophisticated hardware identified a significant improvement in the ability of a machine to play and win popular games. In spite of the spectacular victory of IBM’s Deep Blue over Garry Kasparov, many objections still remain. This is due to the fact that it is not clear how this result can be applied to solve real-world problems or simulate human abilities, e.g., common sense, and also exhibit a form of generalized AI. An evaluation based uniquely on the capacity of playing games, even when enriched by the capability of learning complex rules without any human supervision, is bound to be unsatisfactory. As the internet has dramatically changed the cultural habits and social interaction of users, who continuously exchange information with intelligent agents, it is quite natural to consider cooperation as the next step in AI software evaluation. Although this concept has already been explored in the scientific literature in the fields of economics and mathematics, its consideration in AI is relatively recent and generally covers the study of cooperation between agents. This paper focuses on more complex problems involving heterogeneity (specifically, the cooperation between humans and software agents, or even robots), which are investigated by taking into account ethical issues occurring during attempts to achieve a common goal shared by both parties, with a possible result of either conflict or stalemate. The contribution of this research consists in identifying those factors (trust, autonomy, and cooperative learning) on which to base ethical guidelines in agent software programming, making cooperation a more suitable benchmark for AI applications.","https://www.mdpi.com/2673-2688/3/2/21/pdf",""
2,"Jan Segessenmann, Thilo Stadelmann, Andrew Davison, Oliver Dürr","Assessing deep learning: a work program for the humanities in the age of artificial intelligence",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00408-z","",651,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00408-z","2730-5953","",,,,,2,1.00,1,4,2,"Abstract: Following the success of deep learning (DL) in research, we are now witnessing the fast and widespread adoption of artificial intelligence (AI) in daily life, influencing the way we act, think, and organize our lives. However, much still remains a mystery when it comes to how these systems achieve such high performance and why they reach the outputs they do. This presents us with an unusual combination: of technical mastery on the one hand, and a striking degree of mystery on the other. This conjunction is not only fascinating, but it also poses considerable risks, which urgently require our attention. Awareness of the need to analyze ethical implications, such as fairness, equality, and sustainability, is growing. However, other dimensions of inquiry receive less attention, including the subtle but pervasive ways in which our dealings with AI shape our way of living and thinking, transforming our culture and human self-understanding. If we want to deploy AI positively in the long term, a broader and more holistic assessment of the technology is vital, involving not only scientific and technical perspectives, but also those from the humanities. To this end, we present outlines of a","https://link.springer.com/content/pdf/10.1007/s43681-023-00408-z.pdf",""
2,"Vivek Nallur, Graham Finlay","Empathetic AI for ethics-in-the-small",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01466-3","",664,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01466-3","0951-5666","",38,2,973,974,2,0.67,1,2,3,"","https://link.springer.com/content/pdf/10.1007/s00146-022-01466-3.pdf",""
2,"Guilherme Giantini","The sophistry of the neutral tool. Weaponizing artificial intelligence and big data into threats toward social exclusion",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00311-7","",670,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00311-7","2730-5953","",3,4,1049,1061,2,1.00,2,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00311-7.pdf",""
2,"Swati Chakraborty","AI and Ethics",2023,"Advances in Human and Social Aspects of Technology","IGI Global","https://doi.org/10.4018/978-1-6684-9196-6.ch002","",721,"2025-02-04 16:55:17","book-chapter","10.4018/978-1-6684-9196-6.ch002","2328-1316","",,,25,33,2,1.00,2,1,2,"Artificial intelligence (AI) is rapidly transforming our world and bringing about new possibilities and advancements in various industries. However, with this new technology also comes ethical considerations and challenges. Navigating the moral landscape of AI is crucial in ensuring that its development and implementation align with the values and principles of society. One major ethical concern in the development of AI is its potential to cause harm to humans. For example, biased algorithms in decision-making processes can lead to discrimination and unequal treatment of certain individuals or groups. In addition, the use of AI in areas such as autonomous weapons raises serious questions about accountability and responsibility in the event of harm. Ensuring that AI systems are transparent, explainable, and free from bias is crucial in avoiding negative consequences.","https://www.igi-global.com/viewtitle.aspx?TitleId=331955",""
2,"Abhishek Mishra, Julian Savulescu, Alberto Giubilini","The Ethics of Medical AI",2022,"Oxford Handbook of Digital Ethics","Oxford University Press","https://doi.org/10.1093/oxfordhb/9780198857815.013.25","",734,"2025-02-04 16:55:17","book-chapter","10.1093/oxfordhb/9780198857815.013.25","","",,,487,507,2,0.67,1,3,3,"Abstract: Over the past few years, artificial intelligence (AI) solutions have been increasingly explored and developed for health-care settings. This trend is only expected to accelerate. ‘AI’ in this chapter refers primarily to deep learning models and the chapter considers the main ethical concerns that arise on the basis of (a) bias, discrimination, and fairness; (b) patient-centred medicine; (c) AI and value-based decision-making; (d) responsibility, accountability, and explanation; and (e) the broader long-term societal effects of AI in health care. The various questions posed under these themes are considered, the main arguments are assembled, and directions for future research are considered.","https://academic.oup.com/edited-volume/37078/chapter/387347163",""
2,"Caterina Berbenni-Rehm","Evidence-based AI, ethics and the circular economy of knowledge",2022,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-022-01581-1","",750,"2025-02-04 16:55:17","journal-article","10.1007/s00146-022-01581-1","0951-5666","",38,2,889,895,2,0.67,2,1,3,"","https://link.springer.com/content/pdf/10.1007/s00146-022-01581-1.pdf",""
2,"Kriti Ahuja","Emotion AI in healthcare: Application, challenges, and future directions",2024,"Emotional AI and Human-AI Interactions in Social Networking","Elsevier","https://doi.org/10.1016/b978-0-443-19096-4.00011-0","",751,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-19096-4.00011-0","","",,,131,146,2,2.00,2,1,1,"","https://api.elsevier.com/content/article/PII:B9780443190964000110",""
2,"Michael Klenk","Ethics of generative AI and manipulation: a design-oriented research agenda",2024,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-024-09745-x","",759,"2025-02-04 16:55:17","journal-article","10.1007/s10676-024-09745-x","1388-1957","",26,1,,,2,2.00,2,1,1,"Abstract: Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies.","https://link.springer.com/content/pdf/10.1007/s10676-024-09745-x.pdf",""
2,"Giovanni Rubeis","Ethics of Medical AI",2024,"The International Library of Ethics, Law and Technology","Springer International Publishing","https://doi.org/10.1007/978-3-031-55744-6","",762,"2025-02-04 16:55:17","book","10.1007/978-3-031-55744-6","1875-0044","",,,,,2,2.00,2,1,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-55744-6.pdf",""
2,"Mark Coeckelbergh","The case for global governance of AI: arguments, counter-arguments, and challenges ahead",2024,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-01949-5","",775,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-01949-5","0951-5666","",,,,,2,2.00,2,1,1,"Abstract: It is increasingly recognized that as artificial intelligence becomes more powerful and pervasive in society and creates risks and ethical issues that cross borders, a global approach is needed for the governance of these risks. But why, exactly, do we need this and what does that mean? In this Open Forum paper, author argues for global governance of AI for moral reasons but also outlines the governance challenges that this project raises.","https://link.springer.com/content/pdf/10.1007/s00146-024-01949-5.pdf",""
2,"Keith Begley","Beta-testing the ethics plugin",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01630-3","",781,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01630-3","0951-5666","",38,4,1503,1505,2,1.00,2,1,2,"","https://link.springer.com/content/pdf/10.1007/s00146-023-01630-3.pdf",""
2,"Eryn Rigley, Adriane Chapman, Christine Evers, Will McNeill","Anthropocentrism and Environmental Wellbeing in AI Ethics Standards: A Scoping Review and Discussion",2023,"AI","MDPI AG","https://doi.org/10.3390/ai4040043","",785,"2025-02-04 16:55:17","journal-article","10.3390/ai4040043","2673-2688","",4,4,844,874,2,1.00,1,4,2,"As AI deployment has broadened, so too has an awareness for the ethical implications and problems that may ensue from this deployment. In response, groups across multiple domains have issued AI ethics standards that rely on vague, high-level principles to find consensus. One such high-level principle that is common across the AI landscape is ‘human-centredness’, though oftentimes it is applied without due investigation into its merits and limitations and without a clear, common definition. This paper undertakes a scoping review of AI ethics standards to examine the commitment to ‘human-centredness’ and how this commitment interacts with other ethical concerns, namely, concerns for nonhumans animals and environmental wellbeing. We found that human-centred AI ethics standards tend to prioritise humans over nonhumans more so than nonhuman-centred standards. A critical analysis of our findings suggests that a commitment to human-centredness within AI ethics standards accords with the definition of anthropocentrism in moral philosophy: that humans have, at least, more intrinsic moral value than nonhumans. We consider some of the limitations of anthropocentric AI ethics, which include permitting harm to the environment and animals and undermining the stability of ecosystems.","https://www.mdpi.com/2673-2688/4/4/43/pdf",""
2,"Kostas Karpouzis","Explainable AI for Intelligent Tutoring Systems",2024,"Frontiers of Artificial Intelligence, Ethics and Multidisciplinary Applications","Springer Nature Singapore","https://doi.org/10.1007/978-981-99-9836-4_6","",803,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-99-9836-4_6","2731-8125","",,,59,70,2,2.00,2,1,1,"","https://link.springer.com/content/pdf/10.1007/978-981-99-9836-4_6",""
2,"Satvik Tripathi, Alisha Augustin, Farouk Dako, Edward Kim","Turing test-inspired method for analysis of biases prevalent in artificial intelligence-based medical imaging",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00227-8","",812,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00227-8","2730-5953","",3,4,1193,1201,2,0.67,1,4,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00227-8.pdf",""
2,"Lukas Weidener, Michael Fischer","Role of Ethics in Developing AI-Based Applications in Medicine: Insights From Expert Interviews and Discussion of Implications",2024,"JMIR AI","JMIR Publications Inc.","https://doi.org/10.2196/51204","",815,"2025-02-04 16:55:17","journal-article","10.2196/51204","2817-1705","",3,,,,2,2.00,1,2,1,"Background: The integration of artificial intelligence (AI)–based applications in the medical field has increased significantly, offering potential improvements in patient care and diagnostics. However, alongside these advancements, there is growing concern about ethical considerations, such as bias, informed consent, and trust in the development of these technologies. Objective: This study aims to assess the role of ethics in the development of AI-based applications in medicine. Furthermore, this study focuses on the potential consequences of neglecting ethical considerations in AI development, particularly their impact on patients and physicians. Methods: Qualitative content analysis was used to analyze the responses from expert interviews. Experts were selected based on their involvement in the research or practical development of AI-based applications in medicine for at least 5 years, leading to the inclusion of 7 experts in the study. Results: The analysis revealed 3 main categories and 7 subcategories reflecting a wide range of views on the role of ethics in AI development. This variance underscores the subjectivity and complexity of integrating ethics into the development of AI in medicine. Although some experts view ethics as fundamental, others prioritize performance and efficiency, with some perceiving ethics as potential obstacles to technological progress. This dichotomy of perspectives clearly emphasizes the subjectivity and complexity surrounding the role of ethics in AI development, reflecting the inherent multifaceted nature of this issue. Conclusions: Despite the methodological limitations impacting the generalizability of the results, this study underscores the critical importance of consistent and integrated ethical considerations in AI development for medical applications. It advocates further research into effective strategies for ethical AI development, emphasizing the need for transparent and responsible practices, consideration of diverse data sources, physician training, and the establishment of comprehensive ethical and legal frameworks.","",""
2,"Suzanne Tolmeijer, Vicky Arpatzoglou, Luca Rossetto, Abraham Bernstein","Trolleys, crashes, and perception—a survey on how current autonomous vehicles debates invoke problematic expectations",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00284-7","",816,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00284-7","2730-5953","",4,2,473,484,2,1.00,1,4,2,"Abstract: Ongoing debates about ethical guidelines for autonomous vehicles mostly focus on variations of the ‘Trolley Problem’. Using variations of this ethical dilemma in preference surveys, possible implications for autonomous vehicles policy are discussed. In this work, we argue that the lack of realism in such scenarios leads to limited practical insights. We run an ethical preference survey for autonomous vehicles by including more realistic features, such as time pressure and a non-binary decision option. Our results indicate that such changes lead to different outcomes, calling into question how the current outcomes can be generalized. Additionally, we investigate the framing effects of the capabilities of autonomous vehicles and indicate that ongoing debates need to set realistic expectations on autonomous vehicle challenges. Based on our results, we call upon the field to re-frame the current debate towards more realistic discussions beyond the Trolley Problem and focus on which autonomous vehicle behavior is considered","https://link.springer.com/content/pdf/10.1007/s43681-023-00284-7.pdf",""
2,"Noah Schöppl, Mariarosaria Taddeo, Luciano Floridi","Ethics Auditing: Lessons from Business Ethics for Ethics Auditing of AI",2022,"Digital Ethics Lab Yearbook","Springer International Publishing","https://doi.org/10.1007/978-3-031-09846-8_13","",818,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09846-8_13","2524-7719","",,,209,227,2,0.67,1,3,3,"","https://link.springer.com/content/pdf/10.1007/978-3-031-09846-8_13",""
2,"Kirsten Martin, Bidhan Parmar","What Firms Must Know Before Adopting AI: The Ethics of AI Transparency",2022,"SSRN Electronic Journal","Elsevier BV","https://doi.org/10.2139/ssrn.4207128","",848,"2025-02-04 16:55:17","journal-article","10.2139/ssrn.4207128","1556-5068","",,,,,2,0.67,1,2,3,"","",""
2,"Angela Faragasso, Fabio Bonsignorio","Reproducibility challenges in robotic surgery",2023,"Frontiers in Robotics and AI","Frontiers Media SA","https://doi.org/10.3389/frobt.2023.1127972","",876,"2025-02-04 16:55:17","journal-article","10.3389/frobt.2023.1127972","2296-9144","",10,,,,2,1.00,1,2,2,"Reproducibility of results is, in all research fields, the cornerstone of the scientific method and the minimum standard for assessing the value of scientific claims and conclusions drawn by other scientists. It requires a systematic approach and accurate description of the experimental procedure and data analysis, which allows other scientists to follow the steps described in the published work and obtain the “same results.” In general and in different research contexts with “same” results, we mean different things. It can be almost identical measures in a fully deterministic experiment or “validation of a hypothesis” or statistically similar results in a non-deterministic context. Unfortunately, it has been shown by systematic meta-analysis studies that many findings in fields like psychology, sociology, medicine, and economics do not hold up when other researchers try to replicate them. Many scientific fields are experiencing what is generally referred to as a “reproducibility crisis,” which undermines the trust in published results, imposes a thorough revision of the methodology in scientific research, and makes progress difficult. In general, the reproducibility of experiments is not a mainstream practice in artificial intelligence and robotics research. Surgical robotics is no exception. There is a need for developing new tools and putting in place a community effort to allow the transition to more reproducible research and hence faster progress in research. Reproducibility, replicability, and benchmarking (operational procedures for the assessment and comparison of research results) are made more complex for medical robotics and surgical systems, due to patenting, safety, and ethical issues. In this review paper, we selected 10 relevant published manuscripts on surgical robotics to analyze their clinical applicability and underline the problems related to reproducibility of the reported experiments, with the aim of finding possible solutions to the challenges that limit the translation of many scientific research studies into real-world applications and slow down research progress.","https://www.frontiersin.org/articles/10.3389/frobt.2023.1127972/full",""
2,"Nathan Gabriel Wood","Explainable AI in the military domain",2024,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-024-09762-w","",886,"2025-02-04 16:55:17","journal-article","10.1007/s10676-024-09762-w","1388-1957","",26,2,,,2,2.00,2,1,1,"Abstract: Artificial intelligence (AI) has become nearly ubiquitous in modern society, from components of mobile applications to medical support systems, and everything in between. In societally impactful systems imbued with AI, there has been increasing concern related to opaque AI, that is, artificial intelligence where it is unclear how or why certain decisions are reached. This has led to a recent boom in research on “explainable AI” (XAI), or approaches to making AI more explainable and understandable to human users. In the military domain, numerous bodies have argued that autonomous and AI-enabled weapon systems ought not incorporate unexplainable AI, with the International Committee of the Red Cross and the United States Department of Defense both explicitly including explainability as a relevant factor in the development and use of such systems. In this article, I present a cautiously critical assessment of this view, arguing that explainability will be irrelevant for many current and near-future autonomous systems in the military (which do not incorporate any AI), that it will be trivially incorporated into most military systems which do possess AI (as these generally possess simpler AI systems), and that for those systems with genuinely opaque AI, explainability will prove to be of more limited value than one might imagine. In particular, I argue that explainability, while indeed a virtue in design, is a virtue aimed primarily at designers and troubleshooters of AI-enabled systems, but is far less relevant for users and handlers actually deploying these systems. I further argue that human–machine teaming is a far more important element of responsibly using AI for military purposes, adding that explainability may undermine efforts to improve human–machine teamings by creating a","https://link.springer.com/content/pdf/10.1007/s10676-024-09762-w.pdf",""
2,"Alessandra Cenci, Susanne Jakobsen Ilskov, Nicklas Sindlev Andersen, Marco Chiarandini","The participatory value-sensitive design (VSD) of a mHealth app targeting citizens with dementia in a Danish municipality",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00274-9","",987,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00274-9","2730-5953","",4,2,375,401,2,1.00,1,4,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00274-9.pdf",""
2,"Marc Zeller, Thomas Waschulzik, Reiner Schmid, Claus Bahlmann","Toward a safe MLOps process for the continuous development and safety assurance of ML-based systems in the railway domain",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00392-4","",990,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00392-4","2730-5953","",4,1,123,130,2,2.00,1,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00392-4.pdf",""
2,"Dafna Burema, Nicole Debowski-Weimann, Alexander von Janowski, Jil Grabowski, Mihai Maftei, Mattis Jacobs, Patrick van der Smagt, Djalel Benbouzid","A sector-based approach to AI ethics: Understanding ethical issues of AI-related incidents within their sectoral context",2023,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3600211.3604680","",999,"2025-02-04 16:55:17","proceedings-article","10.1145/3600211.3604680","","",,,705,714,2,1.00,0,8,2,"","https://dl.acm.org/doi/pdf/10.1145/3600211.3604680",""
1,"Laeticia N. Onyejegbu","Challenges of Integrating AI Ethics into Higher Education Curricula in West Africa: Nigerian Universities Narrative",2023,"SpringerBriefs in Ethics","Springer International Publishing","https://doi.org/10.1007/978-3-031-23035-6_5","",19,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-23035-6_5","2211-8101","",,,57,66,1,0.50,1,1,2,"Abstract: Artificial Intelligence (AI) is becoming pervasive. It is also an exciting field because it is making our lives much better, by doing most of the work for us. For example, driving our cars, medical jobs, accounting jobs, all sorts of jobs.","https://link.springer.com/content/pdf/10.1007/978-3-031-23035-6_5",""
1,"Sara Kassir, Lewis Baker, Jackson Dolphin, Frida Polli","Publisher Correction: AI for hiring in context: a perspective on overcoming the unique challenges of employment research to mitigate disparate impact",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00225-w","",30,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00225-w","2730-5953","",3,1,345,345,1,0.33,0,4,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00225-w.pdf",""
1,"Ivana Bartoletti","AI in education",2022,"The Ethics of Artificial Intelligence in Education","Routledge","https://doi.org/10.4324/9780429329067-5","",35,"2025-02-04 16:55:17","book-chapter","10.4324/9780429329067-5","","",,,74,90,1,0.33,1,1,3,"","",""
1,"Christian Goglin","The Ethics of Artificial Intelligence: Review of Ethical Machines: Your Concise Guide to Totally Unbiased, Transparent, and Respectful AI by R. Blackman; Ethics of Artificial Intelligence: Case Studies and Options for Addressing Ethical Challenges by B.C. Stahl, D. Schroeder, and R. Rodrigues; and AI Ethics by M. Coeckelbergh",2023,"Journal of Business Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s10551-023-05538-2","",40,"2025-02-04 16:55:17","journal-article","10.1007/s10551-023-05538-2","0167-4544","",188,3,623,627,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s10551-023-05538-2.pdf",""
1,"Evangelos Pournaras","Science in the era of ChatGPT, large language models and generative AI",2023,"Beyond Quantity","transcript Verlag","https://doi.org/10.1515/9783839467664-015","",72,"2025-02-04 16:55:17","book-chapter","10.1515/9783839467664-015","","",,,275,290,1,0.50,1,1,2,"","https://www.degruyter.com/document/doi/10.1515/9783839467664-015/xml",""
1,"Luciano Floridi","Future",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0003","",74,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0003","","",,,31,54,1,0.50,1,1,2,"Abstract: Previously, in Chapter 2, I argued that AI should be interpreted not as a marriage between some biological-like intelligence and engineered artefacts, but as a divorce between agency and intelligence—that is, a decoupling between the ability to deal with problems and tasks successfully in view of a goal, and the need to be intelligent to do so. This chapter uses the above interpretation of AI as a new form of successful non-intelligent agency to look at its future. After a short introduction in Section 3.1 about the difficulties facing any exercise in prediction, Sections 3.2 and 3.4 argue that the likely developments and possible challenges of AI will depend on the push for synthetic data, the tension between constraining and constitutive rules underpinning areas of AI application, leading to the progressive adaptation of the environment to AI rather than of AI to the environment (what I defined in the previous chapter as enveloping), and the increasing translation of difficult problems into complex problems. Section 3.5 is dedicated to Generative Models the debate about ChatGPT, and why chatbots should be seen as one more implementation of agency without intelligence. Section 3.6 returns to the importance of design and the responsibility to produce the right sort of AI to take advantage of the developments mentioned above. The conclusion in Section 3.7 discusses the seasons of AI, especially its winters, to highlight the lessons we should have learnt and those we can still learn—and hence apply—to make the most of this unique technology. This chapter concludes Part One of the book, which provides a brief philosophical introduction to AI’s past, present, and future.","https://academic.oup.com/book/chapter-pdf/58147821/oso-9780198883098-chapter-3.pdf",""
1,"Lionel Brossi, Ana María Castillo, Sandra Cortesi","Student-centred requirements for the ethics of AI in education",2022,"The Ethics of Artificial Intelligence in Education","Routledge","https://doi.org/10.4324/9780429329067-6","",78,"2025-02-04 16:55:17","book-chapter","10.4324/9780429329067-6","","",,,91,112,1,0.33,0,3,3,"","",""
1,"Leonard Dung","Evaluating approaches for reducing catastrophic risks from AI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00475-w","",79,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00475-w","2730-5953","",,,,,1,1.00,1,1,1,"Abstract: According to a growing number of researchers, AI may pose catastrophic – or even existential – risks to humanity. Catastrophic risks may be taken to be risks of 100 million human deaths, or a similarly bad outcome. I argue that such risks – while contested – are sufficiently likely to demand rigorous discussion of potential societal responses. Subsequently, I propose four desiderata for approaches to the reduction of catastrophic risks from AI. The quality of such approaches can be assessed by their chance of success, degree of beneficence, degree of non-maleficence, and beneficent side effects. Then, I employ these desiderata to evaluate the promises, limitations and risks of alignment research, timelines research, policy research, halting or slowing down AI research, and compute governance for tackling catastrophic AI risks. While more research is needed, this investigation shows that several approaches for dealing with catastrophic AI risks are available, and where their respective strengths and weaknesses lie. It turns out that many approaches are complementary and that the approaches have a nuanced relationship to approaches to present AI harms. While some approaches are similarly useful for addressing catastrophic risks and present harms, this is not always the case.","https://link.springer.com/content/pdf/10.1007/s43681-024-00475-w.pdf",""
1,"Nicholas Barrow","Anthropomorphism and AI hype",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00454-1","",92,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00454-1","2730-5953","",4,3,707,711,1,1.00,1,1,1,"Abstract: As humans, we have an innate tendency to ascribe human-like qualities to non-human entities. Whilst sometimes helpful, such anthropomorphic projections are often misleading. This commentary considers how anthropomorphising AI contributes to its misrepresentation and hype. First, I outline three manifestations (terminology; imagery; and morality). Then, I consider the extent to which we ought to mitigate it.","https://link.springer.com/content/pdf/10.1007/s43681-024-00454-1.pdf",""
1,"Neil O’Hara","Primary recognition, morality and AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00340-2","",98,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00340-2","2730-5953","",4,4,1467,1472,1,0.50,1,1,2,"Abstract: This paper aims to show that the experience of ‘primary recognition’ (O’Hara in Moral certainty and the foundations of morality, Palgrave Macmillan, London, 2018) can be extended to human AI interactions. That is, I argue that human beings can (and do) experience non-rational, reflex moral responses to AI and social robots that fit O’Hara’s description of primary recognition. I give two plausible examples, one involving a military mine-sweeping robot and the other, a toy dinosaur called a ‘Pleo’. These experiences of primary recognition do not, however, settle the question of whether any particular AI can be considered a true moral patient or a ‘person’.","https://link.springer.com/content/pdf/10.1007/s43681-023-00340-2.pdf",""
1,"Simon Knight, Antonette Shibani, Nicole Vincent","Ethical AI governance: mapping a research ecosystem",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00416-z","",113,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00416-z","2730-5953","",,,,,1,1.00,0,3,1,"Abstract: How do we assess the positive and negative impacts of research about- or research that employs artificial intelligence (AI), and how adequate are existing research governance frameworks for these ends? That concern has seen significant recent attention, with various calls for change, and a plethora of emerging guideline documents across sectors. However, it is not clear what kinds of issues are expressed in research ethics with or on AI at present, nor how resources are drawn on in this process to support the navigation of ethical issues. Research Ethics Committees (RECs) have a well-established history in ethics governance, but there have been concerns about their capacity to adequately govern AI research. However, no study to date has examined the ways that AI-related projects engage with the ethics ecosystem, or its adequacy for this context. This paper analysed a single institution’s ethics applications for research related to AI, applying a socio-material lens to their analysis. Our novel methodology provides an approach to understanding ethics ecosystems across institutions. Our results suggest that existing REC models can effectively support consideration of ethical issues in AI research, we thus propose that any new materials should be embedded in this existing well-established ecosystem.","https://link.springer.com/content/pdf/10.1007/s43681-023-00416-z.pdf",""
1,"Evangelos Pournaras","Science in the era of ChatGPT, large language models and generative AI",2023,"KI-Kritik / AI Critique","transcript Verlag","https://doi.org/10.14361/9783839467664-015","",119,"2025-02-04 16:55:17","book-chapter","10.14361/9783839467664-015","2698-7546","",,,275,290,1,0.50,1,1,2,"","",""
1,"Hossein Dabbagh, Brian D. Earp, Sebastian Porsdam Mann, Monika Plozza, Sabine Salloch, Julian Savulescu","AI ethics should be mandatory for schoolchildren",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00462-1","",121,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00462-1","2730-5953","",,,,,1,1.00,0,6,1,"Abstract: As society increasingly integrates artificial intelligence (AI) into its fabric, AI ethics education in primary schools becomes necessary. Drawing parallels between the integration of foundational subjects such as languages and mathematics and the pressing need for AI literacy, we argue for mandatory, age-appropriate AI education focusing on technical proficiency and ethical implications. Analogous to how sex and drug education prepare youth for real-world challenges and decisions, AI education is crucial for equipping students to navigate an AI-driven future responsibly. Our study delineates the ethical pillars, such as data privacy and unbiased algorithms, essential for students to grasp, and presents a framework for AI literacy integration in elementary schools. What is needed is a comprehensive, dynamic, and evidence-based approach to AI education, to prepare students for an AI-driven future.","https://link.springer.com/content/pdf/10.1007/s43681-024-00462-1.pdf",""
1,"Fritz J. McDonald","AI, alignment, and the categorical imperative",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00160-w","",136,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00160-w","2730-5953","",3,1,337,344,1,0.33,1,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00160-w.pdf",""
1,"Clea Bourne","AI hype, promotional culture, and affective capitalism",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00483-w","",137,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00483-w","2730-5953","",4,3,757,769,1,1.00,1,1,1,"Abstract: This article centres AI hype within promotional culture. It incorporates scholarship on hype, affect and emotion from media, communications and cultural studies, as well as from market studies, to pose the following questions: ‘What role does promotional culture play in AI hype cycles?’ ‘What are the main promotional forms of emotion evident in the 2020s AI hype cycle?’ And finally, ‘What are the ethical implications of promoting emotion in AI hype cycles?’ The article explores the growth of twenty-first century promotional culture, particularly in the global tech sector, before examining links between promotional culture, emotion, affect, media and capitalism. Drawing on interdisciplinary approaches, the article contends that AI hype has successfully persisted because now, more than ever, contemporary promotional culture strategically deploys emotions as part of affective capitalism, and the affective nature of a digital media infrastructure controlled by the tech sector. The ensuing analysis isolates different emotions circulated by AI hype, including doomsday hype, drawing on examples from the 2020s AI hype cycle. The article concludes by examining the ethics of promotional culture as part of the combined knowledge apparatus supporting value construction in AI.","https://link.springer.com/content/pdf/10.1007/s43681-024-00483-w.pdf",""
1,"Alessio Tartaro","When things go wrong: the recall of AI systems as a last resort for ethical and lawful AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00327-z","",156,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00327-z","2730-5953","",,,,,1,0.50,1,1,2,"Abstract: This paper presents an initial exploration of the concept of AI system recall, primarily understood as a last resort when AI systems violate ethical norms, societal expectations, or legal obligations. The discussion is spurred by recent incidents involving notable AI systems, demonstrating that AI recalls can be a very real necessity. This study delves into the concept of product recall as traditionally understood in industry and explores its potential application to AI systems. Our analysis of this concept is centered around two prominent categories of recall drivers in the AI domain: ethical-social and legal considerations. In terms of ethical-social drivers, we apply the innovative notion of “moral Operational Design Domain”, suggesting AI systems should be recalled when they violate ethical principles and societal expectation. In addition, we also explore the recall of AI systems from a legal perspective, where the recently proposed AI Act provides regulatory measures for recalling AI systems that pose risks to health, safety, and fundamental rights. The paper also underscores the need for further research, especially around defining precise ethical and societal triggers for AI recalls, creating an efficient recall management framework for organizations, and reassessing the fit of traditional product recall models for AI systems within the AI Act's regulatory context. By probing these complex intersections between AI, ethics, and regulation, this work aims to contribute to the development of robust and responsible AI systems while maintaining readiness for failure scenarios.","https://link.springer.com/content/pdf/10.1007/s43681-023-00327-z.pdf",""
1,"Michael Strange","Three different types of AI hype in healthcare",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00465-y","",164,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00465-y","2730-5953","",4,3,833,840,1,1.00,1,1,1,"Abstract: Healthcare systems are the embodiment of big data – as evident in the logistics of resource management, estate maintenance, diagnoses, patient monitoring, research, etc. – such that human health is often heralded as one of the fields most likely to benefit from AI. Yet, the prevalence of hype – both positive and negative – risks undermining that potential by distracting healthcare policy makers, practitioners, and researchers from many of the non-AI factors that will determine its impact. Here we categorise AI hype in healthcare into three types that include both utopian and dystopian narratives and plot a series of more productive paths ahead by which to realise the potential of AI to improve human healthcare.","https://link.springer.com/content/pdf/10.1007/s43681-024-00465-y.pdf",""
1,"Rand Hirmiz","Against the substitutive approach to AI in healthcare",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00347-9","",169,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00347-9","2730-5953","",4,4,1507,1518,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00347-9.pdf",""
1,"Dawn McAra-Hunter","How AI hype impacts the LGBTQ + community",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00423-8","",171,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00423-8","2730-5953","",4,3,771,790,1,1.00,1,1,1,"Abstract: Hype around Artificial Intelligence (AI) has been a feature of this technology since its inception. However, the most recent wave of AI hype has been leveraged to encourage adoption of AI technologies that cause issues for marginalised communities. Hype is also a means to obfuscate real issues of bias, harm, and exploitation felt most sharply by marginalised communities when AI is implemented. This therefore raises the question of power imbalances as a feature of AI technologies as we currently know them. This paper will study the relationship of AI hype and marginalised communities, with particular emphasis on the LGBTQ + community, and look at the way that AI impacts on this community. This paper will pose two key questions: does hype affect marginalised communities, particularly hype around new technologies such as AI; and what impact does the LGBTQ + community experience as a result of hype. This paper will then move on to discuss areas that provide a focus for discourse of AI hype and the impact on the LGBTQ + community: policy and decision-making, the maintenance of the cisgender heteronormative (cishet) baseline, the ubiquity of a mythology of AI, and the role of market expansion.","https://link.springer.com/content/pdf/10.1007/s43681-024-00423-8.pdf",""
1,"Emmanuel R. Goffi","Teaching Ethics Applied to AI from a Cultural Standpoint: What African “AI Ethics” for Africa?",2023,"SpringerBriefs in Ethics","Springer International Publishing","https://doi.org/10.1007/978-3-031-23035-6_2","",172,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-23035-6_2","2211-8101","",,,13,26,1,0.50,1,1,2,"Abstract: Ethics applied to Artificial Intelligence (AI), improperly called AI ethics, is mainly addressed through a Western perspective focusing on continental philosophy. As a result, discussions on ethics applied to AI are shaped by the West.","https://link.springer.com/content/pdf/10.1007/978-3-031-23035-6_2",""
1,"Martin Peterson, Peter Gärdenfors","How to measure value alignment in AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00357-7","",185,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00357-7","2730-5953","",4,4,1493,1506,1,0.50,1,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00357-7.pdf",""
1,"Serap Keles","Navigating in the moral landscape: analysing bias and discrimination in AI through philosophical inquiry",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00377-3","",186,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00377-3","2730-5953","",,,,,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00377-3.pdf",""
1,"A. Geigel","Machine learning AI systems and the virtue of inventiveness",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00197-x","",187,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00197-x","2730-5953","",3,2,637,645,1,0.33,1,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00197-x.pdf",""
1,"Eduardo Vyhmeister, Gabriel G. Castane","TAI-PRM: trustworthy AI—project risk management framework towards Industry 5.0",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00417-y","",190,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00417-y","2730-5953","",,,,,1,1.00,1,2,1,"Abstract: Artificial Intelligence (AI) is increasingly being used in manufacturing to automate tasks and process data, leading to what has been termed Industry. 4.0. However, as we move towards Industry 5.0, there is a need to incorporate societal and human-centric dimensions into the development and deployment of AI software artefacts. This requires blending ethical considerations with existing practices and standards. To address this need, the TAI-PRM framework has been developed. It builds upon established methods, such as Failure Mode and Effect Analysis (FMEA) and the Industrial ISO 31000, to manage risks associated with AI artefacts in the manufacturing sector. The framework identifies ethical considerations as hazards that can impact system processes and sustainability and provides tools and metrics to manage these risks. To validate the framework, it was applied in an EU project for Digital Twins on AI for manufacturing. The results showed that TAI-PRM can effectively identify and track different failure modes associated with AI artefacts and help users to manage ethical risks associated with their deployment. By incorporating ethical considerations into risk management processes, the framework enables the developing and deploying trustworthy AI in the manufacturing sector.","https://link.springer.com/content/pdf/10.1007/s43681-023-00417-y.pdf",""
1,"Nelson Colón Vargas","Exploiting the margin: How capitalism fuels AI at the expense of minoritized groups",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00502-w","",195,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00502-w","2730-5953","",,,,,1,1.00,1,1,1,"Abstract: This paper explores the intricate relationship between capitalism, racial injustice, and artificial intelligence (AI), arguing that AI acts as a contemporary vehicle for age-old forms of exploitation. By linking historical patterns of racial and economic oppression with current AI practices, this study illustrates how modern technology perpetuates and deepens societal inequalities. It specifically examines how AI is implicated in the exploitation of marginalized communities through underpaid labor in the gig economy, the perpetuation of biases in algorithmic decision-making, and the reinforcement of systemic barriers that prevent these groups from benefiting equitably from technological advances. Furthermore, the paper discusses the role of AI in extending and intensifying the social, economic, and psychological burdens faced by these communities, highlighting the problematic use of AI in surveillance, law enforcement, and mental health contexts. The analysis concludes with a call for transformative changes in how AI is developed and deployed. Advocating for a reevaluation of the values driving AI innovation, the paper promotes an approach that integrates social justice and equity into the core of technological design and policy. This shift is crucial for ensuring that AI serves as a tool for societal improvement, fostering empowerment and healing rather than deepening existing divides.","https://link.springer.com/content/pdf/10.1007/s43681-024-00502-w.pdf",""
1,"Ryan Lemasters","An entryway into technology ethics. Sven Nyholm’s This is Technology Ethics: An Introduction (2023)",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00308-2","",205,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00308-2","2730-5953","",3,3,1033,1035,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00308-2.pdf",""
1,"Luciano Floridi","Present",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0002","",206,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0002","","",,,14,30,1,0.50,1,1,2,"Abstract: Previously, in Chapter 1, we saw how the digital revolution has cut and pasted our realities along with our ideas about our realities, re-ontologizing and re-epistemologizing modernity. This has led to the development of AI as a new form of agency that can be successful without being intelligent. This chapter analyses the above interpretation. Section 2.1 shows how the absence of a definition for AI is evidence that the expression is not a scientific term. Instead, it is a helpful shortcut for referring to a family of sciences, methods, paradigms, technologies, products, and services. Section 2.2 refers to the classic, counterfactual characterization of AI provided by McCarthy, Minsky, Rochester, and Shannon in their ‘Proposal for the Dartmouth Summer Research Project on Artificial Intelligence’, the founding document and later event that established the new field of AI in 1955. This is the characterization adopted for the rest of the book. Turing’s famous question ‘Can machines think?’ is then discussed. Section 2.3 moves from the previous analysis to outline the engineering and cognitive approaches to AI, arguing that the former has been a great success and the latter a complete failure. The interpretation of AI as a new form of agency that does not need any intelligence to be successful is based on the engineering tradition. Section 2.4 suggests that a similar form of agency can be successful because we have been transforming the world (enveloping it) into an increasingly AI-friendly environment. The conclusion in Section 2.5 stresses that such a process generates the risk of nudging humanity to adapt to its smart technologies.","https://academic.oup.com/book/chapter-pdf/58147798/oso-9780198883098-chapter-2.pdf",""
1,"Harry Law","Computer vision: AI imaginaries and the Massachusetts Institute of Technology",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00389-z","",214,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00389-z","2730-5953","",4,3,657,663,1,0.50,1,1,2,"Abstract: This paper explores the way in which computer scientists at the Massachusetts Institute of Technology (MIT) constructed visions of the future in 1960s America to direct the AI and computing research agendas. It argues that MIT computer scientists resisted attempts by the state to control the future of computing by fabricating imaginaries to covertly exert influence over the research environment. The paper examines the impact of the Cold War military–industrial complex on academia, which provided opportunities for research to take place whilst introducing challenges to autonomy. It makes the case that computer scientists such as Marvin Minsky and Fernando J. Corbato carefully shaped narratives across film, television and the media to promote desirable futures centering their own technical approaches. Acknowledging that instruments of the state appealed to the future to guide research towards strategically sensitive areas in the context of Cold War technoscientific contest, it asserts that intensifying ties between military and academic institutions afforded researchers both the latitude and motivation to construct independent visions of the future. In doing so, the paper aims to complicate assumptions about imaginaries as solely tools of governance by highlighting scientists' creativity in navigating institutional constraints to wrest back control of the future.","https://link.springer.com/content/pdf/10.1007/s43681-023-00389-z.pdf",""
1,"Ognjen Arandjelović","Apropos of “Speciesist bias in AI: how AI applications perpetuate discrimination and unfair outcomes against animals”",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00255-4","",230,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00255-4","2730-5953","",3,3,1021,1023,1,0.50,1,1,2,"Abstract: The present comment concerns a recent","https://link.springer.com/content/pdf/10.1007/s43681-022-00255-4.pdf",""
1,"Alessio Tartaro, Enrico Panai, Mariangela Zoe Cocchiaro","AI risk assessment using ethical dimensions",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00401-6","",236,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00401-6","2730-5953","",4,1,105,112,1,1.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00401-6.pdf",""
1,"Garry Young","What would strong AI understand consent to mean, and what are the implications for sexbot rape?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00383-5","",246,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00383-5","2730-5953","",,,,,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00383-5.pdf",""
1,"Tim Räz","Understanding risk with FOTRES?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00223-y","",250,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00223-y","2730-5953","",3,4,1153,1167,1,0.33,1,1,3,"Abstract: The present paper examines the recidivism risk assessment instrument FOTRES, addressing the questions whether FOTRES provides us with an adequate understanding of risk, whether we actually understand FOTRES itself, and whether FOTRES is fair. The evaluation of FOTRES uses the criteria of empirical accuracy, representational accuracy, domain of validity, intelligibility, and fairness. This evaluation is compared to that of COMPAS, a different, much-discussed risk assessment instrument. The paper argues that FOTRES performs poorly in comparison to COMPAS with respect to some of the criteria, and that both FOTRES and COMPAS do not show a satisfactory performance with respect to other criteria.","https://link.springer.com/content/pdf/10.1007/s43681-022-00223-y.pdf",""
1,"Johannes Thumfart","The democratic offset: Contestation, deliberation, and participation regarding military applications of AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00288-3","",256,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00288-3","2730-5953","",4,2,511,526,1,0.50,1,1,2,"Abstract: Authoritarian regimes’ unrestricted collection of citizens’ data might constitute an advantage regarding the development of some types of AI, and AI might facilitate authoritarian practices. This feedback loop challenges democracies. In a critical continuation of the Pentagon’s Third Offset Strategy, I investigate a possible Democratic Offset regarding military applications of AI focussed on contestation, deliberation, and participation. I apply Landemore’s Open Democracy, Hildebrandt’s Agonistic Machine Learning, and Sharp’s Civilian-Based Defence. Discussing value pluralism in AI ethics, I criticise parts of the literature for leaving the fundamental ethical incompatibility of democracies and authoritarian regimes unaddressed. I am focussing on the duty to disobey illegal orders derived from customary international humanitarian law (IHL) and the standard of ‘meaningful human control’, which is central to the partially outdated debate about lethal autonomous weapon systems (LAWS). I criticize the standard of ‘meaningful human control’ following two pathways: First, the ethical and legal principles of just war theory and IHL should be implemented in military applications of AI to submit human commands to more control, in the sense of technological disaffordances. Second, the debate should focus on the societal circumstances for personal responsibility and disobedience to be trained and exerted in deliberation and participation related to military applications of AI, in the sense of societal affordances. In a larger picture, this includes multi-level stakeholder involvement, robust documentation to facilitate auditing, civilian-based defence in decentralized smart cities, and open-source intelligence. This multi-layered approach fosters cognitive diversity, which might constitute a strategic advantage for democracies regarding AI.","https://link.springer.com/content/pdf/10.1007/s43681-023-00288-3.pdf",""
1,"Frank Dietrich","AI-based removal of hate speech from digital social networks: chances and risks for freedom of expression",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00610-7","",258,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00610-7","2730-5953","",,,,,1,1.00,1,1,1,"Abstract: Given the enormous number of posts, major digital social networks, such as Facebook, must rely on artificial intelligence (AI) systems to regulate hate speech. This article explores the risks for free speech that the automated deletion of posts entails and discusses how AI systems can be subjected to human control. In a first step, the article examines the relevance of the individual right to freedom of expression for privately operated Internet platforms. It then highlights the specific risks that arise when AI systems are entrusted with the task of identifying and removing hate speech. The recently passed EU AI Act represents the most ambitious attempt to date to regulate high-risk AI applications. The article examines whether and, if so, to what extent the various forms of human oversight mentioned in the EU AI Act are feasible in the area of hate speech regulation. Three core theses are put forward: First, the deletion of hate speech by AI systems constitutes a high-risk application that requires an extension of the regulatory scope of the EU AI Act. Second, ex-post monitoring is the only feasible kind of human supervision but fails to guarantee full protection of the individual right to freedom of expression. Third, despite this shortcoming, the implementing of ex-post monitoring is necessary and legitimate to curb hate speech on digital social networks.","https://link.springer.com/content/pdf/10.1007/s43681-024-00610-7.pdf",""
1,"Gina Helfrich","The harms of terminology: why we should reject so-called “frontier AI”",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00438-1","",265,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00438-1","2730-5953","",4,3,699,705,1,1.00,1,1,1,"Abstract: In the mid-2023, promoters of artificial intelligence (AI) as an “existential risk” coined a new term, “frontier AI,” that refers to “highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety.” Promoters of this new term were able to disseminate it via the United Kingdom (UK) government’s Frontier AI Taskforce (formerly the Foundation Models Taskforce) as well as the UK’s AI Safety Summit, held in November 2023.","https://link.springer.com/content/pdf/10.1007/s43681-024-00438-1.pdf",""
1,"Ricardo F. Crespo","Moderating the effects of “surveillance capitalism”: an Aristotelian perspective",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00334-0","",268,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00334-0","2730-5953","",,,,,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00334-0.pdf",""
1,"Björn Lundgren","In defense of ethical guidelines",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00244-7","",271,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00244-7","2730-5953","",3,3,1013,1020,1,0.50,1,1,2,"Abstract: Recently, Luke Munn attacked “AI ethics” generally, or guidelines, principles, codes of ethics, ethical frameworks. In particular, he argued that ethical guidelines are useless. Here I respond to this critique, arguing that Munn’s criticism is mostly unfair and misguided, and that his own proposal is already implemented in various guidelines.","https://link.springer.com/content/pdf/10.1007/s43681-022-00244-7.pdf",""
1,"Peter Smith, Laura Smith","This season’s artificial intelligence (AI): is today’s AI really that different from the AI of the past? Some reflections and thoughts",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00388-0","",274,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00388-0","2730-5953","",4,3,665,668,1,0.50,1,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00388-0.pdf",""
1,"Amelia Katirai, Yusuke Nagato","Addressing trade-offs in co-designing principles for ethical AI: perspectives from an industry-academia collaboration",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00477-8","",304,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00477-8","2730-5953","",,,,,1,1.00,1,2,1,"Abstract: The development and deployment of artificial intelligence (AI) has rapidly outpaced regulation. As a result, many organizations opt to develop their own principles for the ethical development of AI, though little research has examined the processes through which they are developed. Prior research indicates that these processes involve perceived trade-offs between competing considerations, and primarily between ethical concerns and organizational benefits or technological development. In this paper, we report on a novel, collaborative initiative in Japan between researchers in the humanities and social sciences, and industry actors to co-design organizational AI ethics principles. We analyzed the minutes from 20 meetings from the formative phase of the development of these principles using an inductive process drawing on thematic analysis, to identify the issues of importance to participants. Through this, we identified four core trade-offs faced by participants. We find that, contrary to prior literature, participants were not just concerned with trade-offs between ethical concerns and organizational benefits or technological development, but also between competing, ethically-oriented considerations. We use the results of this study to highlight a need for further research to understand the longer-term impact on organizations and on society of organization-led approaches to AI ethics.","https://link.springer.com/content/pdf/10.1007/s43681-024-00477-8.pdf",""
1,"Oliver Li","Should we develop AGI? Artificial suffering and the moral development of humans",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00411-4","",340,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00411-4","2730-5953","",,,,,1,1.00,1,1,1,"Abstract: Recent research papers and tests in real life point in the direction that machines in the future may develop some form of possibly rudimentary inner life. Philosophers have warned and emphasized that the possibility of artificial suffering or the possibility of machines as moral patients should not be ruled out. In this paper, I reflect on the consequences for moral development of striving for AGI. In the introduction, I present examples which point into the direction of the future possibility of artificial suffering and highlight the increasing similarity between, for example, machine–human and human–human interaction. Next, I present and discuss responses to the possibility of artificial suffering supporting a cautious attitude for the sake of the machines. From a virtue ethical perspective and the development of human virtues, I subsequently argue that humans should not pursue the path of developing and creating AGI, not merely for the sake of possible suffering in machines, but also due to machine–human interaction becoming more alike to human–human interaction and for the sake of the human’s own moral development. Thus, for several reasons, humanity, as a whole, should be extremely cautious about pursuing the path of developing AGI—Artificial General Intelligence.","https://link.springer.com/content/pdf/10.1007/s43681-023-00411-4.pdf",""
1,"Fred S. Roberts","Socially responsible facial recognition of animals",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00344-y","",341,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00344-y","2730-5953","",4,4,1423,1439,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00344-y.pdf",""
1,"Joana Heil, Dirk Ifenthaler","Ethics in AI-based online assessment in higher education",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00008-1","",342,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00008-1","","",,,55,70,1,1.00,1,2,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000081",""
1,"Tales Marra, Emeric Kubiak","Addressing diversity in hiring procedures: a generative adversarial network approach",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00445-2","",351,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00445-2","2730-5953","",,,,,1,1.00,1,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00445-2.pdf",""
1,"Geoff Keeling","Algorithmic bias, generalist models, and clinical medicine",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00329-x","",353,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00329-x","2730-5953","",4,4,1533,1544,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00329-x.pdf",""
1,"Jeremy Lopez, Claire Textor, Caitlin Lancaster, Beau Schelble, Guo Freeman, Rui Zhang, Nathan McNeese, Richard Pak","The complex relationship of AI ethics and trust in human–AI teaming: insights from advanced real-world subject matter experts",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00303-7","",365,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00303-7","2730-5953","",4,4,1213,1233,1,0.50,0,8,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00303-7.pdf",""
1,"Dafna Burema, Mattis Jacobs, Filip Rozborski","Elusive technologies, elusive responsibilities: on the perceived responsibility of basic AI researchers",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00358-6","",389,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00358-6","2730-5953","",4,4,1453,1466,1,0.50,0,3,2,"Abstract: This paper studies how researchers who work in the field of basic research of artificial intelligence (AI) perceive their responsibility. A case study is conducted on an inter-university and interdisciplinary research cluster in Germany that specializes in basic artificial intelligence research. The reason for studying responsibility through the lens of such researchers is that working in basic research of AI involves a lot of uncertainty about potential consequences, more so than in other domains of AI development. After conducting focus groups with 21 respondents followed by a thematic analysis, results show that respondents restrict the boundaries of their sociotechnical visions, regard time as an influencing factor in their responsibility, and refer to many other players in the field. These themes indicate that respondents had difficulties explaining what they consider themselves responsible for, and referred to many factors beyond their own control. The only type of responsibility that was explicitly acknowledged by respondents is","https://link.springer.com/content/pdf/10.1007/s43681-023-00358-6.pdf",""
1,"Eduardo Vyhmeister, Gabriel Gonzalez-Castane, P.-O. Östbergy","Risk as a driver for AI framework development on manufacturing",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00159-3","",392,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00159-3","2730-5953","",3,1,155,174,1,0.33,0,3,3,"Abstract: Incorporating ethics and values within the life cycle of an AI asset means to secure, under these perspectives, its development, deployment, use and decommission. These processes must be done safely, following current legislation, and incorporating the social needs towards having greater well-being over the agents and environment involved. Standards, frameworks and ethical imperatives—which are also considered a backbone structure for legal considerations—drive the development process of new AI assets for industry. However, given the lack of concrete standards and robust AI legislation, the gap between ethical principles and actionable approaches is still considerable. Different organisations have developed various methods based on multiple ethical principles to facilitate practitioners developing AI components worldwide. Nevertheless, these approaches can be driven by a self-claimed ethical shell or without a clear understanding of the impacts and risks involved in using their AI assets. The manufacturing sector has produced standards since 1990’s to guarantee, among others, the correct use of mechanical machinery, workers security, and environmental impact. However, a revision is needed to blend these with the needs associated with AI’s use. We propose using a vertical-domain framework for the manufacturing sector that will consider ethical perspectives, values, requirements, and well-known approaches related to risk management in the sector.","https://link.springer.com/content/pdf/10.1007/s43681-022-00159-3.pdf",""
1,"Antonio Araújo","From artificial intelligence to semi-creative inorganic intelligence: a blockchain-based bioethical metamorphosis",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00471-0","",393,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00471-0","2730-5953","",,,,,1,1.00,1,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00471-0.pdf",""
1,"Wael Badawy","Data-driven framework for evaluating digitization and artificial intelligence risk: a comprehensive analysis",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00376-4","",396,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00376-4","2730-5953","",,,,,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00376-4.pdf",""
1,"Jesper Ryberg","Artificial intelligence at sentencing: when do algorithms perform well enough to replace humans?",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00442-5","",399,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00442-5","2730-5953","",,,,,1,1.00,1,1,1,"Abstract: Artificial intelligence is currently supplanting the work of humans in many societal contexts. The purpose of this article is to consider the question of when algorithmic tools should be regarded as performing sufficiently well to replace human judgements and decision-making at sentencing. More precisely, the question as to which are the ethically plausible criteria for the comparative performance assessments of algorithms and humans is considered with regard to both risk assessment algorithms that are designed to provide predictions of recidivism and sentencing algorithms designed to determine sentences in individual criminal cases. It is argued, first, that the prima facie most obvious assessment criteria do not stand up to ethical scrutiny. Second, that ethically plausible criteria presuppose ethical theory on penal distribution which currently has not been sufficiently developed. And third, that the current lack of assessment criteria has comprehensive implications regarding when algorithmic tools should be implemented in criminal justice practice.","https://link.springer.com/content/pdf/10.1007/s43681-024-00442-5.pdf",""
1,"Kimon Kieslich, Nicholas Diakopoulos, Natali Helberger","Anticipating impacts: using large-scale scenario-writing to explore diverse implications of generative AI in the news environment",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00497-4","",405,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00497-4","2730-5953","",,,,,1,1.00,0,3,1,"Abstract: The tremendous rise of generative AI has reached every part of society—including the news environment. There are many concerns about the individual and societal impact of the increasing use of generative AI, including issues such as disinformation and misinformation, discrimination, and the promotion of social tensions. However, research on anticipating the impact of generative AI is still in its infancy and mostly limited to the views of technology developers and/or researchers. In this paper, we aim to broaden the perspective and capture the expectations of three stakeholder groups (news consumers; technology developers; content creators) about the potential negative impacts of generative AI, as well as mitigation strategies to address these. Methodologically, we apply scenario-writing and use participatory foresight in the context of a survey (n = 119) to delve into cognitively diverse imaginations of the future. We qualitatively analyze the scenarios using thematic analysis to systematically map potential impacts of generative AI on the news environment, potential mitigation strategies, and the role of stakeholders in causing and mitigating these impacts. In addition, we measure respondents' opinions on a specific mitigation strategy, namely transparency obligations as suggested in Article 52 of the draft EU AI Act. We compare the results across different stakeholder groups and elaborate on different expected impacts across these groups. We conclude by discussing the usefulness of scenario-writing and participatory foresight as a toolbox for generative AI impact assessment.","https://link.springer.com/content/pdf/10.1007/s43681-024-00497-4.pdf",""
1,"Salla Westerstrand, Rauli Westerstrand, Jani Koskinen","Talking existential risk into being: a Habermasian critical discourse perspective to AI hype",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00464-z","",408,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00464-z","2730-5953","",4,3,713,726,1,1.00,0,3,1,"Abstract: Recent developments in Artificial Intelligence (AI) have resulted in a hype around both opportunities and risks of these technologies. In this discussion, one argument in particular has gained increasing visibility and influence in various forums and positions of power, ranging from public to private sector organisations. It suggests that Artificial General Intelligence (AGI) that surpasses human intelligence is possible, if not inevitable, and which can—if not controlled—lead to human extinction (Existential Threat Argument, ETA). Using Jürgen Habermas’s theory of communicative action and the validity claims of truth, truthfulness and rightness therein, we inspect the validity of this argument and its following ethical and societal implications. Our analysis shows that the ETA is problematic in terms of scientific validity, truthfulness, as well as normative validity. This risks directing AI development towards a strategic game driven by economic interests of the few rather than ethical AI that is good for all.","https://link.springer.com/content/pdf/10.1007/s43681-024-00464-z.pdf",""
1,"Paula Boddington","Introduction: Why AI Ethics?",2023,"Artificial Intelligence: Foundations, Theory, and Algorithms","Springer Nature Singapore","https://doi.org/10.1007/978-981-19-9382-4_1","",431,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-19-9382-4_1","2365-3051","",,,1,34,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/978-981-19-9382-4_1",""
1,"Sabine Wiesmüller, Mathias Bauer","Governance of Collaborative AI Development Strategies",2023,"CSR, Sustainability, Ethics &amp; Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-09245-9_4","",437,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09245-9_4","2196-7075","",,,91,109,1,0.50,1,2,2,"","https://link.springer.com/content/pdf/10.1007/978-3-031-09245-9_4",""
1,"Gwyneth Sutherlin","Who is the human in the machine? Releasing the human–machine metaphor from its cultural roots can increase innovation and equity in AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00382-6","",444,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00382-6","2730-5953","",,,,,1,0.50,1,1,2,"Abstract: Computer science and cognitive science have a shared past, with many intertwined goals and perspectives. The conceptual metaphor, shaping the discoveries of these fields for decades, has been","https://link.springer.com/content/pdf/10.1007/s43681-023-00382-6.pdf",""
1,"Jake Burley, Nir Eisikovits","Workplace automation and political replacement: a valid analogy?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00245-6","",448,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00245-6","2730-5953","",3,4,1361,1370,1,0.33,1,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00245-6.pdf",""
1,"Esther Keymolen","Trustworthy tech companies: talking the talk or walking the walk?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00254-5","",453,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00254-5","2730-5953","",4,2,169,177,1,0.50,1,1,2,"Abstract: While people are increasingly dependent on tech companies to live a flourishing life, numerous incidents reveal that these companies struggle with genuinely taking the interests of customers to heart. Regulators and companies alike acknowledge that this should change and that companies must take responsibility for their impact. If society is to benefit from these innovations, it is paramount that tech companies are trustworthy. However, it is unclear what is required of tech companies to be recognized as trustworthy. This vagueness is risky, as it may lead to ethics washing and an ill-founded sense of security. This raises the question: what should tech companies do to deserve our trust? What would make them","https://link.springer.com/content/pdf/10.1007/s43681-022-00254-5.pdf",""
1,"Wegene Demisie Jima, Tesfaye Adisu Tarekegn, Taye Girma Debelee","State of artificial intelligence eco-system in Ethiopia",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00436-3","",462,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00436-3","2730-5953","",,,,,1,1.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00436-3.pdf",""
1,"Donatella Casaburo, Irina Marsh","Ensuring fundamental rights compliance and trustworthiness of law enforcement AI systems: the ALIGNER Fundamental Rights Impact Assessment",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00560-0","",468,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00560-0","2730-5953","",4,4,1569,1582,1,1.00,1,2,1,"Abstract: Artificial intelligence systems can expand the capabilities and enhance the efficiency of law enforcement agencies preventing, investigating, detecting, and prosecuting criminal offences in the European Union. At the same time, the deployment of artificial intelligence in the security domain often raises numerous legal and ethical concerns. The ALIGNER Fundamental Rights Impact Assessment is an operational tool, rooted in fundamental rights and in the principles of AI ethics, ready to be integrated in the AI governance measures of European law enforcement agencies to inform their decision-making processes and ensure compliance with the recently adopted Artificial Intelligence Act. This paper first introduces the main tensions between law enforcement AI and fundamental rights, as enshrined in the Charter of Fundamental Rights of the European Union; then, it gives an overview of the main developments and best practices in AI governance and their relationship with fundamental rights as well as AI ethics; and finally, it describes the structure of the ALIGNER Fundamental Rights Impact Assessment.","https://link.springer.com/content/pdf/10.1007/s43681-024-00560-0.pdf",""
1,"Rodrigo L. Canalli","Artificial intelligence and the model of rules: better than us?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00210-3","",478,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00210-3","2730-5953","",3,3,879,885,1,0.33,1,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00210-3.pdf",""
1,"Paula Boddington","Normative Ethical Theory and AI Ethics",2023,"Artificial Intelligence: Foundations, Theory, and Algorithms","Springer Nature Singapore","https://doi.org/10.1007/978-981-19-9382-4_6","",480,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-19-9382-4_6","2365-3051","",,,229,276,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/978-981-19-9382-4_6",""
1,"Mohammed Ghaly","What Makes Work “Good” in the Age of Artificial Intelligence (AI)? Islamic Perspectives on AI-Mediated Work Ethics",2023,"The Journal of Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s10892-023-09456-3","",483,"2025-02-04 16:55:17","journal-article","10.1007/s10892-023-09456-3","1382-4554","",28,3,429,453,1,0.50,1,1,2,"Abstract: Artificial intelligence (AI) technologies are increasingly creeping into the work sphere, thereby gradually questioning and/or disturbing the long-established moral concepts and norms communities have been using to define what makes work good. Each community, and Muslims make no exception in this regard, has to revisit their moral world to provide well-thought frameworks that can engage with the challenging ethical questions raised by the new phenomenon of AI-mediated work. For a systematic analysis of the broad topic of AI-mediated work ethics from an Islamic perspective, this article focuses on presenting an accessible overview of the “moral world” of work in the Islamic tradition. Three main components of this moral world were selected due to their relevance to the AI context, namely (1) Work is inherently good for humans, (2) Practising a religiously permitted profession and (c) Maintaining good relations with involved stakeholders. Each of these three components is addressed in a distinct section, followed by a sub-section highlighting the relevance of the respective component to the particular context of AI-mediated work. The article argues that there are no unsurmountable barriers in the Islamic tradition against the adoption of AI technologies in work sphere. However, important precautions should be considered to ensure that embracing AI will not be at the cost of work-related moral values. The article also highlights how important lessons can be learnt from the positive historical experience of automata that thrived in the Islamic civilization.","https://link.springer.com/content/pdf/10.1007/s10892-023-09456-3.pdf",""
1,"Bertrand Braunschweig, Stefan Buijsman, Faïcel Chamroukhi, Fredrik Heintz, Foutse Khomh, Juliette Mattioli, Maximilian Poretschkin","AITA: AI trustworthiness assessment",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00397-z","",488,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00397-z","2730-5953","",4,1,1,3,1,1.00,0,7,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00397-z.pdf",""
1,"Shanmuga Pria, Iman Al Rubaie, Venkatavara Prasad","Enhancing Business Intelligence Through AI-Driven Integration of Sustainability Metrics via ESG Factors",2024,"Advances in Finance, Accounting, and Economics","IGI Global","https://doi.org/10.4018/979-8-3693-2185-0.ch004","",506,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-2185-0.ch004","2327-5677","",,,57,89,1,1.00,0,3,1,"In recent years, the imperative for businesses to integrate Environmental, Social, and Governance (ESG) factors into their decision-making processes has become increasingly evident, reflecting a broader societal shift towards sustainable practices. This transition is driven by a recognition of the interconnectedness between business operations and environmental and social impacts, to create long-term value for all stakeholders. The framework underpinning AI-driven integration elucidates how machine learning algorithms and natural language p To address these challenges, the framework offers recommendations for policymakers and regulatory bodies to promote the adoption of AI-driven integration for ESG factors. By fostering an enabling environment that incentivizes sustainability-oriented decision-making, policymakers can accelerate the transition towards a more sustainable and resilient economy. By embracing AI technologies, organizations can navigate the complexity of ESG factors","https://www.igi-global.com/viewtitle.aspx?TitleId=352612",""
1,"Erwan Le Merrer, Ronan Pons, Gilles Tredan","Algorithmic audits of algorithms, and the law",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00343-z","",514,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00343-z","2730-5953","",4,4,1365,1375,1,0.50,0,3,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00343-z.pdf",""
1,"Nripsuta Ani Saxena, Wenbin Zhang, Cyrus Shahabi","Unveiling and mitigating bias in ride-hailing pricing for equitable policy making",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00498-3","",518,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00498-3","2730-5953","",,,,,1,1.00,0,3,1,"Abstract: Ride-hailing services have skyrocketed in popularity due to their convenience. However, recent research has shown that their pricing strategies can have a disparate impact on some riders, such as those living in disadvantaged neighborhoods with a greater share of residents of color or residents below the poverty line. Analyzing real-world data, we additionally show that these communities tend to be more dependent on ride-hailing services (e.g., for work commutes) due to a lack of adequate public transportation infrastructure. To this end, we present the first thorough study on fair pricing for ride-hailing services by first devising applicable fairness measures to quantify this bias and then proposing novel fair pricing mechanisms to alleviate this bias. We present two pricing mechanisms to provide flexibility and account for different platform needs. By taking affordability into account and potentially providing discounts that may be government-subsidized, our approaches result in an increased number and more affordable rides for the disadvantaged community. Experiments on real-world Chicago ride-hailing data demonstrate worse scores for the proposed fairness metrics for rides corresponding to disadvantaged neighborhoods than those of a control group (random mix of neighborhoods). Subsequently, the results show that our fair pricing mechanisms eliminate this inequality gap. Our mechanisms provide a basis for the government and the ride-hailing platforms to implement fair ride-hailing policies.","https://link.springer.com/content/pdf/10.1007/s43681-024-00498-3.pdf",""
1,"Mario D. Schultz, Ludovico Giacomo Conti, Peter Seele","Digital ethicswashing: a systematic review and a process-perception-outcome framework",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00430-9","",519,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00430-9","2730-5953","",,,,,1,1.00,0,3,1,"Abstract: The term “ethicswashing” was recently coined to describe the phenomenon of instrumentalising ethics by misleading communication, creating the impression of ethical Artificial Intelligence (AI), while no substantive ethical theory, argument, or application is in place or ethicists involved. Ethicswashing resembles greenwashing for environmental issues and has become an issue – particularly since 2019 with Thomas Metzinger’s harsh criticisms as a member of the EU panel for developing ethical guidelines for AI, which he called “ethicswashing.” Nowadays, increased ethics washing has changed the perception of AI ethics, leading critics to find a “trivialization” of ethics that may even lead to “ethics bashing.” Considering the scattered literature body and the various manifestations of digital ethicswashing, we recognise the need to assess the existing literature comprehensively. To fill this gap, this research systematically reviews current knowledge about digital ethicswashing stemming from various academic disciplines, contributing to an up-to-date assessment of its underlying characteristics. Applying content analysis to map the field leads us to present five thematic clusters: ethicswashing, ethics bashing, policymaking and regulation, watchdogs, and academia. In conclusion, we synthesise ethicswashing along a process-perception-outcome framework to provide future research to explore the multiple meanings of digital ethicswashing.","https://link.springer.com/content/pdf/10.1007/s43681-024-00430-9.pdf",""
1,"Johan Rochel","Learning from the Ethics of AI &amp;ndash; A Research Proposal on Soft Law and Ethics of AI",2023,"Tilburg Law Review","Ubiquity Press, Ltd.","https://doi.org/10.5334/tilr.297","",527,"2025-02-04 16:55:17","journal-article","10.5334/tilr.297","2211-2545","",27,1,,,1,0.50,1,1,2,"This contribution outlines a research proposal combining ethical guidelines on AI and a law-as-data approach. Building upon the definitions of soft law discussed in legal scholarship, it proposes a way of structuring the regulatory landscape on AI and of addressing the question of what is included in the “soft law of AI” today. By adopting a building-blocks approach (combining distinct definitional components of soft law), the paper shows that the state of current soft law on AI depends on which position on international law one defends. Concretely, the paper firstly offers a complete codebook for identifying the different types of soft law. Secondly, it applies this codebook as a proof-of-concept for the research proposal by analyzing 40+ ethical guidelines and by clustering preliminary results according to the actor enacting the guidelines and the legally relevant effects they could deploy. Four paradigmatic types of soft law emerge: statist and international organization soft law, process-oriented soft law, expertise-oriented soft law, and de facto relevant standards soft law. These results illustrate the contributions which are to be expected from a law-as-data research proposal.","https://account.tilburglawreview.com/index.php/up-j-tlr/article/download/297/233",""
1,"Thomas Grote","Fairness as adequacy: a sociotechnical view on model evaluation in machine learning",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00280-x","",528,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00280-x","2730-5953","",4,2,427,440,1,0.50,1,1,2,"Abstract: This paper develops an account of model evaluation—with an emphasis on fairness concerns—that takes the social situatedness of ML models as its starting point. Such a view entails that ML models are not deemed isolated entities, but rather tools, used for specific purposes and potentially impacting their social environment in manifold ways. This shift of perspective opens up a new problem space and facilitates rethinking criteria for model evaluation. By drawing on the adequacy-for-purpose view in philosophy of science, epistemic norms and desiderata for an adequate deployment of ML models along the dimensions of Social Objectives, Measurement, Social Dynamics, and interaction are then identified. The account thus developed also highlights why any auditing of ML models that ought to assist in consequential decision-making cannot be limited to an assessment of statistical properties, but needs to incorporate a variety of methods from the social sciences instead. Moreover, while the process of model evaluation might be deemed as a mere technical exercise, it is in fact riddled by epistemic and morally normative considerations.","https://link.springer.com/content/pdf/10.1007/s43681-023-00280-x.pdf",""
1,"Maria do Rosário Pinto-Alves","Dermatological diagnostic-assistive technologies: a call for regulatory action",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00262-z","",529,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00262-z","2730-5953","",4,2,247,255,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00262-z.pdf",""
1,"Gadosey Pius Kwao, Deborah Dormah Kanubala, Belona Sonna","AI Ethics Education for Future African Leaders",2023,"SpringerBriefs in Ethics","Springer International Publishing","https://doi.org/10.1007/978-3-031-23035-6_7","",544,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-23035-6_7","2211-8101","",,,87,101,1,0.50,0,3,2,"Abstract: From the Greek word “ethos”, which means custom, habit or character, the word ethics can mean and has been defined in many different ways by ethics and morality theorists.","https://link.springer.com/content/pdf/10.1007/978-3-031-23035-6_7",""
1,"Michael B. McCarthy, Sundaraparipurnan Narayanan","Fairness–accuracy tradeoff: activation function choice in a neural network",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00250-9","",565,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00250-9","2730-5953","",3,4,1423,1432,1,0.50,1,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00250-9.pdf",""
1,"Diogo Cortiz","A narrative review of fairness and morality in neuroscience: insights to artificial intelligence",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00203-2","",571,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00203-2","2730-5953","",3,3,769,780,1,0.33,1,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00203-2.pdf",""
1,"B. Sanz-Urquijo, E. Fosch-Villaronga, M. Lopez-Belloso","The disconnect between the goals of trustworthy AI for law enforcement and the EU research agenda",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00235-8","",580,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00235-8","2730-5953","",3,4,1283,1294,1,0.33,0,3,3,"Abstract: In this paper, we investigate whether AI deployment for law enforcement will enable or impede the exercise of citizens' fundamental rights by juxtaposing the promises and policy goals with the crude reality of practices, funded projects, and practicalities of law enforcement. To this end, we map the projects funded by H2020 in AI for law enforcement and juxtapose them to the goals and aims of the EU in terms of Trustworthy AI and fundamental rights. We then bring forward existing research stressing that AI implementation in sensitive domains such as defense and law enforcement does not come without drawbacks, especially regarding discrimination, surveillance, data protection, and human dignity. We thoroughly analyze and assess human-centric and socially-driven lens risks and threats of using AI factors from an ethical, legal, and societal perspective (ELSA), including organizational and gender worries.","https://link.springer.com/content/pdf/10.1007/s43681-022-00235-8.pdf",""
1,"Guilherme Dean Pelegrina, Miguel Couceiro, Leonardo Tomazeli Duarte","A statistical approach to detect disparity prone features in a group fairness setting",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00363-9","",595,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00363-9","2730-5953","",,,,,1,0.50,0,3,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00363-9.pdf",""
1,"Ayse Ocal","Perceptions of AI Ethics on Social Media",2023,"2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology (ETHICS)","IEEE","https://doi.org/10.1109/ethics57328.2023.10155069","",609,"2025-02-04 16:55:17","proceedings-article","10.1109/ethics57328.2023.10155069","","",,,1,1,1,0.50,1,1,2,"","http://xplorestaging.ieee.org/ielx7/10154894/10154907/10155069.pdf?arnumber=10155069",""
1,"Luca Nannini, Eleonora Bonel, Davide Bassi, Michele Joshua Maggini","Beyond phase-in: assessing impacts on disinformation of the EU Digital Services Act",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00467-w","",614,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00467-w","2730-5953","",,,,,1,1.00,0,4,1,"Abstract: This work proposes a comprehensive research agenda to empirically evaluate the real-world impacts of the European Union’s Digital Services Act (DSA) on combating online disinformation. It provides background on the DSA’s context, mechanisms, timeline, and expected effects on platforms to situate the need for rigorous impact assessment. A detailed legal, technical, psychological, behavioral and ethical critique reveals meaningful gaps in the DSA requiring ongoing regulatory refinement and oversight. Most critically, the paper puts forth an encompassing framework spanning computational analytics, interviews, ethnography, surveys, discourse analysis and mixed methods to rigorously assess the DSA’s multi-dimensional effects on complex factors enabling disinformation proliferation. Priorities include evaluating notice-and-takedown efficacy, advertising transparency improvements, risk assessment outcomes, oversight integration, and procedural shifts in platform governance. Coordinated efforts between researchers, regulators and platforms are needed to address methodological challenges around isolating DSA impacts amidst an evolving EU regulatory landscape, constrained data access from platforms, and difficulties generalizing findings across the sociotechnical diversity of platforms and national contexts in EU Member States.","https://link.springer.com/content/pdf/10.1007/s43681-024-00467-w.pdf",""
1,"Simon Geerkens, Christian Sieberichs, Alexander Braun, Thomas Waschulzik","QI$$^2$$: an interactive tool for data quality assurance",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00390-6","",615,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00390-6","2730-5953","",4,1,141,149,1,1.00,0,4,1,"Abstract: The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also, the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper, we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well-known MNIST data set based an handwritten digits.","https://link.springer.com/content/pdf/10.1007/s43681-023-00390-6.pdf",""
1,"Mariarosaria Taddeo, Alexander Blanchard, Chris Thomas","From AI Ethics Principles to Practices: A Teleological Methodology to Apply AI Ethics Principles in The Defence Domain",2023,"SSRN Electronic Journal","Elsevier BV","https://doi.org/10.2139/ssrn.4520945","",623,"2025-02-04 16:55:17","journal-article","10.2139/ssrn.4520945","1556-5068","",,,,,1,0.50,0,3,2,"","",""
1,"Simon Geerkens, Christian Sieberichs, Alexander Braun, Thomas Waschulzik","Correction: QI2: an interactive tool for data quality assurance",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00422-9","",625,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00422-9","2730-5953","",4,4,1587,1587,1,1.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00422-9.pdf",""
1,"Léo Andéol, Thomas Fel, Florence de Grancey, Luca Mossina","Conformal prediction for trustworthy detection of railway signals",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00400-7","",632,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00400-7","2730-5953","",4,1,157,161,1,1.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00400-7.pdf",""
1,"Biplav Srivastava, Kausik Lakkaraju, Mariana Bernagozzi, Marco Valtorta","Advances in automatically rating the trustworthiness of text processing services",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00391-5","",652,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00391-5","2730-5953","",4,1,5,13,1,0.50,0,4,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00391-5.pdf",""
1,"Animesh Mukherjee","AI and Ethics",2023,"","IOP Publishing","https://doi.org/10.1088/978-0-7503-6116-3","",655,"2025-02-04 16:55:17","monograph","10.1088/978-0-7503-6116-3","","",,,,,1,0.50,1,1,2,"","",""
1,"Atle Ottesen Søvik","How a non-conscious robot could be an agent with capacity for morally responsible behaviour",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00140-0","",657,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00140-0","2730-5953","",2,4,789,800,1,0.33,1,1,3,"Abstract: People have different opinions about which conditions robots would need to fulfil—and for what reasons—to be moral agents. Standardists hold that specific internal states (like rationality, free will or phenomenal consciousness) are necessary in artificial agents, and robots are thus not moral agents since they lack these internal states. Functionalists hold that what matters are certain behaviours and reactions—independent of what the internal states may be—implying that robots can be moral agents as long as the behaviour is adequate. This article defends a standardist view in the sense that the internal states are what matters for determining the moral agency of the robot, but it will be unique in being an internalist theory defending a large degree of robot responsibility, even though humans, but not robots, are taken to have phenomenal consciousness. This view is based on an event-causal libertarian theory of free will and a revisionist theory of responsibility, which combined explain how free will and responsibility can come in degrees. This is meant to be a middle position between typical compatibilist and libertarian views, securing the strengths of both sides. The theories are then applied to robots, making it possible to be quite precise about what it means that robots can have a certain degree of moral responsibility, and why. Defending this libertarian form of free will and responsibility then implies that non-conscious robots can have a stronger form of free will and responsibility than what is commonly defended in the literature on robot responsibility.","https://link.springer.com/content/pdf/10.1007/s43681-022-00140-0.pdf",""
1,"Sabine Wiesmüller, Nele Fischer, Wenzel Mehnert, Sabine Ammon","Responsible AI Adoption Through Private-Sector Governance",2023,"CSR, Sustainability, Ethics &amp; Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-09245-9_5","",658,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09245-9_5","2196-7075","",,,111,132,1,0.50,0,4,2,"","https://link.springer.com/content/pdf/10.1007/978-3-031-09245-9_5",""
1,"Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Patrick Schramowski, Sasha Luccioni, Kristian Kersting","Auditing and instructing text-to-image generation models on fairness",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00531-5","",666,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00531-5","2730-5953","",,,,,1,1.00,0,7,1,"Abstract: Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called","https://link.springer.com/content/pdf/10.1007/s43681-024-00531-5.pdf",""
1,"Dan Heaton, Elena Nichele, Jeremie Clos, Joel E. Fischer","“ChatGPT says no”: agency, trust, and blame in Twitter discourses after the launch of ChatGPT",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00414-1","",678,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00414-1","2730-5953","",,,,,1,1.00,0,4,1,"Abstract: ChatGPT, a chatbot using the GPT-n series large language model, has surged in popularity by providing conversation, assistance, and entertainment. This has raised questions about its agency and resulting implications on trust and blame, particularly when concerning its portrayal on social media platforms like Twitter. Understanding trust and blame is crucial for gauging public perception, reliance on, and adoption of AI-driven tools like ChatGPT. To explore ChatGPT’s perceived status as an algorithmic social actor and uncover implications for trust and blame through agency and transitivity, we examined 88,058 tweets about ChatGPT, published in a ‘hype period’ between November 2022 and March 2023, using Corpus Linguistics and Critical Discourse Analysis, underpinned by Social Actor Representation. Notably, ChatGPT was presented in tweets as a social actor on 87% of occasions, using personalisation and agency metaphor to emphasise its role in content creation, information dissemination, and influence. However, a dynamic presentation, oscillating between a creative social actor and an information source, reflected users’ uncertainty regarding its capabilities and, thus, blame attribution occurred. On 13% of occasions, ChatGPT was presented passively through backgrounding and exclusion. Here, the emphasis on ChatGPT’s role in informing and influencing underscores interactors’ reliance on it for information, bearing implications for information dissemination and trust in AI-generated content. Therefore, this study contributes to understanding the perceived social agency of decision-making algorithms and their implications on trust and blame, valuable to AI developers and policymakers and relevant in comprehending and dealing with power dynamics in today’s age of AI.","https://link.springer.com/content/pdf/10.1007/s43681-023-00414-1.pdf",""
1,"Rebekka Görge, Elena Haedecke, Michael Mock","Using ScrutinAI for visual inspection of DNN performance in a medical use case",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00399-x","",679,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00399-x","2730-5953","",4,1,151,156,1,0.50,0,3,2,"Abstract: Our Visual Analytics (VA) tool ScrutinAI supports human analysts to investigate interactively model performance and data sets. Model performance depends on labeling quality to a large extent. In particular in medical settings, generation of high quality labels requires in depth expert knowledge and is very costly. Often, data sets are labeled by collecting opinions of groups of experts. We use our VA tool to analyze the influence of label variations between different experts on the model performance. ScrutinAI facilitates to perform a root cause analysis that distinguishes weaknesses of deep neural network (DNN) models caused by varying or missing labeling quality from true weaknesses. We scrutinize the overall detection of intracranial hemorrhages and the more subtle differentiation between subtypes in a publicly available data set.","https://link.springer.com/content/pdf/10.1007/s43681-023-00399-x.pdf",""
1,"Giovanni Rubeis","Ethical Foundations: Medical Ethics and Data Ethics",2024,"The International Library of Ethics, Law and Technology","Springer International Publishing","https://doi.org/10.1007/978-3-031-55744-6_4","",699,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-55744-6_4","1875-0044","",,,55,87,1,1.00,1,1,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-55744-6_4",""
1,"Isabell Claus, Matthias Szupories","AI and Leadership: Automation and the Change of Management Tasks and Processes",2023,"CSR, Sustainability, Ethics &amp; Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-09245-9_14","",701,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09245-9_14","2196-7075","",,,267,277,1,0.50,1,2,2,"","https://link.springer.com/content/pdf/10.1007/978-3-031-09245-9_14",""
1,"Zarrin Tasnim Sworna, Danilo Urzedo, Andrew J Hoskins, Catherine J Robinson","The ethical implications of Chatbot developments for conservation expertise",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00460-3","",723,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00460-3","2730-5953","",4,4,917,926,1,1.00,0,4,1,"Abstract: Chatbots have emerged as a potent artificial intelligence (AI) tool for expediting expert knowledge, including evidence used for conservation research and practices. While digital technologies can support the curation and analysis of vast amounts of conservation datasets to inform best practices, AI-driven solutions raise ethical concerns around what source of evidence is used or not. This paper examines the ethical issues around sources, biases, and representation of conservation evidence formulated by chatbots. We interviewed two versions of ChatGPT, GPT-3.5-turbo and GPT-4, regarding knowledge available for ecological restoration and analysed 40,000 answers. Our results show that these chatbot developments are expanding the inclusion of diverse data sources and improving the accuracy of the responses. However, these technical developments do not necessarily imply ethical considerations in terms of fair representation and unbiased inclusion of diverse knowledge offered by different sources of expertise. While the updated model expands the descriptions ofgeographical locations and organizations, there remain limitations regarding equitable representation of different expertise and stakeholders. The updated version of GPT still relies heavily on evidence from high-income countries (88%), North American expertise (67%), and male academics (46%) with limited contributions from minority groups, such as Indigenous organizations (10%) and low-income countries (2%). In conclusion, the ethical implications within generative AI reveal the crucial requirement of human-centered negotiations to consider how knowledge practices are legitimized and embedded in the development and use of chatbots.","https://link.springer.com/content/pdf/10.1007/s43681-024-00460-3.pdf",""
1,"Brandon Schweitze","Artificial Intelligence (AI) Ethics in Accounting",2024,"Journal of Accounting, Ethics &amp; Public Policy","Wyższa Szkoła Ekonomii i Informatyki w Krakowie","https://doi.org/10.60154/jaepp.2024.v25n1p67","",735,"2025-02-04 16:55:17","journal-article","10.60154/jaepp.2024.v25n1p67","2956-8390","",25,1,,,1,1.00,1,1,1,"The rapid advancement of artificial intelligence (AI) has revolutionized the accounting profession, automating tasks, identifying patterns, and improving accuracy. However, the increasing reliance on AI raises ethical concerns regarding privacy, bias, transparency, and accountability. This research paper delves into the ethical considerations of AI implementation in accounting practices.Thepaper begins by examining the potential benefits of AI in accounting, highlighting its ability to streamline operations, enhance efficiency, and reduce errors. However, it also acknowledges the ethical risks associated with AI, including data privacy breaches, biased decision-making, lack of transparency, and accountability issues.The paper proposes a framework for responsible AI implementation in accounting to address these ethical concerns. The framework emphasizes establishing clear ethical guidelines,ensuring data privacy and security, mitigating AI algorithms' bias, promoting AI decisionmaking transparency, and establishing accountability mechanisms.The paper further explores the role of accountants in addressing AI ethics. Accountants are responsible for upholding ethical standards and ensuring that AI systems are used responsibly and ethically. They must be aware of the ethical implications of AI and have the knowledge and skills to mitigate ethical risks.In conclusion, the paper emphasizes the need for a proactive approach to AI ethics in accounting. By establishing clear ethical guidelines, promoting responsible AI implementation, and empowering accountants with ethical knowledge and skills, the accounting profession can harness the potential of AI while upholding ethical principles and safeguarding public trust.","",""
1,"Amelia Katirai, Noa Garcia, Kazuki Ide, Yuta Nakashima, Atsuo Kishimoto","Situating the social issues of image generation models in the model life cycle: a sociotechnical approach",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00517-3","",749,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00517-3","2730-5953","",,,,,1,1.00,0,5,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00517-3.pdf",""
1,"Saleh Afroogh, Ali Mostafavi, Ali Akbari, Yasser Pouresmaeil, Sajedeh Goudarzi, Faegheh Hajhosseini, Kambiz Rasoulkhani","Embedded Ethics for Responsible Artificial Intelligence Systems (EE-RAIS) in disaster management: a conceptual model and its deployment",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00309-1","",755,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00309-1","2730-5953","",4,4,1117,1141,1,0.50,0,7,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00309-1.pdf",""
1,"Katarzyna Kapusta, Lucas Mattioli, Boussad Addad, Mohammed Lansari","Protecting ownership rights of ML models using watermarking in the light of adversarial attacks",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00412-3","",770,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00412-3","2730-5953","",4,1,95,103,1,1.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00412-3.pdf",""
1,"Aníbal M. Astobiza","Do people believe that machines have minds and free will? Empirical evidence on mind perception and autonomy in machines",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00317-1","",774,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00317-1","2730-5953","",4,4,1175,1183,1,0.50,1,1,2,"Abstract: Recently, we are witnessing an unprecedented advance and development in Artificial Intelligence (AI). AI systems are capable of reasoning, perceiving, and processing spoken (and written) natural language, and their applications vary from recommendation systems, automated translation software, prioritization of news in social media, to self-driving cars and/or robotics. A dystopian narrative predicts that AI may reach a point of singularity or a phase where machines surpass human beings in general intelligence and enslave us, but until that day comes, it is interesting to know how the general public perceive current artificial systems. Do people really attribute mind (i.e., mental states) and/or free will to artificial systems? Knowing how the general public perceive artificial systems is crucial because it could help understand how to apply AI in medicine, law, politics and other areas of human life. One study that I present here with a convenience sample (","https://link.springer.com/content/pdf/10.1007/s43681-023-00317-1.pdf",""
1,"Satinder P. Gill","Editorial: Beyond regulatory ethics",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01657-6","",776,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01657-6","0951-5666","",38,2,437,438,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s00146-023-01657-6.pdf",""
1,"Pamela Robinson","Action Guidance and AI Alignment",2023,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3600211.3604714","",782,"2025-02-04 16:55:17","proceedings-article","10.1145/3600211.3604714","","",,,387,395,1,0.50,1,1,2,"","https://dl.acm.org/doi/pdf/10.1145/3600211.3604714",""
1,"Yu-Chen Cheng, Po-An Chen, Feng-Chi Chen, Ya-Wen Cheng","Adversarial learning with optimism for bias reduction in machine learning",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00356-8","",788,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00356-8","2730-5953","",4,4,1389,1402,1,0.50,0,4,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00356-8.pdf",""
1,"Paula Sweeney","The ethics of ex-bots",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01754-6","",791,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01754-6","0951-5666","",39,6,3055,3056,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s00146-023-01754-6.pdf",""
1,"Zhiyi Liu, Yejie Zheng","AI Legislation in Computational Society",2022,"AI Ethics and Governance","Springer Nature Singapore","https://doi.org/10.1007/978-981-19-2531-3_9","",796,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-19-2531-3_9","","",,,127,141,1,0.33,1,2,3,"","https://link.springer.com/content/pdf/10.1007/978-981-19-2531-3_9",""
1,"Stefano Nolfi","Progress and challenges in adaptive robotics",2022,"Frontiers in Robotics and AI","Frontiers Media SA","https://doi.org/10.3389/frobt.2022.1020462","",820,"2025-02-04 16:55:17","journal-article","10.3389/frobt.2022.1020462","2296-9144","",9,,,,1,0.33,1,1,3,"","https://www.frontiersin.org/articles/10.3389/frobt.2022.1020462/full",""
1,"Tabu S. Kondo, Salim A. Diwani, Ally S. Nyamawe, Mohamed M. Mjahidi","Exploring the status of artificial intelligence for healthcare research in Africa: a bibliometric and thematic analysis",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00359-5","",826,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00359-5","2730-5953","",,,,,1,0.50,0,4,2,"Abstract: This paper explores the status of Artificial Intelligence (AI) for healthcare research in Africa. The aim was to use bibliometric and thematic analysis methods to determine the publication counts, leading authors, top journals and publishers, most active institutions and countries, most cited institutions, funding bodies, top subject areas, co-occurrence of keywords and co-authorship. Bibliographic data were collected on April 9 2022, through the Lens database, based on the critical areas of authorship studies, such as authorship pattern, number of authors, etc. The findings showed that several channels were used to disseminate the publications, including articles, conference papers, reviews, and others. Publications on computer science topped the list of documented subject categories. The Annals of Tropical Medicine and Public Health is the top journal, where articles on AI have been published. One of the top nations that published AI research was the United Kingdom. With 143 publications, Harvard University was the higher education institution that produced the most in terms of affiliation. It was discovered that the Medical Research Council was one of the funding organizations that supported research, resulting in the publication of articles in AI. By summarizing the current research themes and trends, this work serves as a valuable resource for researchers, practitioners, and funding organizations interested in Artificial intelligence for healthcare research in Africa.","https://link.springer.com/content/pdf/10.1007/s43681-023-00359-5.pdf",""
1,"Eduard Fosch-Villaronga, Gianclaudio Malgieri","Queering the Ethics of AI",2024,"Handbook on the Ethics of Artificial Intelligence","Edward Elgar Publishing","https://doi.org/10.4337/9781803926728.00026","",829,"2025-02-04 16:55:17","book-chapter","10.4337/9781803926728.00026","","",,,301,315,1,1.00,1,2,1,"","https://www.elgaronline.com/view/book/9781803926728/9781803926728.xml",""
1,"Leilasadat Mirghaderi, Monika Sziron, Elisabeth Hildt","Ethics and Transparency Issues in Digital Platforms: An Overview",2023,"AI","MDPI AG","https://doi.org/10.3390/ai4040042","",832,"2025-02-04 16:55:17","journal-article","10.3390/ai4040042","2673-2688","",4,4,831,844,1,0.50,0,3,2,"There is an ever-increasing application of digital platforms that utilize artificial intelligence (AI) in our daily lives. In this context, the matters of transparency and accountability remain major concerns that are yet to be effectively addressed. The aim of this paper is to identify the zones of non-transparency in the context of digital platforms and provide recommendations for improving transparency issues on digital platforms. First, by surveying the literature and reflecting on the concept of platformization, choosing an AI definition that can be adopted by different stakeholders, and utilizing AI ethics, we will identify zones of non-transparency in the context of digital platforms. Second, after identifying the zones of non-transparency, we go beyond a mere summary of existing literature and provide our perspective on how to address the raised concerns. Based on our survey of the literature, we find that three major zones of non-transparency exist in digital platforms. These include a lack of transparency with regard to who contributes to platforms; lack of transparency with regard to who is working behind platforms, the contributions of those workers, and the working conditions of digital workers; and lack of transparency with regard to how algorithms are developed and governed. Considering the abundance of high-level principles in the literature that cannot be easily operationalized, this is an attempt to bridge the gap between principles and operationalization.","https://www.mdpi.com/2673-2688/4/4/42/pdf",""
1,"Themis Tzimas","A Public Sphere for AI",2023,"Journal of Politics and Ethics in New Technologies and AI","National Documentation Centre (EKT)","https://doi.org/10.12681/jpentai.33299","",837,"2025-02-04 16:55:17","journal-article","10.12681/jpentai.33299","2944-9243","",2,1,,,1,0.50,1,1,2,"The present article addresses key elements of the unique ontology of AI and argues that these require the expansion of the public sphere, in order to successfully manage the entry of new intelligent actors in legally regulated relationships which are based on the identification of causal connections. In this sense it attempts to link law and political science, given that the governance of any phenomenon or field includes law and in particular the detection, of legally interesting, causal relationships. Regulating such relationships effectively offers legal certainty, which in turn is a fundamental element of effective governance. In our self- evidently, human- centered world, whether we are talking about natural persons, or for legal persons, it is self- evident that there is, in the end, a human hand behind the causal relations with which law is involved. Once other, non- human, intelligent actors gradually enter the forefront, these causal relations become further complicated. It is on these complications and their impact that we focus.","https://ejournals.epublishing.ekt.gr/index.php/jpentai/article/download/33299/25416",""
1,"Ammar Younas, Yi Zeng","Proposing Central Asian AI Ethics Principles: A Multilevel Approach for Responsible AI",2024,"SSRN Electronic Journal","Elsevier BV","https://doi.org/10.2139/ssrn.4689770","",838,"2025-02-04 16:55:17","journal-article","10.2139/ssrn.4689770","1556-5068","",,,,,1,1.00,1,2,1,"","",""
1,"Éric Pardoux","Ethical Design for AI in Medicine",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3539564","",840,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3539564","","",,,907,907,1,0.33,1,1,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3539564",""
1,"Petar Radanliev, Omar Santos","Ethics and Responsible AI Deployment",2023,"","Cambridge University Press (CUP)","https://doi.org/10.33774/apsa-2023-f1fkq","",857,"2025-02-04 16:55:17","posted-content","10.33774/apsa-2023-f1fkq","","",,,,,1,0.50,1,2,2,"On the recent Bletchley summit, some British officials had hoped other countries would agree to establish a UK based AI task force, to test new models from around the world before they are released to the public. But instead, Raimondo used the summit to announce a separate American AI Safety Institute within the country's National Institute of Standards and Technology. This article explores the need for international and collaborative effort, led by the new American AI Safety Institute, on regualting AI systems to be more ethical , and to safeguard individual privacy while complying with existing privacy standards. The study concludes that these algorithms effectively enhance privacy protection while balancing the utility of AI with the need to protect personal data. This requires a comprehensive approach that combines technological innovation with ethical and regulatory strategies to harness the power of AI in a way that respects and protects individual privacy.","https://preprints.apsanet.org/engage/api-gateway/apsa/assets/orp/resource/item/6550e1a2dbd7c8b54b1fe5f9/original/ethics-and-responsible-ai-deployment.pdf",""
1,"Nishita Agrawal, Isha Pendharkar, Jugal Shroff, Jatin Raghuvanshi, Akashdip Neogi, Shruti Patil, Rahee Walambe, Ketan Kotecha","A-XAI: adversarial machine learning for trustable explainability",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00368-4","",865,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00368-4","2730-5953","",4,4,1143,1174,1,1.00,0,8,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00368-4.pdf",""
1,"Tina Miller","9 Messy Ethics: Negotiating the Terrain between Ethics Approval and Ethical Practice",2022,"Ethics in the Field","Berghahn Books","https://doi.org/10.1515/9780857459633-011","",883,"2025-02-04 16:55:17","book-chapter","10.1515/9780857459633-011","","",,,140,155,1,0.33,1,1,3,"","https://www.degruyter.com/document/doi/10.1515/9780857459633-011/xml",""
1,"Kareem Othman","Understanding how moral decisions are affected by accidents of autonomous vehicles, prior knowledge, and perspective-taking: a continental analysis of a global survey",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00310-8","",893,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00310-8","2730-5953","",4,4,1473,1490,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00310-8.pdf",""
1,"Meshandren Naidoo","AI and Legal Personhood: An African Perspective",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3539548","",899,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3539548","","",,,906,906,1,0.33,1,1,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3539548",""
1,"Paola Ricaurte, Mariel Zasso","AI, Ethics, and Coloniality: A Feminist Critique",2023,"What AI Can Do","Chapman and Hall/CRC","https://doi.org/10.1201/b23345-4","",900,"2025-02-04 16:55:17","book-chapter","10.1201/b23345-4","","",,,39,57,1,0.50,1,2,2,"","",""
1,"Meshandren Naidoo","AI and Legal Personhood: An African Perspective",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3539548","",906,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3539548","","",,,906,906,1,0.33,1,1,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3539548",""
1,"Paola Ricaurte, Mariel Zasso","AI, Ethics, and Coloniality: A Feminist Critique",2023,"What AI Can Do","Chapman and Hall/CRC","https://doi.org/10.1201/b23345-4","",907,"2025-02-04 16:55:17","book-chapter","10.1201/b23345-4","","",,,39,57,1,0.50,1,2,2,"","",""
1,"Mikael Laaksoharju, Thomas Taro Lennerfors, Anders Persson, Lars Oestreicher","What is the problem to which AI chatbots are the solution? AI ethics through Don Ihde's embodiment, hermeneutic, alterity, and background relationships",2023,"Ethics and Sustainability in Digital Cultures","Routledge","https://doi.org/10.4324/9781003367451-4","",910,"2025-02-04 16:55:17","book-chapter","10.4324/9781003367451-4","","",,,31,48,1,0.50,0,4,2,"","",""
1,"Stefan Buijsman","Transparency for AI systems: a value-based approach",2024,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-024-09770-w","",944,"2025-02-04 16:55:17","journal-article","10.1007/s10676-024-09770-w","1388-1957","",26,2,,,1,1.00,1,1,1,"Abstract: With the widespread use of artificial intelligence, it becomes crucial to provide information about these systems and how they are used. Governments aim to disclose their use of algorithms to establish legitimacy and the EU AI Act mandates forms of transparency for all high-risk and limited-risk systems. Yet, what should the standards for transparency be? What information is needed to show to a wide public that a certain system can be used legitimately and responsibly? I argue that process-based approaches fail to satisfy, as knowledge about the development process is insufficient to predict the properties of the resulting system. Current outcome-based approaches [Mitchell et al., 2019; Loi et al., 2021] are also criticized for a lack of attention to the broader socio-technical system and failure to account for empirical results that show that people care about more than just the outcomes of a process [as reported by Meyerson et al. (Procedural justice and relational theory: Empirical, philosophical, and legal perspectives, Taylor & Francis, 2021)]. Instead, I propose value-based transparency, on which the information we need to provide is what values have been considered in the design and how successful these have been realized in the final system. This can handle the objections to other frameworks, matches with current best practices on the design of responsible AI and provides the public with information on the crucial aspects of a system’s design.","https://link.springer.com/content/pdf/10.1007/s10676-024-09770-w.pdf",""
1,"Cristian Moyano-Fernández, Jon Rueda","AI, Sustainability, and Environmental Ethics",2023,"The International Library of Ethics, Law and Technology","Springer Nature Switzerland","https://doi.org/10.1007/978-3-031-48135-2_11","",950,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-48135-2_11","1875-0044","",,,219,236,1,0.50,1,2,2,"","https://link.springer.com/content/pdf/10.1007/978-3-031-48135-2_11",""
1,"","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",2022,"","ACM","https://doi.org/10.1145/3514094","",958,"2025-02-04 16:55:17","proceedings","10.1145/3514094","","",,,,,1,0.33,0,0,3,"","",""
1,"Giovanni Rubeis","Introduction",2024,"The International Library of Ethics, Law and Technology","Springer International Publishing","https://doi.org/10.1007/978-3-031-55744-6_1","",968,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-55744-6_1","1875-0044","",,,3,13,1,1.00,1,1,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-55744-6_1",""
1,"S Manikandan","AI; A New Horizon of Promises &amp; Challenges: ‘Exploring the Impact of Artificial Intelligence (AI) in Mental Health Care’.",2023,"International Journal of Research Publication and Reviews","Genesis Global Publication","https://doi.org/10.55248/gengpi.4.823.51356","",980,"2025-02-04 16:55:17","journal-article","10.55248/gengpi.4.823.51356","2582-7421","",4,8,2038,2045,1,0.50,1,1,2,"","",""
1,"Mykola Makhortykh","No AI After Auschwitz? Bridging AI and Memory Ethics in the Context of Information Retrieval of Genocide-Related Information",2023,"Studies in Computational Intelligence","Springer Nature Singapore","https://doi.org/10.1007/978-981-99-7184-8_4","",992,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-99-7184-8_4","1860-949X","",,,71,83,1,0.50,1,1,2,"","https://link.springer.com/content/pdf/10.1007/978-981-99-7184-8_4",""
1,"Rachel Dlugatch, Antoniya Georgieva, Angeliki Kerasidou","AI-driven decision support systems and epistemic reliance: a qualitative study on obstetricians’ and midwives’ perspectives on integrating AI-driven CTG into clinical decision making",2024,"BMC Medical Ethics","Springer Science and Business Media LLC","https://doi.org/10.1186/s12910-023-00990-1","",997,"2025-02-04 16:55:17","journal-article","10.1186/s12910-023-00990-1","1472-6939","",25,1,,,1,1.00,0,3,1,"Background: Given that AI-driven decision support systems (AI-DSS) are intended to assist in medical decision making, it is essential that clinicians are willing to incorporate AI-DSS into their practice. This study takes as a case study the use of AI-driven cardiotography (CTG), a type of AI-DSS, in the context of intrapartum care. Focusing on the perspectives of obstetricians and midwives regarding the ethical and trust-related issues of incorporating AI-driven tools in their practice, this paper explores the conditions that AI-driven CTG must fulfill for clinicians to feel justified in incorporating this assistive technology into their decision-making processes regarding interventions in labor. Methods: This study is based on semi-structured interviews conducted online with eight obstetricians and five midwives based in England. Participants were asked about their current decision-making processes about when to intervene in labor, how AI-driven CTG might enhance or disrupt this process, and what it would take for them to trust this kind of technology. Interviews were transcribed verbatim and analyzed with thematic analysis. NVivo software was used to organize thematic codes that recurred in interviews to identify the issues that mattered most to participants. Topics and themes that were repeated across interviews were identified to form the basis of the analysis and conclusions of this paper. Results: There were four major themes that emerged from our interviews with obstetricians and midwives regarding the conditions that AI-driven CTG must fulfill: (1) the importance of accurate and efficient risk assessments; (2) the capacity for personalization and individualized medicine; (3) the lack of significance regarding the type of institution that develops technology; and (4) the need for transparency in the development process. Conclusions: Accuracy, efficiency, personalization abilities, transparency, and clear evidence that it can improve outcomes are conditions that clinicians deem necessary for AI-DSS to meet in order to be considered reliable and therefore worthy of being incorporated into the decision-making process. Importantly, healthcare professionals considered themselves as the epistemic authorities in the clinical context and the bearers of responsibility for delivering appropriate care. Therefore, what mattered to them was being able to evaluate the reliability of AI-DSS on their own terms, and have confidence in implementing them in their practice.","https://link.springer.com/content/pdf/10.1186/s12910-023-00990-1.pdf",""
0,"Shashank Yadav","Social botnets and the challenges of cyber situation awareness",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00530-6","",1,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00530-6","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00530-6.pdf",""
0,"Marie Oldfield","Technical challenges and perception: does AI have a PR issue?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00316-2","",3,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00316-2","2730-5953","",4,4,975,995,0,0.00,0,1,2,"Abstract: Increasingly, models have been highlighted that not only disadvantage society but those whom the model was originally designed to benefit. An increasing number of legal challenges around the world illustrates this. A surge of recent work has focussed on the technical, legal or regulatory challenges but not necessarily the real-world day to day challenges for practitioners such as data collection or fairness by design. Since the publication of the Holstein et al.’s study in 2019, additional legislation, regulation and multiple bodies have been created to address practitioner challenge. This study asks what, if anything, has improved for practitioners between 2019 and 2022. Study 1 conducts an investigation into real-world needs within industry and asks whether practitioners are now able to mitigate challenges in a more robust manner. A further pilot study on the perception of AI examines whether perception of AI impacts practitioner work. The results show increasing and continuing interdisciplinary issues. Where increased regulation and legislation might have seemed reasonable, the result for practitioners is indecision and overwhelm. Based on these findings, we highlight directions for future research in this area. The most problematic area being human factors.","https://link.springer.com/content/pdf/10.1007/s43681-023-00316-2.pdf",""
0,"Wael Badawy","The ethical implications of using children’s photographs in artificial intelligence: challenges and recommendations",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00615-2","",5,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00615-2","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00615-2.pdf",""
0,"Sandfreni, Ritika Bansal","Challenges in Large Language Model Development and AI Ethics",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch002","",7,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch002","2327-0411","",,,25,81,0,0.00,0,2,1,"In the digital era, ethical AI development is crucial. This chapter outlines an ethics framework emphasizing fairness, accountability, and transparency. It advocates integrating ethical considerations throughout the AI lifecycle and forming multidisciplinary teams, including ethicists, to ensure alignment with ethical norms. Deployment requires adaptable decision-making frameworks to address evolving ethical challenges. Continuous assessment of AI's societal impact, including stakeholder feedback, is vital. The chapter stresses ongoing vigilance, inclusive discourse, and adaptability to meet AI's evolving challenges and promote societal betterment.","https://www.igi-global.com/viewtitle.aspx?TitleId=354392",""
0,"Md Syful Islam","Navigating modern era at sea: legal challenges and opportunities of unmanned and autonomous shipping",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00554-y","",8,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00554-y","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00554-y.pdf",""
0,"Helena Machado, Susana Silva, Laura Neiva","Publics’ views on ethical challenges of artificial intelligence: a scoping review",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00387-1","",10,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00387-1","2730-5953","",,,,,0,0.00,0,3,2,"Abstract: This scoping review examines the research landscape about publics’ views on the ethical challenges of AI. To elucidate how the concerns voiced by the publics are translated within the research domain, this study scrutinizes 64 publications sourced from PubMed","https://link.springer.com/content/pdf/10.1007/s43681-023-00387-1.pdf",""
0,"Romny Ly, Bora Ly","Ethical challenges and opportunities in ChatGPT integration for education: insights from emerging economy",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-025-00667-y","",11,"2025-02-04 16:55:17","journal-article","10.1007/s43681-025-00667-y","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-025-00667-y.pdf",""
0,"Chloe Gros, Leon Kester, Marieke Martens, Peter Werkhoven","Addressing ethical challenges in automated vehicles: bridging the gap with hybrid AI and augmented utilitarianism",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00592-6","",12,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00592-6","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: In the realm of automated vehicles (AVs), the focus is predominantly on the potential of sub-symbolic deep-learning-based artificial intelligence (AI) systems. Our study questions the suitability of this data-driven approach for AVs, particularly in embodying societal values in their behaviour. Through a systematic examination of sub-symbolic and symbolic AI, we identify key issues for AVs, including adaptability, safety, reliability, trust, fairness, transparency, and control. Deep learning systems’ lack of adaptability and inherent complexities pose significant safety concerns and hinder meaningful human control. This limitation prevents humans from effectively updating AI decision-making processes to better reflect ethical values. Furthermore, deep learning systems are prone to biases and unfairness, leading to incidents that are difficult to explain and rectify. In contrast, symbolic, model-based approaches offer a structured framework for encoding ethical goals and principles within AV systems, thus enabling meaningful human control. However, they also face challenges, such as inefficiencies in handling large amounts of unstructured data for low-level tasks and maintaining explicit knowledge bases. Therefore, we advocate for hybrid AI, combining symbolic and sub-symbolic models with symbolic goal functions. We propose Augmented Utilitarianism (AU) as an ethical framework for developing these goal functions, aiming to minimise harm by integrating principles from consequentialism, deontology, and virtue ethics, while incorporating the perspective of the experiencer. Our methodology for eliciting moral attributes to construct an explicit ethical goal function engages collective societal values through iterative refinement, contributing to the development of safer, more reliable, and ethically aligned automated driving systems.","https://link.springer.com/content/pdf/10.1007/s43681-024-00592-6.pdf",""
0,"Vaibhav Khobragade","Ethics and AI: Confronting the Challenges Ahead",2024,"","Front Matter","https://doi.org/10.59350/t5gj9-1fe45","",13,"2025-02-04 16:55:17","posted-content","10.59350/t5gj9-1fe45","","",,,,,0,0.00,0,1,1,"<p>&lt;strong&gt; Exploring AI’s Ethical Terrain: Addressing Bias, Security, and Beyond &lt;/strong&gt; Author: Vaibhav Khobragade ( &lt;strong&gt; ORCID: &lt;/strong&gt; 0009–0009–8807–5982) Large language models (LLMs) like OpenAI’s GPT-4, Meta’s LLaMA, and Google Gemini (previously called Bard) have showcased their vast capabilities, from passing bar exams and crafting articles to generating images and website code.</p>","https://api.rogue-scholar.org/posts/10.59350/t5gj9-1fe45.pdf",""
0,"Nitika Bhalla, Laurence Brooks, Tonii Leach","Ensuring a ‘Responsible’ AI future in India: RRI as an approach for identifying the ethical challenges from an Indian perspective",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00370-w","",16,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00370-w","2730-5953","",4,4,1409,1422,0,0.00,0,3,2,"Abstract: Artificial intelligence (AI) can be seen to be at an inflexion point in India, a country which is keen to adopt and exploit new technologies, but needs to carefully consider how they do this. AI is usually deployed with good intentions, to unlock value and create opportunities for the people; however it does not come without its challenges. There are a set of ethical–social issues associated with AI, which include concerns around privacy, data protection, job displacement, historical bias and discrimination. Through a series of focus groups with knowledgeable people embedded in India and its culture, this research explores the ethical–societal changes and challenges that India now faces. Further, it investigates whether the principles and practices of responsible research and innovation (RRI) might provide a framework to help identify and deal with these issues. The results show that the areas in which RRI could offer scope to improve this outlook include education, policy and governance, legislation and regulation, and innovation and industry practices. Some significant challenges described by participants included: the lack of awareness of AI by the public as well as policy makers; India’s access and implementation of Western datasets, resulting in a lack of diversity, exacerbation of existing power asymmetries, increase in social inequality and the creation of bias; the potential replacement of jobs by AI. One option was to look at a hybrid approach, a mix of AI and humans, with expansion and upskilling of the current workforce. In terms of strategy, there seems to be a gap between the rhetoric of the government and what is seen on the ground, and therefore going forward there needs to be a much greater engagement with a wider audience of stakeholders.","https://link.springer.com/content/pdf/10.1007/s43681-023-00370-w.pdf",""
0,"Andreas Brenneis","Assessing dual use risks in AI research: necessity, challenges and mitigation strategies",2024,"Research Ethics","SAGE Publications","https://doi.org/10.1177/17470161241267782","",18,"2025-02-04 16:55:17","journal-article","10.1177/17470161241267782","1747-0161","",,,,,0,0.00,0,1,1,"This article argues that due to the difficulty in governing AI, it is essential to develop measures implemented early in the AI research process. The goal of dual use considerations is to create robust strategies that uphold AI’s integrity while protecting societal interests. The challenges of applying dual use frameworks to AI research are examined and dual use and dual use research of concern (DURC) are defined while highlighting the difficulties in balancing the technology’s benefits and risks. AI’s dual use potential is discussed, particularly in areas like NLP and LLMs, and the need for early consideration of dual use risks to ensure ethical and secure development is underscored. In the section on shared responsibilities in AI research and avenues for mitigation strategies the importance of early-stage risk assessments and ethical guidelines to mitigate misuse is emphasized, accentuating self-governance within scientific communities and structured measures like checklists and pre-registration to promote responsible research practices. The final section argues that research ethics committees play a crucial role in evaluating the dual use implications of AI technologies within the research pipeline. The need for tailored ethics review processes is articulated, drawing parallels with medical research ethics committees.","https://journals.sagepub.com/doi/pdf/10.1177/17470161241267782",""
0,"Luciano Floridi","Soft Ethics and the Governance of AI",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0006","",21,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0006","","",,,77,91,0,0.00,0,1,2,"Abstract: Previously, in Chapters 4 and 5, a unified framework for the ethical principles of AI was suggested and some main ethical risks that arise when translating principles into practices were identified. This chapter discusses the governance of AI, and more generally of digital technologies, as the new challenge posed by technological innovation. A new distinction between soft and hard ethics is introduced. Hard ethics first precede and then further contribute to shaping legislation. In contrast, soft ethics apply after legal compliance with legislation (that is, post-compliance ethics), such as the GDPR in the EU. The chapter concludes by developing an analysis of the role of digital ethics with respect to digital regulation and digital governance, thus preparing for the next chapter on the fundamental ethical principles for an ethics of AI.","https://academic.oup.com/book/chapter-pdf/58147894/oso-9780198883098-chapter-6.pdf",""
0,"Michele Murgia","Overcoming AI ethics, towards AI realism",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00552-0","",22,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00552-0","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00552-0.pdf",""
0,"Konstantinos Konstantis","The Main Challenges of AI Ethics: Historical Contextualization, Black-Boxing, Social Biases, Labor Invisibility",2025,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i2.31899","",23,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i2.31899","3065-8365","",7,2,23,25,0,0.00,0,1,1,"In the research described here, I argue that an adequate approach to AI ethics should include the four topics below. My aim is to answer the question of which are the necessary topics that someone should have under consideration in order to make an adequate approach to AI ethics. First, a critical history of AI, which focuses not on the technical differentiations between previous and following technologies, but on the social, economic, and political context in which artificial intelligence is designed, developed, and used. Second, an overview of the issues that most of the time are described as AI ethics, such as fairness, accountability, and transparency, in order to have the ability to understand what is missing from these approaches. A study on the black box of AI is necessary, not only from a technical perspective, but mainly from a perspective that is directly related to the political, social, and economic reasons that enforce and reinforce this black box, revealing, among others, the social relations, the hidden labor, and the “unintelligence” that are hidden under this black box. Third, an analysis of specific cases through critical approaches which take into account capitalism, with all the social, political, and economic relations that are connected with it. In this way, the emergence of biases, inequalities, and discriminations, becomes not a bag, but the substance of AI. Fourth, a study on the hidden labor of AI and the concerns regarding the future of work and AI. The study on hidden labor which is related with AI, is important in order, first, to criticize the intelligence and autonomy of AI systems, and second, to make visible the terrible working conditions of some workers, as a try to change them. The discussion regarding the future of work should not only contain discourses regarding the circular function of capitalism or vague ideas about ethical implementations of AI in the workplace. An adequate discussion should take into account the social, political, and economic relations of our society and ultimately challenge the current form of capitalism. I argue that all the above should be included in an adequate study of AI ethics.","https://ojs.aaai.org/index.php/AIES/article/download/31899/34066",""
0,"Michele Murgia","Correction: Overcoming AI ethics, towards AI realism",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00570-y","",24,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00570-y","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00570-y.pdf",""
0,"Nihal Al Riyami","Advancing AI in Omani Medical Research: Progress, Challenges, and Ethics (Preprint)",2024,"","JMIR Publications Inc.","https://doi.org/10.2196/preprints.66991","",28,"2025-02-04 16:55:17","posted-content","10.2196/preprints.66991","","",,,,,0,0.00,0,1,1,"<sec> <title>BACKGROUND</title> <p>Artificial Intelligence (AI) has the potential to transform medical education and research. However, in Omani medical institutions, AI adoption remains limited due to high costs, faculty training needs, and infrastructure challenges. Traditional research methods are less effective in managing the growing complexity of medical data.</p> </sec> <sec> <title>OBJECTIVE</title> <p>This innovation sought to improve data analysis and decision-making in Omani medical schools by integrating AI tools such as DataRobot and SAS Viya, aiming to enhance research efficiency and educational outcomes.</p> </sec> <sec> <title>METHODS</title> <p>The study adopted desktop research to map AI Integration gaps in the research process at Sultan Qaboos University (SQU) and other healthcare institutions in Oman. A benchmarking exercise comparing Omani medical schools and healthcare institutions with regional universities like the University of Sharjah College of Medicine and Western institutions such as Harvard Medical School was conducted.</p> </sec> <sec> <title>RESULTS</title> <p>A gap exists in Integrating AI Tools for data cleaning and preparation and data analysis in research process for medical schools and healthcare institutions in Oman, including SQU. AI tools reduced data processing times by 30% and improved research accuracy.</p> </sec> <sec> <title>CONCLUSIONS</title> <p>AI integration in Omani medical education is both feasible and effective, offering significant improvements in research efficiency and educational outcomes. Continued investment in AI infrastructure and faculty development is critical for maximizing its potential in medical education.</p> </sec> <sec> <title>CLINICALTRIAL</title> <p>Not applicable</p> </sec>","",""
0,"Ankita Manohar Walawalkar, Massoud Moslehpour, Thanaporn Phattanaviroj, Suman Kumar","Foundations of AI Ethics",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch003","",31,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch003","2327-0411","",,,82,114,0,0.00,0,4,1,"The rapid growth in artificial intelligence (AI) has created many opportunities, this further leads to ethical concerns. Everyone claims to be ethical however, there is a notable gap between stating ethical behavior and maintaining high ethical standards. AI ethics is an arena that has arisen as a response to rising concern. AI ethics is a subclass of digital ethics, reporting concerns about the influence of AI related to their growth and deployment. This chapter undertakes a comprehensive exploration of AI ethics, discussing its basic concepts, and historical prescriptive, along with key ethical theories and their roles, while focusing on the responsibilities of stakeholders in AI ethics. It is important to challenge the AI ethics for AI experts and decision-makers. To accomplish this, the chapter analyses AI ethics' previous, existing, and upcoming statuses.","https://www.igi-global.com/viewtitle.aspx?TitleId=354393",""
0,"Jeannie Marie Paterson","AI Deepfakes on the Web: The 'Wicked' Challenges for AI Ethics, Law and Technology",2024,"Proceedings of the ACM Web Conference 2024","ACM","https://doi.org/10.1145/3589334.3649116","",33,"2025-02-04 16:55:17","proceedings-article","10.1145/3589334.3649116","","",,,3,3,0,0.00,0,1,1,"","https://dl.acm.org/doi/pdf/10.1145/3589334.3649116",""
0,"Iris Howley, Darakhshan Mir, Evan Peck","Integrating AI ethics across the computing curriculum",2022,"The Ethics of Artificial Intelligence in Education","Routledge","https://doi.org/10.4324/9780429329067-13","",36,"2025-02-04 16:55:17","book-chapter","10.4324/9780429329067-13","","",,,255,270,0,0.00,0,3,3,"","",""
0,"Joshua Alexander González-Martín","The hard problem of the androcentric context of AI: challenges for EU policy agendas",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00013-5","",37,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00013-5","","",,,323,345,0,0.00,0,1,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000135",""
0,"Ammar Younas, Yi Zeng","Proposing Central Asian AI ethics principles: a multilevel approach for responsible AI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00505-7","",38,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00505-7","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00505-7.pdf",""
0,"Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, Xiwei Xu","Right to be forgotten in the Era of large language models: implications, challenges, and solutions",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00573-9","",39,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00573-9","2730-5953","",,,,,0,0.00,0,7,1,"Abstract: The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja González, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. It was a significant emergent right as the result of the evolution of technology. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of differential privacy, machine unlearning, model editing, and guardrails. With the rapid advancement of AI and the increasing need of regulating this powerful technology, learning from the case of RTBF can provide valuable lessons for technical practitioners, legal experts, organizations, and authorities.","https://link.springer.com/content/pdf/10.1007/s43681-024-00573-9.pdf",""
0,"Mugalula Kalule Grancia","Decolonizing AI ethics in Africa’s healthcare: An ethical perspective",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00650-z","",41,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00650-z","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Owing to the ethical tension between the ethics frameworks developed in the Global North and the African understanding of ethics, health, and care, decolonization can be a useful tool through which Artificial Intelligence (AI) ethics is understood and applied in Africa. While existing AI ethics frameworks often reflect Western values and perspectives like individuality, the African setting is still largely communalist. Thus, the current frameworks, developed in the Global North, need to be interpreted in the African context to represent the values of people in Africa. It is then that they will be suitable for addressing the healthcare challenges in Africa today. Challenges like resource constraints, coloniality and the paternalism of the Global North toward the Global South have put Africa at the periphery of the AI Ethics debate. This article discusses the need to “decolonize” AI ethics to ensure just, equitable, and inclusive AI in healthcare in Africa. It critiques the current ethical frameworks largely developed from the Global North in shaping ethical AI development in Africa, calling for the decolonization of ethical principles. The paper discusses key considerations for a decolonized approach in healthcare and proposes key principles that can ensure an approach rooted in African contexts and values.","https://link.springer.com/content/pdf/10.1007/s43681-024-00650-z.pdf",""
0,"Dessislava S. Fessenko, Adelaida Jasperse","Ethics at the heart of AI regulation",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00562-y","",42,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00562-y","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00562-y.pdf",""
0,"Larissa Bolte, Aimee van Wynsberghe","Sustainable AI and the third wave of AI ethics: a structural turn",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00522-6","",44,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00522-6","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: With the introduction of the concept of Sustainable AI, considerations of the environmental impact of the technology have begun to enter AI ethics discussions. This, Aimee van Wynsberghe suggests, constitutes a new “third wave of AI ethics” which yet needs to be ushered in. In this paper, we ask what is entailed by Sustainable AI that should warrant such special accentuation. Do we find simply run-of-the-mill AI ethics applied to an environmental context? Or does Sustainable AI constitute a true a “game-changer”? We engage in a discussion about what the “waves of AI ethics” ought to mean and the criteria for labelling a wave as such. We argue that the third wave of AI ethics rests on a turn towards a structural approach for uncovering ethical issues on a broader scale, often paired with an analysis of power structures that prevent the uncovering of these issues.","https://link.springer.com/content/pdf/10.1007/s43681-024-00522-6.pdf",""
0,"Hisham Khogali, Samir Mekid","Perception Challenges and Ethics on the Future of Ai as Encountered by Surveyed New Engineers",2024,"","Elsevier BV","https://doi.org/10.2139/ssrn.4803295","",46,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.4803295","","",,,,,0,0.00,0,2,1,"","",""
0,"Pouria Akbarighatar","Operationalizing responsible AI principles through responsible AI capabilities",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00524-4","",47,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00524-4","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Responsible artificial intelligence (RAI) has emerged in response to growing concerns about the impact of AI. While high-level principles have been provided, operationalizing these principles poses challenges. This study, grounded in recent RAI literature in organizational contexts and dynamic capability theory, and informed by literature on RAI principles and expert interviews in organizations deploying AI systems, (1) problematizes the high-level principles and low-level requirements and underscores the need for mid-level norms by adopting dynamic capability as a theoretical lens, and (2) develops five themes to capture firms’ RAI capability, including (i) understandable AI model, (ii) bias remediation, (iii) responsiveness, (iv) harmless, and vi) common good. As our contribution to the field of information systems (IS), this study extends the emerging literature on operationalizing RAI and dynamic capabilities, empirically elucidating the capabilities needed by firms. For IS practice, we provide organizations deploying AI with novel insights to aid in the responsible implementation of AI.","https://link.springer.com/content/pdf/10.1007/s43681-024-00524-4.pdf",""
0,"Seth D Baum","Manipulating Aggregate Societal values to Bias AI Social Choice Ethics",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00495-6","",49,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00495-6","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00495-6.pdf",""
0,"Otto Sahlgren","Action-guidance and AI ethics: the case of fair machine learning",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00437-2","",50,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00437-2","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: A prominent approach to implementing AI ethics involves translating ethical principles, such as fairness and transparency, into practical frameworks and tools that responsible agents, such as ML developers, can use to ensure that machine learning systems act according to the relevant principles. Fair machine learning research exemplifies this approach by producing frameworks and software toolkits that responsible agents could apply to align machine learning systems with principles such as fairness, equality, and justice. However, the application of available frameworks and tools has proven challenging both due to ambiguous operationalization of the relevant principles and many real-life obstacles that agents face in the context of machine learning system design and development, such as lack of access to proper evaluation data. This article conceptualizes these problems as instances of a more general “action-guidance gap” in AI ethics. The article addresses the action-guidance gap by outlining a philosophical account of action-guidance that can be used to identify and address problems related to the specification and practical implementation of AI ethics principles. Centering on fair machine learning practice as a case example, the article presents a set of detailed requirements for action-guidance in fair machine learning practice which explain problems that previous studies have identified with regard to the real-life application of fair machine learning frameworks and tools. Paving a way forward, the article presents theoretical and practical lessons for ensuring action-guidance in fairness-sensitive design, with implications for AI ethics more generally.","https://link.springer.com/content/pdf/10.1007/s43681-024-00437-2.pdf",""
0,"Mandy Zafar","Normativity and AI moral agency",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00566-8","",53,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00566-8","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: The meanings of the concepts of moral agency in application to AI technologies differ vastly from the ones we use for human agents. Minimal definitions of AI moral agency are often connected with other normative agency-related concepts, such as rationality or intelligence, autonomy, or responsibility. This paper discusses the problematic application of minimal concepts of moral agency to AI. I explore why any","https://link.springer.com/content/pdf/10.1007/s43681-024-00566-8.pdf",""
0,"Alessio Tartaro","Value-laden challenges for technical standards supporting regulation in the field of AI",2024,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-024-09809-y","",54,"2025-02-04 16:55:17","journal-article","10.1007/s10676-024-09809-y","1388-1957","",26,4,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s10676-024-09809-y.pdf",""
0,"Edward Feldman, David De Cremer","Preserving physician ethics in the era of autonomous AI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00602-7","",55,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00602-7","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: For this commentary we explore the how and why of a looming responsibility shift to autonomous AI in medicine and the potential unintended consequence of reducing doctors’ sense of obligation to preserve ethics. In exploring this proposition, we address the difficulties of relying upon intelligent machines to uphold ethics and we offer suggestions on how policymakers and medical educators might prevent ethics degradation caused by the alignment of doctors with autonomous AI.","https://link.springer.com/content/pdf/10.1007/s43681-024-00602-7.pdf",""
0,"Sara Kijewski, Elettra Ronchi, Effy Vayena","The rise of checkbox AI ethics: a review",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00563-x","",56,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00563-x","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: The rapid advancement of artificial intelligence (AI) sparked the development of principles and guidelines for ethical AI by a broad set of actors. Given the high-level nature of these principles, stakeholders seek practical guidance for their implementation in the development, deployment and use of AI, fueling the growth of practical approaches for ethical AI. This paper reviews, synthesizes and assesses current practical approaches for AI in health, examining their scope and potential to aid organizations in adopting ethical standards. We performed a scoping review of existing reviews in accordance with the PRISMA extension for scoping reviews (PRISMA-ScR), systematically searching databases and the web between February and May 2023. A total of 4284 documents were identified, of which 17 were included in the final analysis. Content analysis was performed on the final sample. We identified a highly heterogeneous ecosystem of approaches and a diverse use of terminology, a higher prevalence of approaches for certain stages of the AI lifecycle, reflecting the dominance of specific stakeholder groups in their development, and several barriers to the adoption of approaches. These findings underscore the necessity of a nuanced understanding of the implementation context for these approaches and that no one-size-fits-all approach exists for ethical AI. While common terminology is needed, this should not come at the cost of pluralism in available approaches. As governments signal interest in and develop practical approaches, significant effort remains to guarantee their validity, reliability, and efficacy as tools for governance across the AI lifecycle.","https://link.springer.com/content/pdf/10.1007/s43681-024-00563-x.pdf",""
0,"Rajagopal Sankaranarayanan, Jennifer Jihae Park","AI-Driven Instructional Design: Ethical Challenges and Practical Solutions",2023,"Applied Ethics for Instructional Design and Technology","EdTech Books","https://doi.org/10.59668/270.16880","",59,"2025-02-04 16:55:17","book-chapter","10.59668/270.16880","","",,,,,0,0.00,0,2,2,"Given the unprecedented exponential growth of Artificial Intelligence (AI) technology in our personal and professional lives, its rapid integration into higher education has become an imminent reality rather than a futuristic ideal. AI is getting increasingly recognized as a transformative design tool with the potential to revolutionize the teaching and learning practices of instructional designers, scholars, and educators. Therefore, maintaining a harmonious equilibrium between harnessing the capabilities of AI and upholding ethical principles is crucial for ensuring the responsible integration of AI within educational settings. This chapter offers practical approaches and ethical considerations for the strategic use of AI in designing courses and workshops, thereby contributing to the design and development of responsible and ethically sound educational environments.","",""
0,"Luciano Floridi","Past",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0001","",60,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0001","","",,,3,13,0,0.00,0,1,2,"Abstract: Section 1.1 begins by offering a brief overview of how digital developments have led to the current availability and success of AI systems. Section 1.2 interprets the disruptive impact of digital technologies, sciences, practices, products, and services—in short, the digital—as being due to its ability to cut and paste realities and ideas that we have inherited from modernity. This is the cleaving power of the digital, which is illustrated through some concrete examples. Then it is used to interpret AI as a new form of smart agency brought about by the digital decoupling of agency and intelligence, an unprecedented phenomenon that has caused some distractions and misunderstandings such as ‘the Singularity’, for example. Section 1.3 presents a short excursus into political agency, the other significant kind of agency that is transformed by the cleaving power of the digital. It briefly explains why this topic is essential and highly relevant, yet also lies beyond the scope of this book. Section 1.4 returns to the main issue of a conceptual interpretation of AI and introduces Chapter 2 by reminding the reader of the difficulty of defining and characterizing precisely what AI is. The concluding section argues that design is the counterpart to the cleaving power of the digital and anticipates some of the topics discussed in the second half of the book","https://academic.oup.com/book/chapter-pdf/58147779/oso-9780198883098-chapter-1.pdf",""
0,"Kefu Zhu","Trust and generative AI: embodiment considered",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00611-6","",61,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00611-6","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00611-6.pdf",""
0,"Yuhang Guo, Michael Kühler","History, AI and utilitarianism",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00581-9","",62,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00581-9","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00581-9.pdf",""
0,"","The Ethics of AI Creativity: Emerging Challenges",2024,"Journal of Technology and Humanities","Association for Researcher of Skills and Vocational Training","https://doi.org/10.53797/jthkkss.v5i1.3.2024","",63,"2025-02-04 16:55:17","journal-article","10.53797/jthkkss.v5i1.3.2024","2805-4431","",,5,24,31,0,0.00,0,0,1,"The utilization of Artificial Intelligence (AI) in educational institutions has the potential to bring about a significant transformation in current educational systems. As more educational establishments incorporate AI tools into their teaching and learning practices, there is a growing adoption of Large Language Model (LLM) technologies, including within the field of education. This adoption is driven by the ever-increasing volume of data and evolving educational requirements. However, despite the advantages offered by these technologies, there remains a consistent lack of clarity surrounding the ethical guidelines, technical standards, and best practices that are vital for their effective implementation. This paper primarily focuses on two key areas of research. Firstly, it seeks to investigate the potential benefits, risks, and outcomes associated with the use of LLM technologies in education. Secondly, it delves into the ethical considerations that should guide the utilization of LLM technologies within this domain. The findings underscore the significance of affording students access to LLM technologies in order to enhance the learning environment, with an emphasis on the necessity of transparent and reliable data collection in research. Moreover, given the considerable potential for the dissemination of misinformation and harmful content through LLM technologies, it is imperative to integrate ethical considerations throughout the field of education. This necessitates educating users and reinforcing measures to control the content in order to mitigate associated risks.","",""
0,"Jayakumar Manoharan","Navigating the AI Wave in Martech: A Systematic Literature Review of Developments, Challenges, and Ethics",2024,"SoutheastCon 2024","IEEE","https://doi.org/10.1109/southeastcon52093.2024.10500176","",64,"2025-02-04 16:55:17","proceedings-article","10.1109/southeastcon52093.2024.10500176","","",,,616,622,0,0.00,0,1,1,"","http://xplorestaging.ieee.org/ielx7/10500015/10500024/10500176.pdf?arnumber=10500176",""
0,"Aurélie Halsband","Embryo selection, AI and reproductive choice",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00651-y","",66,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00651-y","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: In reproductive medicine, current research into the use of artificial intelligence (AI) to improve embryo selection has been met with enthusiasm. Within ethics, previous assessments of AI-assisted embryo selection have focused, for example, on liability gaps or risks arising from opaque decision-making. I argue that this focus on the ethical issues raised by AI in embryo selection alone is incomplete because it neglects how AI’s convergence with other innovative reproductive technologies raises further ethical issues. I describe how AI is acting as a catalyst for a social disruption of human reproduction and for a profound change in reproductive morality. The social disruption is the result of the convergence of improved embryo culture, the optimization of embryo selection through AI and the possibility of selecting a screened embryo. This technological interplay creates a pull towards assisted reproduction, even for those prospective parents who can reproduce without medical assistance. In discussing a fictional case of prospective parents, I argue that this","https://link.springer.com/content/pdf/10.1007/s43681-024-00651-y.pdf",""
0,"David De Cremer","Opinion piece: on the ethics of a pending AI crisis in business",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00551-1","",67,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00551-1","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00551-1.pdf",""
0,"Sara Kijewski, Elettra Ronchi, Effy Vayena","Correction: The rise of checkbox AI ethics: a review",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00582-8","",68,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00582-8","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00582-8.pdf",""
0,"Brian Kogelmann, Jeffrey Carroll","The ethics of AI automation: the importance of treating like cases alike",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00620-5","",69,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00620-5","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00620-5.pdf",""
0,"Erez Firt","Addressing corrigibility in near-future AI systems",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00484-9","",71,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00484-9","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: When we discuss future advanced autonomous AI systems, one of the worries is that these systems will be capable enough to resist external intervention, even when such intervention is crucial, for example, when the system is not behaving as intended. The rationale behind such worries is that such intelligent systems will be motivated to resist attempts to modify or shut them down so they can preserve their objectives. To mitigate and face these worries, we want our future systems to be corrigible, i.e., to tolerate, cooperate or assist many forms of outside correction. One important reason for considering corrigibility as an important safety property is that we already know how hard it is to construct AI agents with a generalized enough utility function; and the more advanced and capable the agent is, the more it is unlikely that a complex baseline utility function built into it will be perfect from the start. In this paper, we try to achieve corrigibility in (at least) systems based on known or near-future (imaginable) technology, by endorsing and integrating different approaches to building AI-based systems. Our proposal replaces the attempts to provide a corrigible utility function with the proposed corrigible software architecture; this takes the agency off the RL agent – which now becomes an RL solver – and grants it to the system as a whole.","https://link.springer.com/content/pdf/10.1007/s43681-024-00484-9.pdf",""
0,"Rashmi Singh, Anwar Ahamed Ansari","AI Ethics and Challenges in Healthcare",2023,"Handbook on Augmenting Telehealth Services","CRC Press","https://doi.org/10.1201/9781003346289-6","",75,"2025-02-04 16:55:17","book-chapter","10.1201/9781003346289-6","","",,,93,104,0,0.00,0,2,2,"","",""
0,"Ludwig Weh, Magdalena Soetebeer","AI Ethics and Neuroethics Promote Relational AI Discourse",2023,"Work and AI 2030","Springer Fachmedien Wiesbaden","https://doi.org/10.1007/978-3-658-40232-7_6","",76,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-658-40232-7_6","","",,,47,55,0,0.00,0,2,2,"","https://link.springer.com/content/pdf/10.1007/978-3-658-40232-7_6",""
0,"Maria Assunta Cappelli, Giovanna Di Marzo Serugendo","A semi-automated software model to support AI ethics compliance assessment of an AI system guided by ethical principles of AI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00480-z","",80,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00480-z","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: Compliance with principles and guidelines for ethical AI has a significant impact on companies engaged in the development of artificial intelligence (AI) systems. Specifically, ethics is a broad concept that continuously evolves over time and across cultural and geographical boundaries. International organisations (IOs), individual states, and private groups, all have an interest in defining the concept of ethics of AI. IOs, as well as regional and national bodies, have issued many decisions on AI ethics. Developing a system that complies with the ethical framework poses a complex challenge for companies, and the consequences of not complying with ethical principles can have severe consequences, making compliance with these requirements a key issue for companies. Furthermore, there is a shortage of technical tools to ensure that such AI systems comply with ethical criteria. The scarcity of ethics compliance checking tools for AI, and the current focus on defining ethical guidelines for AI development, has led us to undertake a proposal consisting in a semi-automated software model to verify the ethical compliance of an AI system’s code. To implement this model, we focus on the following important aspects: (1) a literature review to identify existing ethical compliance systems, (2) a review of principles and guidelines for ethical AI to determine the international and European views regarding AI ethics, and (3) the identification of commonly accepted principles and sub-principles of AI. These elements served to inform (4) our proposal for the design of a semi-automated software for verifying the ethical compliance of AI systems both at design-time (ethics-by-design perspective) and afterwards on the resulting software.","https://link.springer.com/content/pdf/10.1007/s43681-024-00480-z.pdf",""
0,"Kinfe Yilma","African AI Ethics?—The Role of AI Ethics Initiatives in Africa",2024,"","Elsevier BV","https://doi.org/10.2139/ssrn.4910877","",81,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.4910877","","",,,,,0,0.00,0,1,1,"","",""
0,"Gleb Papyshev","Governing AI through interaction: situated actions as an informal mechanism for AI regulation",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00446-1","",82,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00446-1","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: This article presents a perspective that the interplay between high-level ethical principles, ethical praxis, plans, situated actions, and procedural norms influences ethical AI practices. This is grounded in six case studies, drawn from fifty interviews with stakeholders involved in AI governance in Russia. Each case study focuses on a different ethical principle—privacy, fairness, transparency, human oversight, social impact, and accuracy. The paper proposes a feedback loop that emerges from human-AI interactions. This loop begins with the operationalization of high-level ethical principles at the company level into ethical praxis, and plans derived from it. However, real-world implementation introduces situated actions—unforeseen events that challenge the original plans. These turn into procedural norms via routinization and feed back into the understanding of operationalized ethical principles. This feedback loop serves as an informal regulatory mechanism, refining ethical praxis based on contextual experiences. The study underscores the importance of bottom-up experiences in shaping AI's ethical boundaries and calls for policies that acknowledge both high-level principles and emerging micro-level norms. This approach can foster responsive AI governance, rooted in both ethical principles and real-world experiences.","https://link.springer.com/content/pdf/10.1007/s43681-024-00446-1.pdf",""
0,"Renée Otmar, Rose Michael, Sharon Mullins, Katherine Day","Ethics and the use of generative AI in professional editing",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00521-7","",83,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00521-7","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: Generative artificial intelligence (GnAI) has garnered significant attention worldwide across diverse industries, including in book publishing. To date, more attention has been paid to its potential in creative collaboration and less to the editorial possibilities of its application. Interest has accelerated since the breakthrough of a new Large Language Model in late 2022. This paper engages with the ethical and industrial implications of using GnAI in a creative context, namely literary publishing. It raises crucial questions about intellectual property, trust, the author–editor relationship and publishing professionals’ evolving roles in shaping quality literature. Using a published story as a test case, we compare edits using GnAI with those by professional editors over multiple drafts and at different stages of editorial development. We consider the potential ethical implications of the use of GnAI in literary fiction editing, highlighting the principles and practices that underpin professional editing to consider how these may or may not translate in the use of GnAI. This is followed by a discussion of the risks and opportunities in using GnAI in editing literary texts in the trade publishing context.","https://link.springer.com/content/pdf/10.1007/s43681-024-00521-7.pdf",""
0,"Linus Ta-Lun Huang, Gleb Papyshev, James K. Wong","Democratizing value alignment: from authoritarian to democratic AI ethics",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00624-1","",84,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00624-1","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Value alignment is essential for ensuring that AI systems act in ways that are consistent with human values. Existing approaches, such as reinforcement learning with human feedback and constitutional AI, however, exhibit power asymmetries and lack transparency. These “authoritarian” approaches fail to adequately accommodate a broad array of human opinions, raising concerns about whose values are being prioritized. In response, we introduce the Dynamic Value Alignment approach, theoretically grounded in the principles of parallel constraint satisfaction, which models moral reasoning as a dynamic process that balances multiple value principles. Our approach also enhances users’ moral and epistemic agency by granting users greater control over the values that influence AI behavior. As a more user-centric, transparent, and participatory framework for AI ethics, our approach not only addresses the democratic deficits inherent in current practices but also ensures that AI systems are flexibly aligned with a diverse array of human values.","https://link.springer.com/content/pdf/10.1007/s43681-024-00624-1.pdf",""
0,"Simon Knight, Cormac McGrath, Olga Viberg, Teresa Cerratto Pargman","Learning about AI ethics from cases: a scoping review of AI incident repositories and cases",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00639-8","",86,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00639-8","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: Cases provide a practical resource for learning regarding the uses and challenges of AI applications. Cases give insight into how principles and values are implicated in real contexts, the trade-offs and different perspectives held regarding these contexts, and the—sometimes hidden—relationships between cases, relationships that may support analogical reasoning across contexts. We aim to (1) provide an approach for structuring ethics cases and (2) investigate existing case repository structures. We motivate a scoping review through a conceptual analysis of ethics case desirable features. The review sought to retrieve repositories, (sometimes known as observatories, catalogues, galleries, or incident databases), and their cases, for analysis of their expression of ethics concepts. We identify n = 14 repositories, extracting the case schema used in each, to identify how this metadata can express ethical concepts. We find that most repositories focus on harm-indicators, with some indicating positive impacts, but with little explicit reference to ethical concepts; a subset (n = 4) includes no structural elements addressing ethical concepts or impacts. We extract a subset of cases from the total cases (n = 2000) across repositories addressing education (n = 100). These are grouped by topic, with a structured content analysis provided of ethical implications from one sub-theme, offering qualitative insights into the ethical coverage. Our conceptual analysis and empirical review exemplify a model for ethics cases (shorthanded as Ethics-case-CPR), while highlighting gaps both in existing case repositories and specific examples of cases.","https://link.springer.com/content/pdf/10.1007/s43681-024-00639-8.pdf",""
0,"Luciano Floridi","The Gambit",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0011","",88,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0011","","",,,180,192,0,0.00,0,1,2,"Abstract: Chapter 4 presented the ethical principles that provide a framework for AI. One of these principles, beneficence, includes sustaining the planet. This and the following chapter further explore that requirement by analysing the positive and negative environmental impacts of AI. The goal is to provide policy recommendations for a path towards a greener and more climate-friendly development of AI—development especially in line with EU values and legislation. The timing is critical because AI is already used today to model events related to climate change and contribute to efforts in combating global warming. It can thus be a great positive force for a fair and sustainable society.","https://academic.oup.com/book/chapter-pdf/58148098/oso-9780198883098-chapter-11.pdf",""
0,"Asma Ayari, Ahmed Ammar","AI Accountability, Ethics, and Human Resource Implications",2024,"Ethical Challenges for the Future of Neurosurgery","Springer Nature Switzerland","https://doi.org/10.1007/978-3-031-71477-1_4","",89,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-71477-1_4","","",,,47,56,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-71477-1_4",""
0,"Luciano Floridi","AI and the UN Sustainable Development Goals",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0012","",90,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0012","","",,,193,200,0,0.00,0,1,2,"Abstract: Chapter 11 analysed the positive and negative impact of AI on climate change and offered some recommendations to increase the former and decrease the latter. Climate change is one of the areas where AI is being used to support the UN SDGs. As we saw in Chapter 9, initiatives relying on AI to deliver socially beneficial outcomes (so-called AI4SG) are on the rise. However, existing attempts to understand and foster AI4SG initiatives have so far been limited by the lack of normative analyses and a shortage of empirical evidence. Following the analyses provided in Chapters 9–11, this chapter addresses these limits by supporting use of the United Nations’ SDGs as a benchmark for tracing the scope and spread of AI4SG. The chapter also presents in more detail a database of AI4SG projects (already mentioned in Chapter 11) collected using this benchmark. Several key insights are discussed, including the extent to which different SDGs are being addressed. The goal of the chapter is to facilitate the identification of pressing problems that, if left unaddressed, risk hampering the effectiveness of AI4SG initiatives.","https://academic.oup.com/book/chapter-pdf/58148115/oso-9780198883098-chapter-12.pdf",""
0,"David M. Douglas, Justine Lacey, David Howard","Ethical risk for AI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00549-9","",91,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00549-9","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: The term ‘ethical risk’ often appears in discussions about the responsible development and deployment of artificial intelligence (AI). However, ethical risk remains inconsistently defined in this context, obscuring what distinguishes it from other forms of risk, such as social, reputational or legal risk, for example. In this paper we present a definition of ethical risk for AI as being any risk associated with an AI that may cause stakeholders to fail one or more of their ethical responsibilities towards other stakeholders. To support our definition, we describe how stakeholders have role responsibilities that follow from their relationship with the AI, and that these responsibilities are towards other stakeholders associated with the AI. We discuss how stakeholders may differ in their ability to make decisions about an AI, their exposure to risk, and whether they or others may benefit from these risks. Stakeholders without the ability to make decisions about the risks associated with an AI and how it is used are dependent on other stakeholders with this ability. This relationship places those who depend on decision-making stakeholders at ethical risk of being dominated by them. The decision-making stakeholder is ethically responsible for the risks their decisions about the AI impose on those affected by them. We illustrate our account of ethical risk for AI with two examples: AI-designed attachments for surgical robots that are optimised for treating specific patients, and self-driving ‘robotaxis’ that carry passengers on public roads.","https://link.springer.com/content/pdf/10.1007/s43681-024-00549-9.pdf",""
0,"Brij Gupta","Challenges in Large Language Model Development and AI Ethics",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5","",93,"2025-02-04 16:55:17","edited-book","10.4018/979-8-3693-3860-5","2327-0411","",,,,,0,0.00,0,1,1,"","",""
0,"Christina Cociancig, Hendrik Heuer, Andreas Breiter","AI ethics unwrapped: an empirical investigation of ethical principles in collaborative ideation processes",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00638-9","",94,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00638-9","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Motivated by ongoing criticism of the practical operationalization of ethical principles in artificial intelligence (AI) development, this study targets the ethical practice of AI developers in Germany. We focus on design as a key technological practice and developers as designers of AI-based systems when we investigate the socially, historically, and contextually influenced practice of AI ethics in the design process. We embed our methodology in value sensitive design (VSD), conduct design thinking workshops prototyping AI tools for hypothetical use cases, and ground our analysis on established ethical guidelines for AI. The results of this study reveal not only awareness of ethical principles in developers, more importantly, a strong influence of ethics on design decisions. Developers adapt their designs with technical interventions in favor of those using and being affected by their solutions. Our contribution is threefold: we establish a fine-grained categorization system of ethical principles based on AI ethics guidelines and VSD. We corroborate previous empirical research examining the prompted and self-reported influence and prioritization of ethical principles. Finally, we synthesize our findings with tangible design recommendations for AI ethics by design. We focus on recommendations for human involvement, privacy, and non-discrimination: encourage participatory AI design and avoid end-to-end automation in cases where humans are impacted; empower developers to integrate technical interventions from the onset of the design process to establish AI privacy by design; and support developers in emphasizing non-discriminatory AI, especially in contexts historically associated with discrimination.","https://link.springer.com/content/pdf/10.1007/s43681-024-00638-9.pdf",""
0,"Arisa Yasuda","Metaverse ethics: exploring the social implications of the metaverse",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00507-5","",95,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00507-5","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: The emergence of the metaverse transforms the way humans interact with computers; the metaverse brings about a new form of human-computer interaction that is more immersive, intuitive, and seamless. In the present paper we thus aim to elucidate the role of human-computer interactions in the age of the metaverse. New forms of human-computer interaction via the metaverse are beneficial for humans in many ways; at the same time, however, there are new types of social issues that are emerging as the metaverse develops and that need to be taken seriously. Specifically, we focus upon issues such as privacy, surveillance capitalism, cyber-syndromes, amplifications of other social problems, environmental problems, and discuss what regulations would be appropriate in order to balance the adequate development of the metaverse with the safety and security of it that is required for social good, in particular for sustainable development goals. We finally propose ethical design principles for the sustainable metaverse in order to address the aforementioned and other social issues.","https://link.springer.com/content/pdf/10.1007/s43681-024-00507-5.pdf",""
0,"Amna Batool, Didar Zowghi, Muneera Bano","AI governance: a systematic literature review",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00653-w","",99,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00653-w","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: As artificial intelligence (AI) transforms a wide range of sectors and drives innovation, it also introduces different types of risks that should be identified, assessed, and mitigated. Various AI governance frameworks have been released recently by governments, organizations, and companies to mitigate risks associated with AI. However, it can be challenging for AI stakeholders to have a clear picture of the available AI governance frameworks, tools, or models and analyze the most suitable one for their AI system. To fill the gap, we present the literature to answer key questions: WHO is accountable for AI systems’ governance, WHAT elements are being governed, WHEN governance occurs within the AI development life cycle, and HOW it is implemented through frameworks, tools, policies, or models. Adopting the systematic literature review (SLR) methodology, this study meticulously searched, selected, and analyzed 28 articles, offering a foundation for understanding different facets of AI governance. The analysis is further enhanced by categorizing artifacts of AI governance under team-level governance, organization-level governance, industry-level governance, national-level governance, and international-level governance. The findings of this study on existing AI governance solutions can assist research communities in proposing comprehensive AI governance practices.","https://link.springer.com/content/pdf/10.1007/s43681-024-00653-w.pdf",""
0,"Anders Søgaard","Can machines be trustworthy?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00351-z","",100,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00351-z","2730-5953","",,,,,0,0.00,0,1,2,"Abstract: AI regulators promote ‘trustworthy AI’, but what exactly does trustworthy AI mean, and what does it have to do with trust? Many philosophers argue that the phrase is a contradiction of terms. Trust, unlike reliance, is said to be a uniquely human relationship involving direct responsiveness or intent. I argue that the objective of trustworthy AI can be real trust in the general sense of Karen Jones and others, and very similar to the kind of trust we place in institutions. The idea that trustworthiness does not apply to machines, stems from a","https://link.springer.com/content/pdf/10.1007/s43681-023-00351-z.pdf",""
0,"Diego Gosmar","Conversational hyperconvergence: an onlife evolution model for conversational AI agency",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00463-0","",101,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00463-0","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00463-0.pdf",""
0,"Scott Hill","Algorithm evaluation without autonomy",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00499-2","",102,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00499-2","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: In","https://link.springer.com/content/pdf/10.1007/s43681-024-00499-2.pdf",""
0,"Kevin Mills","Technology, liberty, and guardrails",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00625-0","",103,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00625-0","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00625-0.pdf",""
0,"Fabio Tollon","Reactive agency and technology",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00366-6","",104,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00366-6","2730-5953","",,,,,0,0.00,0,1,2,"Abstract: Is there room for genuine human agency in a world populated by almost incessant technological distraction and influence? It often feels as though our technological landscape is pulling us in a number of directions, and that our agency is more a function of us","https://link.springer.com/content/pdf/10.1007/s43681-023-00366-6.pdf",""
0,"Paul Hayes, Noel Fitzpatrick, José Manuel Ferrández","From applied ethics and ethical principles to virtue and narrative in AI practices",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00472-z","",107,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00472-z","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: The question of how we can use ethics and ethical frameworks to avert the negative consequences of AI through guidance on human behaviour and the design of technological systems has recently been receiving increasing attention. The appropriate response to an ethics of AI has certainly been contentious. For some years the wisdom of deontology and utilitarianism in the ethics of technology has been questioned. Today, a kind of AI ethics principlism has gained a degree of widespread acceptance, yet it still invites harsh rejections in recent scholarship. In this paper, we wish to explore the contribution to an ethics of AI made by a narrative philosophy and ethics of technology inspired by the ‘little ethics’ of Paul Ricoeur, and virtue ethics of Alasdair MacIntyre, most recently and promisingly built upon by Wessel Reijers and Mark Coeckelbergh. The objective of this paper is to examine the extent to which a narrative and virtue based ethics (or, VPD, i.e., virtuous practice design) might be a plausible candidate for the foundation of an ethics of AI, or rather ethical AI practice. This will be achieved by exploring the ways in which this approach can respond to some of the significant faults with or critiques of applied and principles and guidelines based ethical approaches to AI ethics.","https://link.springer.com/content/pdf/10.1007/s43681-024-00472-z.pdf",""
0,"Bindu Vijayakumar, Ciza Thomas","The ethics of envisioning spam free email inboxes",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00526-2","",108,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00526-2","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00526-2.pdf",""
0,"Morten Bay","Participation, prediction, and publicity: avoiding the pitfalls of applying Rawlsian ethics to AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00341-1","",110,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00341-1","2730-5953","",4,4,1545,1554,0,0.00,0,1,2,"Abstract: Given the popularity of John Rawls’ theory of justice as fairness as an ethical framework in the artificial intelligence (AI) field, this article examines how the theory fits with three different conceptual applications of AI technology. First, the article discusses a proposition by Ashrafian to let an AI agent perform the deliberation that produces a Rawlsian social contract governing humans. The discussion demonstrates the inviability of such an application as it contradicts foundational aspects of Rawls’ theories. An exploration of more viable applications of Rawlsian theory in the AI context follows, introducing the distinction between","https://link.springer.com/content/pdf/10.1007/s43681-023-00341-1.pdf",""
0,"Brij B. Gupta, Akshat Gaurav, Kwok Tai Chui, Konstantinos Psannis","The Future of Ethical AI in Large Language Models",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch013","",112,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch013","2327-0411","",,,410,435,0,0.00,0,4,1,"As artificial intelligence (AI) continues to evolve, its ethical implications become increasingly critical. This chapter explores the future of ethical AI, highlighting emerging trends, anticipating future challenges, and proposing a roadmap for building an ethical AI ecosystem. It delves into the importance of ethics in AI, examining trends like explainable AI, fairness, and privacy preservation. The chapter identifies potential ethical dilemmas and risks associated with emerging AI technologies, offering strategies for mitigation. It emphasizes the roles of stakeholders in fostering an ethical AI environment and provides examples of successful implementations. Finally, it envisions a responsible AI future, advocating for transparency, fairness, and inclusivity in AI development, supported by robust policy and governance frameworks.","https://www.igi-global.com/viewtitle.aspx?TitleId=354403",""
0,"Zhicheng Lin","Beyond principlism: practical strategies for ethical AI use in research practices",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00585-5","",115,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00585-5","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00585-5.pdf",""
0,"Luciano Floridi","A Unified Framework of Ethical Principles for AI",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0004","",116,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0004","","",,,57,66,0,0.00,0,1,2,"Abstract: Previously, in Chapters 1–3, we saw how AI is a new form of agency that can deal with tasks and problems successfully, in view of a goal, without any need for intelligence. Every success of any AI application does not move the bar of what it means to be an intelligent agent. Instead, it bypasses the bar altogether. The success of such artificial agency is increasingly facilitated by the enveloping (that is, reshaping into AI-friendly contexts) of the environments in which AI operates. The decoupling of agency and intelligence and the enveloping of the world generate significant ethical challenges, especially in relation to autonomy, bias, explainability, fairness, privacy, responsibility, transparency, and trust (yes, mere alphabetic order). For this reason, many organizations launched a wide range of initiatives to establish ethical principles for the adoption of socially beneficial AI after the Asilomar AI Principles and the Montreal Declaration for a Responsible Development of Artificial Intelligence were published in 2017. This soon became a cottage industry. Unfortunately, the sheer volume of proposed principles threatens to overwhelm and confuse.","https://academic.oup.com/book/chapter-pdf/58147870/oso-9780198883098-chapter-4.pdf",""
0,"Zari McFadden","ACESOR: a critical engagement in systems of oppression AI assessment tool",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00478-7","",117,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00478-7","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00478-7.pdf",""
0,"Emmeke Veltmeijer, Charlotte Gerritsen","Legal and ethical implications of AI-based crowd analysis: the AI Act and beyond",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00644-x","",120,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00644-x","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: The increasing global population and the consequent rise in crowded environments have amplified the risks of accidents and tragedies. This underscores the need for effective crowd management strategies, with Artificial Intelligence (AI) holding potential to complement traditional methods. While AI offers promise in analysing crowd dynamics and predicting escalations, its deployment raises significant ethical concerns, regarding privacy, bias, accuracy, and accountability. This paper investigates the legal and ethical implications of AI in automated crowd analysis, with a focus on the European perspective. We examine the effect of the GDPR and the recently accepted AI Act on the field. The study then delves into remaining concerns post-legislation and proposes recommendations for ethical deployment. Key findings highlight challenges in notifying individuals of data usage, protecting vulnerable groups, balancing privacy with safety, and mitigating biased outcomes. Recommendations advocate for non-invasive data collection methods, refraining from predicting and decision-making AI systems, contextual considerations, and individual responsibility. The recommendations offer a foundational framework for ethical AI deployment, with universal applicability to benefit citizens globally.","https://link.springer.com/content/pdf/10.1007/s43681-024-00644-x.pdf",""
0,"Manuel Wörsdörfer","Biden’s Executive Order on AI: strengths, weaknesses, and possible reform steps",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00510-w","",122,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00510-w","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00510-w.pdf",""
0,"Ivan Mladenović","The prospects for digital democracy",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00627-y","",123,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00627-y","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00627-y.pdf",""
0,"Andreas Schönau","Agency in augmented reality: exploring the ethics of Facebook’s AI-powered predictive recommendation system",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00158-4","",124,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00158-4","2730-5953","",3,2,407,417,0,0.00,0,1,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00158-4.pdf",""
0,"Jakob Mökander, Luciano Floridi","Correction: Operationalising AI governance through ethics-based auditing: an industry case study",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00191-3","",126,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00191-3","2730-5953","",3,2,659,659,0,0.00,0,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00191-3.pdf",""
0,"Shana Kleiner, Jessica A. Grieser, Shug Miller, James Shepard, Javier Garcia-Perez, Nick Deas, Desmond U. Patton, Elsbeth Turcan, Kathleen McKeown","Unmasking camouflage: exploring the challenges of large language models in deciphering African American language &amp; online performativity",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00623-2","",127,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00623-2","2730-5953","",,,,,0,0.00,0,9,1,"Abstract: The growing accessibility of large language models (LLMs) has raised many questions about the reliability of probabilistically generated natural language responses. While researchers have documented how bias in the training data leads to biased and ethically problematic output, little attention has been paid to the problems which arise from the nature of the varieties of language on which these models are trained. In particular, certain kinds of expressive and performative language use are more common among African American social media users than they occur in the naturalistic speech of African Americans, a discrepancy which models may fail to take into account when they are training on easily-scraped data as being representative of African American speech. Because LLM training data is generally proprietary, in this work we simulate the training data using a collected dataset consisting of 274 posts from Twitter, Reddit, and Hip-Hop lyrics and analyze how LLMs interpreted their meaning. We highlight the difficulties LLMs, including GPT-3 and GPT-4, have in understanding performative AAL and examine how camouflaging and performativity are addressed (or not) by LLMs and demonstrate the harmful implications of misinterpreting online performance.","https://link.springer.com/content/pdf/10.1007/s43681-024-00623-2.pdf",""
0,"Rita Patrício Gomes","Book Review: Ethics of Privacy and Surveillance by Carissa Veliz",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00506-6","",128,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00506-6","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00506-6.pdf",""
0,"Matt A. Murphy","Using structured ethical techniques to facilitate reasoning in technology ethics",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00371-9","",129,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00371-9","2730-5953","",,,,,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00371-9.pdf",""
0,"Thanaporn Phattanaviroj, Massoud Moslehpour, Ankita Manohar Walawalkar","Data Ethics and Privacy",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch010","",131,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch010","2327-0411","",,,321,353,0,0.00,0,3,1,"The publicity about information does not appear to decrease nor do the disgraces. Confidentiality openings in the collection, use, and distribution of data have affected all the major technology users, be it Facebook, Google, and AI go beyond the business world with administrations, cities, and educational and health organizations. However, with the speedy growth of social media and advanced technology such as mobile phone apps, various investors gather and use great quantities of data, ignoring ethics and privacy. This chapter explores and discusses the ethical considerations surrounding data collection and use, with a specific focus on privacy concerns related to Large Language Model (LLM) training data. The research further discovers topics such as anonymization, data rights, and consensus, aiming to highlight the importance of ethical practices in conducting data. Additionally, the inclusion of case studies on data misuse and privacy breaches serves to provide real-world examples, emphasizing the need for vigilance and responsible approaches in the realm of data collection and utilization.","https://www.igi-global.com/viewtitle.aspx?TitleId=354400",""
0,"Reuben Sass","Equity, autonomy, and the ethical risks and opportunities of generalist medical AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00380-8","",132,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00380-8","2730-5953","",,,,,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00380-8.pdf",""
0,"Thomas Souverain","AI to renew public employment services? Explanation and trust of domain experts",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00629-w","",134,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00629-w","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00629-w.pdf",""
0,"Mark Coeckelbergh","Artificial intelligence, the common good, and the democratic deficit in AI governance",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00492-9","",135,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00492-9","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: There is a broad consensus that artificial intelligence should contribute to the common good, but it is not clear what is meant by that. This paper discusses this issue and uses it as a lens for analysing what it calls the “democracy deficit” in current AI governance, which includes a tendency to deny the inherently political character of the issue and to take a technocratic shortcut. It indicates what we may agree on and what is and should be up to (further) deliberation when it comes to AI ethics and AI governance. Inspired by the republican tradition in political theory, it also argues for a more active role of citizens and (end-)users: not only as participants in deliberation but also in ensuring, creatively and communicatively, that AI contributes to the common good.","https://link.springer.com/content/pdf/10.1007/s43681-024-00492-9.pdf",""
0,"Akshat Gaurav, Brij B. Gupta, Arcangelo Castiglione","Ethical AI Development and Deployment",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch004","",138,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch004","2327-0411","",,,115,144,0,0.00,0,3,1,"Due to the development of AI-based models and technologies, ethical AI concepts are needed. Ethical AI helps in the development and deployment of AI-based technologies and models. In this context, this chapter explains the guidelines for ethical AI and also defines the best industry practices for the inclusion of ethical AI. Further, it explains the importance of the inclusion of ethical AI in the model development lifecycle. Also, the chapter analyzes ethical testing and validation steps in detail. This chapter also examines the importance of ethical decision-making. Along with this, this chapter also studies the impact of ethical AI on society. As this chapter analyzes all the parts of the AI development lifecycle, it will help researchers and industry professionals to understand the importance of ethical AI.","https://www.igi-global.com/viewtitle.aspx?TitleId=354394",""
0,"Mona Sloane, David Danks, Emanuel Moss","Tackling AI Hyping",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00481-y","",139,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00481-y","2730-5953","",4,3,669,677,0,0.00,0,3,1,"Abstract: The introduction of a new generation of AI systems has kicked off another wave of AI hype. Now that AI systems have added the ability to produce new content to their predictive capabilities, extreme excitement about their alleged capabilities and opportunities is matched only by long held fears about job loss and machine control.","https://link.springer.com/content/pdf/10.1007/s43681-024-00481-y.pdf",""
0,"Saša Josifović","Legal and administrative frameworks as foundations for AI alignment with human volition",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00640-1","",140,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00640-1","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: This paper examines the need for regulatory frameworks in future AI development, questioning the common belief that AI, particularly Artificial General Intelligence (AGI), should be morally aligned with human welfare. It advocates for a legal-centric approach over a moral one, suggesting that in the event of AGI emergence, AI systems should be capable of self-regulation, informed by human law and jurisprudence. This aligns with Eliezer Yudkowsky's Coherent Extrapolated Volition (CEV), focusing on legal frameworks as historically grown manifestations of coherent human volition. This paper aims to contribute to the development of a comprehensive AI regulatory framework, integrating ethical and legal aspects and using historical legal records as a foundation for AI training, ensuring alignment with human values and societal norms.","https://link.springer.com/content/pdf/10.1007/s43681-024-00640-1.pdf",""
0,"Zhicheng Lin","Correction: Beyond principlism: practical strategies for ethical AI use in research practices",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00599-z","",141,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00599-z","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00599-z.pdf",""
0,"Eleonora Lima","AI art and public literacy: the miseducation of Ai-Da the robot",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00488-5","",142,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00488-5","2730-5953","",4,3,841,854,0,0.00,0,1,1,"Abstract: This article examines the implications of the artworks and public performances of the robot artist Ai-Da. While the project claims to advance AI public literacy and foster critical debate around intelligent systems, it instead ends up perpetuating popular misunderstandings about AI creativity, agency, and consciousness. Built in 2019, Ai-Da is a humanoid robot capable of creating drawings, paintings, and composing poetry. However, the project often conceals or miscommunicates the technical aspects of Ai-Da’s capabilities in a manner that encourages the public to misattribute human-like traits to the robot. This lack of transparency in the presentation of Ai-Da’s abilities and the creative processes involved risks reinforcing existing misconceptions about AI, rather than promoting a more nuanced understanding. By employing discourse analysis and drawing on scholarship on machine and computational creativity, anthropomorphism in social robots, and posthuman embodiment, this article uses the Ai-Da project as a case study to illustrate how the dangers of AI hype can be obscured when presented through the lens of public art. The analysis examines how the Ai-Da project, despite its stated goals of advancing AI literacy, fails to effectively challenge and may even exacerbate public misperceptions about the nature of AI-generated art and creativity.","https://link.springer.com/content/pdf/10.1007/s43681-024-00488-5.pdf",""
0,"Sabyasachi Pramanik","AI-Powered Hospital Accounting",2024,"Advances in Finance, Accounting, and Economics","IGI Global","https://doi.org/10.4018/979-8-3693-2185-0.ch013","",144,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-2185-0.ch013","2327-5677","",,,291,319,0,0.00,0,1,1,"Healthcare organizations have unique challenges in preserving their financial stability. Due to the rising cost of healthcare, changing payment models, and the need to provide patients with high-quality treatment, hospitals are under continual pressure to optimize their financial operations. Conventional hospital accounting methods may not be enough in this rapidly evolving environment. The potential of artificial intelligence (AI) to transform hospital accounting and provide a means of enhancing financial stability is explored in this chapter. Automation, data analytics, and advanced machine learning algorithms are used in AI-driven hospital accounting to improve accuracy, speed up financial processes, and support well-informed decision-making.","https://www.igi-global.com/viewtitle.aspx?TitleId=352621",""
0,"Dan Hendrycks","Beneficial AI and Machine Ethics",2024,"Introduction to AI Safety, Ethics, and Society","CRC Press","https://doi.org/10.1201/9781003530336-6","",146,"2025-02-04 16:55:17","book-chapter","10.1201/9781003530336-6","","",,,283,361,0,0.00,0,1,1,"","",""
0,"Suzanne Kawamleh","Algorithmic evidence in U.S criminal sentencing",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00473-y","",147,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00473-y","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00473-y.pdf",""
0,"Mariarosaria Taddeo","Adversarial and Non-kinetic Uses of AI: Conceptual and Ethical Challenges",2024,"The Ethics of Artificial Intelligence in Defence","Oxford University Press","https://doi.org/10.1093/oso/9780197745441.003.0004","",148,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780197745441.003.0004","","",,,97,123,0,0.00,0,1,1,"Abstract: This chapter analyses the use of AI for adversarial and non-kinetic purposes, like cyberwarfare. This is the most challenging of the three categories of use of AI in defence to analyse from an ethical perspective. This is because this category of use hinges on a series of conceptual changes prompted by the digital revolution. Understanding the nature of this changes is crucial to address their ethical implications adequately. The goals of this chapter are to analyse the nature of cyberwarfare and how it differs from kinetic one, and to develop a theory of just cyberwarfare, by merging Just War Theory and information ethics.","https://academic.oup.com/book/chapter-pdf/59588596/oso-9780197745441-chapter-4.pdf",""
0,"Luciano Floridi","How to Deliver a Good AI Society",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0010","",149,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0010","","",,,168,179,0,0.00,0,1,2,"Abstract: Chapters 4–6 analysed the foundational concepts that can ground a future ‘Good AI Society’. Chapters 7–9 discussed the challenges, the bad, and the good practices characterizing the use of AI systems. This chapter, along with the next, turns to some constructive and concrete recommendations about how to assess, develop, incentivize, and support good AI. In some cases, these recommendations may be undertaken directly by national or supranational policymakers. In others, changes may be led by other stakeholders ranging from civil society to private actors and sectoral organizations. The hope is that, if adopted, these recommendations may support a firm foundation for the establishment of a Good AI Society.","https://academic.oup.com/book/chapter-pdf/58148078/oso-9780198883098-chapter-10.pdf",""
0,"Louie Kangeter, Brian Patrick Green","AGI and slavery",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00618-z","",150,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00618-z","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00618-z.pdf",""
0,"Suraj Nistala, Yeran Lu, Ruoshui Liu, Zoe Yi, Zichen Huang","Balancing Innovation and Ethics: Transformative Potential and Ethical Challenges of Generative AI in Education",2025,"","Elsevier BV","https://doi.org/10.2139/ssrn.5031907","",151,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.5031907","","",,,,,0,0.00,0,5,1,"","",""
0,"Athanasios Simotas, Dimitrios Kardamakis","Medical Ethics in the Era of Artificial Intelligence: A New Landscape in Medical Practice?",2024,"AI - Ethical and Legal Challenges [Working Title]","IntechOpen","https://doi.org/10.5772/intechopen.1007438","",152,"2025-02-04 16:55:17","book-chapter","10.5772/intechopen.1007438","","",,,,,0,0.00,0,2,1,"Purpose of this study is to examine how AI interferes with medical practice by enhancing medical ethics rules and standards. Also, this research examines how AI contributes to medical confidentiality which stands as a key role of medical ethics to its mandate for beneficence. Another matter is a complete medical diagnosis that demands accurate family history and psychological information about the patients. Qualitative research has been conducted from August 2020 through December 2021 with the use of a closed type of questionnaire including both questions and case studies. The type and form of the questionnaire have been determined by the nature of the medical profession and the very limited free time of physicians. The questionnaire was distributed only to medical doctors and physicians in Greece who were registered as active members of medical associations. Within the context of Medical Ethics, AI can be used to minimize the human error and help the doctor decide according to the best interest of the patient. In the future AI will be even more capable so further research must be under way to recreate boundaries and keep AI accountable for actions or mistakes that have been made under its control.","https://intech-files.s3.amazonaws.com/a043Y00000xzFKUQA2/a09Tc000000yJxRIAU/Final-Medical%20Ethics%20in%20the%20Era%20of%20Artificial%20Intelligen%20%282024-11-04%2010%3A37%3A44%29.pdf",""
0,"Priyanshu Rawat, Prerna, Prabhdeep Singh","Ethics and Regulations of AI in Society 5.0",2023,"Artificial Intelligence and Society 5.0","Chapman and Hall/CRC","https://doi.org/10.1201/9781003397052-5","",153,"2025-02-04 16:55:17","book-chapter","10.1201/9781003397052-5","","",,,37,48,0,0.00,0,3,2,"","",""
0,"Luciano Floridi","Bad Practices",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0008","",155,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0008","","",,,113,141,0,0.00,0,1,2,"Abstract: Chapter 7 provided an overview of the various ethical challenges posed by the widespread use of algorithms. This chapter concentrates on the negative side of AI’s impact (the positive side will be discussed in Chapter 9). AI research and regulation seek to balance the benefits of innovation against any potential harms and disruption. But one unintended consequence of the recent surge in AI research is the potential reorientation of AI technologies to facilitate criminal acts, called here AI crime or AIC. We already know that AIC is theoretically feasible thanks to published experiments in automating fraud targeted at social media users as well as demonstrations of AI-driven manipulation of simulated markets. Yet because AIC is still a relatively young and inherently interdisciplinary area—spanning socio-legal studies to formal science—there remains some uncertainty about what an AIC future might look like. This chapter analyses the foreseeable threats of AIC to offer a synthesis of the current problems and outline a possible solution space.","https://academic.oup.com/book/chapter-pdf/58147945/oso-9780198883098-chapter-8.pdf",""
0,"Tricia A. Griffin, Brian P. Green, Jos V.M. Welie","The ethical wisdom of AI developers",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00458-x","",157,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00458-x","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: This paper explores ethical wisdom in the artificial intelligence (AI) developer community. Despite robust literature about the need for virtue ethics approaches in AI development, little research has directly engaged with the developer community about their progress in this regard. We have thus conducted semi-structured interviews with a worldwide cohort of 40 developers, which focused on their awareness of ethics issues, how they navigate ethical challenges, and the barriers they encounter in developing ethical wisdom. We find developers are largely aware of the ethical territories they must navigate and the moral dilemmas they personally encounter, but they face limited and inconsistent resources for ethical guidance or training. Furthermore, there are significant barriers inhibiting the development of ethical wisdom in the AI developer community, including the industry’s fixation on innovation, the narrow scope of technical practice, limited provisions for reflection and dialogue, and incentive structures that prioritize profits and prestige. The paper concludes by emphasizing the need to address the gap in domain-specific ethical skill and provides recommendations for organizations, educators, and the AI developer community.","https://link.springer.com/content/pdf/10.1007/s43681-024-00458-x.pdf",""
0,"Marten H. L. Kaas, Ibrahim Habli","Assuring AI safety: fallible knowledge and the Gricean maxims",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00490-x","",158,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00490-x","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: In this paper we argue that safety claims, when justified by a safety case, are descriptive fallible knowledge claims. Even if the aim of a safety case was to justify infallible knowledge about the safety of a system, such infallible safety knowledge is impossible to attain in the case of AI-enabled systems. By their nature AI-enabled systems preclude the possibility of obtaining infallible knowledge concerning their safety or lack thereof. We suggest that one can communicate knowledge of an AI-enabled system’s safety by structuring their exchange according to Paul Grice’s Cooperative Principle which can be achieved via adherence to the Gricean maxims of communication. Furthermore, these same maxims can be used to evaluate the calibre of the exchange, with the aim being to ensure that communicating knowledge about an AI-enabled system’s safety is of the highest calibre, in short, that the communication is relevant, of sufficient quantity and quality, and communicated perspicuously. The high calibre communication of safety claims to an epistemically diverse group of stakeholders is vitally important given the increasingly participatory nature of AI-enabled system design, development and assessment.","https://link.springer.com/content/pdf/10.1007/s43681-024-00490-x.pdf",""
0,"Andreas Tsamados, Luciano Floridi, Mariarosaria Taddeo","Human control of AI systems: from supervision to teaming",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00489-4","",159,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00489-4","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: This article reviews two main approaches to human control of AI systems: supervisory human control and human–machine teaming. It explores how each approach defines and guides the operational interplay between human behaviour and system behaviour to ensure that AI systems are effective throughout their deployment. Specifically, the article looks at how the two approaches differ in their conceptual and practical adequacy regarding the control of AI systems based on foundation models––i.e., models trained on vast datasets, exhibiting general capabilities, and producing non-deterministic behaviour. The article focuses on examples from the defence and security domain to highlight practical challenges in terms of human control of automation in general, and AI in particular, and concludes by arguing that approaches to human control are better served by an understanding of control as the product of collaborative agency in a multi-agent system rather than of exclusive human supervision.","https://link.springer.com/content/pdf/10.1007/s43681-024-00489-4.pdf",""
0,"Patricia Engel-Hermann, Alexander Skulmowski","Appealing, but misleading: a warning against a naive AI realism",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00587-3","",160,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00587-3","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: Scientists, educators, and instructional designers are facing numerous challenges due to the introduction of generative AI tools that can create appealing realistic imagery based on text prompts. Given that realism contributes to the trustworthiness of images coupled with people’s eagerness to externalize complex tasks to AI systems, the problem of a naive AI realism arises in which image creation and optimization is offloaded without considering the limitations of AI-driven technology. However, scientific visualizations and images used for educational purposes must go beyond an appealing presentation; above all, they should be accurate and factually correct. We argue that the utilization of generative AI tools for these types of visualizations requires human oversight, subject matter knowledge, and knowledge of effective design. In particular, we warn of a naive adoption of technological possibilities to “optimize” visualizations for educational purposes, such as memorability. A reductionist and naive view of AI-based optimization that fails to take into account the complex prerequisites for learning and instruction is likely to have negative consequences.","https://link.springer.com/content/pdf/10.1007/s43681-024-00587-3.pdf",""
0,"Ann-Katrien Oimann, Adriana Salatino","Command responsibility in military AI contexts: balancing theory and practicality",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00512-8","",162,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00512-8","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: Artificial intelligence (AI) has found extensive applications to varying degrees across diverse domains, including the possibility of using it within military contexts for making decisions that can have moral consequences. A recurring challenge in this area concerns the allocation of moral responsibility in the case of negative AI-induced outcomes. Some scholars posit the existence of an insurmountable “responsibility gap”, wherein neither the AI system nor the human agents involved can or should be held responsible. Conversely, other scholars dispute the presence of such gaps or propose potential solutions. One solution that frequently emerges in the literature on AI ethics is the concept of command responsibility, wherein human agents may be held responsible because they perform a supervisory role over the (subordinate) AI. In the article we examine the compatibility of command responsibility in light of recent empirical studies and psychological evidence, aiming to anchor discussions in empirical realities rather than relying exclusively on normative arguments. Our argument can be succinctly summarized as follows: (1) while the theoretical foundation of command responsibility appears robust (2) its practical implementation raises significant concerns, (3) yet these concerns alone should not entirely preclude its application (4) they underscore the importance of considering and integrating empirical evidence into ethical discussions.","https://link.springer.com/content/pdf/10.1007/s43681-024-00512-8.pdf",""
0,"Maria Pokholkova, Auxane Boch, Ellen Hohma, Christoph Lütge","Measuring adherence to AI ethics: a methodology for assessing adherence to ethical principles in the use case of AI-enabled credit scoring application",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00468-9","",163,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00468-9","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: This article discusses the critical need to find solutions for ethically assessing artificial intelligence systems, underlining the importance of ethical principles in designing, developing, and employing these systems to enhance their acceptance in society. In particular, measuring AI applications’ adherence to ethical principles is determined to be a major concern. This research proposes a methodology for measuring an application’s adherence to acknowledged ethical principles. The proposed concept is grounded in existing research on quantification, specifically, Expert Workshop, which serves as a foundation of this study. The suggested method is tested on the use case of AI-enabled Credit Scoring applications using the ethical principle of transparency as an example. AI development, AI Ethics, finance, and regulation experts were invited to a workshop. The study’s findings underscore the importance of ethical AI implementation and highlight benefits and limitations for measuring ethical adherence. A proposed methodology thus offers insights into a foundation for future AI ethics assessments within and outside the financial industry, promoting responsible AI practices and constructive dialogue.","https://link.springer.com/content/pdf/10.1007/s43681-024-00468-9.pdf",""
0,"Daniela Vacek","Two remarks on the new AI control problem",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00339-9","",165,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00339-9","2730-5953","",4,4,1403,1408,0,0.00,0,1,2,"Abstract: This paper examines the new AI control problem and the control dilemma recently formulated by Sven Nyholm. It puts forth two remarks that may be of help in (dis)solving the problem and resolving the corresponding dilemma. First, the paper suggests that the idea of complete control should be replaced with the notion of considerable control. Second, the paper casts doubt on what seems to be assumed by the dilemma, namely that control over another human being is, by default, morally problematic. I suggest that there are some contexts (namely, relations of vicarious responsibility and vicarious agency) where having considerable control over another human being is morally unproblematic, if not desirable. If this is the case, control over advanced humanoid robots could well be another instance of morally unproblematic control. Alternatively, what makes it a problematic instance remains an open question insofar as the representation of control over another human being is not sufficient for wrongness, since even considerable control over another human being is often not wrong.","https://link.springer.com/content/pdf/10.1007/s43681-023-00339-9.pdf",""
0,"Michal Gladiš, Matúš Mesarčík, Natália Slosiarová","Advising AI assistant: ethical risks of Oura smart ring",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00544-0","",168,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00544-0","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00544-0.pdf",""
0,"Dae-Hyun Yoo","A logical approach in autonomous vehicle ethics: the skeptical reasoning in dilemma",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00616-1","",173,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00616-1","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00616-1.pdf",""
0,"Kasra Ghaharian, Fatemeh Binesh, Marta Soligo, Lukasz Golab, Brett Abarbanel","AI ethics in a controversial industry: the case of gambling and its ethical paradox",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00520-8","",175,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00520-8","2730-5953","",,,,,0,0.00,0,5,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00520-8.pdf",""
0,"Isabel Richards","‘Hypernudging’: a threat to moral autonomy?",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00449-y","",178,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00449-y","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: It is well-recognised that cognitive irrationalities can be exploited to influence behaviour. ‘Hypernudging’ was coined by Karen Yeung to describe a powerful version of this phenomenon seen in digital systems that use large quantities of user data and machine learning to guide decision-making in highly personalised ways. Authors have worried about the societal impacts of the use of these capabilities at scale in commercial systems but have only begun to articulate them concretely. In this paper I look to elucidate one concern of this sort by focusing specifically on the employment of these techniques within social media and considering how it threatens our autonomy in forming moral judgments. By moral judgments I mean our judgments of someone’s actions or character as good versus bad. A threat to our autonomy in forming these is of real concern because moral judgments and their associated beliefs provide a critical backdrop for what is deemed acceptable in society, both individually and collectively and therefore what futures are possible and probable.","https://link.springer.com/content/pdf/10.1007/s43681-024-00449-y.pdf",""
0,"Yung-Hsuan Wu","Capturing the unobservable in AI development: proposal to account for AI developer practices with ethnographic audit trails (EATs)",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00535-1","",180,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00535-1","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: The prevalence of artificial intelligence (AI) tools has inspired social studies researchers, ethicists, and policymakers to seriously examine AI’s sociopolitical and ethical impacts. AI ethics literature provides guidance on which ethical principles to implement via AI governance; AI auditing literature, especially ethics-based auditing (EBA), suggests methods to verify if such principles are respected in AI model development and deployment. As much as EBA methods are abundant, I argue that most currently take a","https://link.springer.com/content/pdf/10.1007/s43681-024-00535-1.pdf",""
0,"Tuuli Turja, Anna-Aurora Kork, Sakari Ilomäki, Ingvil Hellstrand, Aino-Kaisa Koistinen","Care robot literacy: integrating AI ethics and technological literacy in contemporary healthcare",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00576-6","",182,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00576-6","2730-5953","",,,,,0,0.00,0,5,1,"Abstract: Healthcare work is guided by care ethics, and any technological changes, including the use of robots and artificial intelligence (AI), must comply with existing norms, values and work practices. By bridging technological literacy and AI ethics, this study provides a nuanced definition and an integrative conceptualization of care robot literacy (CRL) for contemporary care work. Robotized care tasks require new orientation and qualifications on the part of employees. CRL is considered as one of these new demands, which requires practitioners to have the resources, skills and understanding necessary to work with robots. This study builds on sociotechnical approach of literacy by highlighting a dynamic relationship of care robotization in which successful human–technology interaction relies on exchanges between the technological and the social. Our findings from directed content analysis and theoretical synthesis of in-demand technological literacy and AI ethics in care work emphasize competencies and situational awareness regarding both using the robot and communicating about the care robot. The initial conceptualization of CRL provides a conceptual framework for future studies, implementation and product development of care robots, drastically differing from studying, implementing and developing robots in general. In searching for technologically sound and ethically compliant solutions, the study advocates for the future significance of context-specific CRL as valuable addition to the terminology of ethical AI in healthcare.","https://link.springer.com/content/pdf/10.1007/s43681-024-00576-6.pdf",""
0,"Diana Mariana Popa","Frontrunner model for responsible AI governance in the public sector: the Dutch perspective",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00596-2","",183,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00596-2","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Across the European Union, considerable discrepancies can be observed regarding the current state of AI adoption in the public sector and the complexity of functioning AI governance structures. This can be attributed to diverse levels of digitalisation, AI maturity and governance styles across EU member states. In the field of AI implementation and AI governance models in the public sector the frontrunner is the Netherlands, scoring first in the Global Index on Responsible AI. Analysing this example of good practices in terms of AI governance, with a focus on the delegation acceptance perspective, is of relevance for the state of art on AI governance within the EU. The article looks into the structure of the public Dutch Algorithm Register which currently contains over 400 entries, the AI framework for the public sector, supervisory structures in place and risks management approaches, addressing the importance of values in the development and deployment of AI systems and algorithms. The article demonstrates how in the case of AI also, early adaptors shape future behaviours, thus carrying a burden of responsibility when developing and deploying key enabling technologies in line with the core values.","https://link.springer.com/content/pdf/10.1007/s43681-024-00596-2.pdf",""
0,"Nicole Gross","A powerful potion for a potent problem: transformative justice for generative AI in healthcare",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00519-1","",191,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00519-1","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Generative Artificial Intelligence (AI), as a transformative technology, holds significant promise for applications in healthcare. At the same time, the datafication, AI integration, and commodification of health have opened the floodgates for ethical issues, including those related to fairness, access, beneficence, democracy, solidarity, inclusion, and societal harms. As further the digitalization, innovation, and disruption of healthcare is inevitable, the paper maps out how power, equity, access, identity, participation, and knowledge contribute to creating social injustice issues. It also discusses that current justice approaches—distributive justice, representational justice, restorative justice, and capabilities-centered justice—do not have enough impact to prevent or remedy the many harms and injustices that AI has already created in healthcare or will continue to do so. The paper proposes that a transformative justice approach is needed for generative AI as a transformative technology, focused on (1) peace, emancipation, and eliminating the root causes of injustice, (2) holistic conflict resolution, (3) human rights-based approaches, and (4) the empowerment of agency and actors.","https://link.springer.com/content/pdf/10.1007/s43681-024-00519-1.pdf",""
0,"Clea Bourne","Author Correction: AI hype, promotional culture, and affective capitalism",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00534-2","",193,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00534-2","2730-5953","",4,4,1585,1585,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00534-2.pdf",""
0,"Markus Pantsar","The need for ethical guidelines in mathematical research in the time of generative AI",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-025-00660-5","",196,"2025-02-04 16:55:17","journal-article","10.1007/s43681-025-00660-5","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Generative artificial intelligence (AI) applications based on large language models have not enjoyed much success in symbolic processing and reasoning tasks, thus making them of little use in mathematical research. However, recently DeepMind’s AlphaProof and AlphaGeometry 2 applications have been reported to perform well in mathematical problem solving. These applications are hybrid systems combining large language models with rule-based systems, an approach sometimes called neuro-symbolic AI. In this paper, I present a scenario in which such systems are used in research mathematics, more precisely in theorem proving. In the most extreme case, such a system could be an autonomous automated theorem prover (AATP), with the potential of proving new humanly interesting theorems and even presenting them in research papers. The use of such AI applications would be transformative to mathematical practice and demand clear ethical guidelines. In addition to that scenario, I identify other, less radical, uses of generative AI in mathematical research. I analyse how guidelines set for ethical AI use in scientific research can be applied in the case of mathematics, arguing that while there are many similarities, there is also a need for mathematics-specific guidelines.","https://link.springer.com/content/pdf/10.1007/s43681-025-00660-5.pdf",""
0,"Naeem AllahRakha","UNESCO's AI Ethics Principles: Challenges and Opportunities",2024,"International Journal of Law and Policy","Irshad Journals","https://doi.org/10.59022/ijlp.225","",198,"2025-02-04 16:55:17","journal-article","10.59022/ijlp.225","3005-2289","",2,9,24,36,0,0.00,0,1,1,"This paper examines UNESCO's Recommendation on the Ethics of Artificial Intelligence, which outlines key principles for ensuring responsible AI development. The aim is to explore the challenges and opportunities in implementing these principles in the current AI landscape. Through a literature review, comparative analysis of existing frameworks, and case studies. This research identifies key challenges such as cultural variability, regulatory gaps, and the rapid pace of AI innovation. Conversely, it highlights opportunities like establishing global ethical standards, fostering public trust, and promoting responsible AI innovation. The study proposes strategies for overcoming challenges, including clear ethical metrics, international oversight, and ethics education in AI curricula. The findings emphasize the requirement for global cooperation and robust governance mechanisms to ensure ethical AI development. The research concludes that while implementing UNESCO's AI ethics principles is complex, it is crucial for safeguarding human rights and promoting sustainable AI growth worldwide.","https://irshadjournals.com/index.php/ijlp/article/download/225/185",""
0,"Catarina Fontes, Rohit K. Dubey","Urban futures: possibilities and challenges for ethical virtual cities",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00020","",199,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00020","","",,,290,312,0,0.00,0,2,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Goutham Sabbani","Ethics and Bias in AI: Challenges and Solutions",2022,"Journal of Artificial Intelligence, Machine Learning and Data Science","United Research Forum","https://doi.org/10.51219/jaimld/goutham-sabbani/186","",201,"2025-02-04 16:55:17","journal-article","10.51219/jaimld/goutham-sabbani/186","2583-9888","",1,1,747,749,0,0.00,0,1,3,"","",""
0,"Alexander Skulmowski","AI-Related Threats to Information Sovereignty and Challenges for Research Ethics",2024,"Educational Psychology Review","Springer Science and Business Media LLC","https://doi.org/10.1007/s10648-024-09939-1","",202,"2025-02-04 16:55:17","journal-article","10.1007/s10648-024-09939-1","1040-726X","",36,4,,,0,0.00,0,1,1,"Abstract: Unnoticed by most, some technology corporations have changed their terms of service to allow user data to be transferred to clouds and even to be used to train artificial intelligence systems. As a result of these developments, remote data collection may in many cases become impossible to be conducted anonymously. Researchers need to react by reconsidering their mode of data collection, raising awareness, and expanding informed consent to ensure information sovereignty. Considerations for data sharing are discussed.","https://link.springer.com/content/pdf/10.1007/s10648-024-09939-1.pdf",""
0,"Luciano Floridi","Good Practices",2023,"The Ethics of Artificial Intelligence","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198883098.003.0009","",204,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198883098.003.0009","","",,,142,167,0,0.00,0,1,2,"Abstract: Chapter 8 reviewed the main issues concerning the illegal use of AI or what may be called AI for social evil. This chapter focuses on AI for social good (AI4SG). AI4SG is gaining traction within information societies in general and the AI community in particular. It has the potential to tackle social problems through the development of AI-based solutions. Yet to date, there is only limited understanding of what makes AI socially good in theory, what counts as AI4SG in practice, and how to reproduce its initial successes in terms of policies. This chapter addresses the gap in understanding by first offering a definition of AI4SG, then identifying seven ethical factors that are essential for future AI4SG initiatives. The analysis is supported by some case examples of AI4SG projects. Some of the factors discussed in this chapter are almost entirely novel to AI, while the significance of other factors is heightened by the use of AI. Finally, the chapter formulates corresponding best practices from each of these factors. Subject to context and balance, these practices may serve as preliminary guidelines to ensure that well-designed AI is more likely to serve the social good. The chapter does not offer specific recommendations, which are left to Chapter 10.","https://academic.oup.com/book/chapter-pdf/58148012/oso-9780198883098-chapter-9.pdf",""
0,"Re’em Segev","The moral status of input and output discrimination",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00349-7","",209,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00349-7","2730-5953","",,,,,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00349-7.pdf",""
0,"Masashi Takeshita, Rafal Rzepka","Speciesism in natural language processing research",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00606-3","",210,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00606-3","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: Natural Language Processing (NLP) research on AI Safety and social bias in AI has focused on safety for humans and social bias against human minorities. However, some AI ethicists have argued that the moral significance of nonhuman animals has been ignored in AI research. Therefore, the purpose of this study is to investigate whether there is speciesism, i.e., discrimination against nonhuman animals, in NLP research. First, we explain why nonhuman animals are relevant in NLP research. Next, we survey the findings of existing research on speciesism in NLP researchers, data, and models and further investigate this problem in this study. The findings of this study suggest that speciesism exists within researchers, data, and models, respectively. Specifically, our survey and experiments show that (a) among NLP researchers, even those who study social bias in AI, do not recognize speciesism or speciesist bias; (b) among NLP data, speciesist bias is inherent in the data annotated in the datasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models, exhibit speciesist bias by default. Finally, we discuss how we can reduce speciesism in NLP research.","https://link.springer.com/content/pdf/10.1007/s43681-024-00606-3.pdf",""
0,"Ms. Kritika","A comprehensive study on navigating neuroethics in Cyberspace",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00486-7","",211,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00486-7","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00486-7.pdf",""
0,"Dwayne Woods","Optimizing beyond optimization: Heideggerian limits and artificial intelligence",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00607-2","",212,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00607-2","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00607-2.pdf",""
0,"Cheng-hung Tsai, Hsiu-lin Ku","Why AI may undermine phronesis and what to do about it",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00617-0","",216,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00617-0","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00617-0.pdf",""
0,"Iskender Volkan Sancar","How can we design autonomous weapon systems?",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00428-3","",217,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00428-3","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00428-3.pdf",""
0,"Torben Swoboda, Lode Lauwaert","Can artificial intelligence embody moral values?",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-025-00662-3","",219,"2025-02-04 16:55:17","journal-article","10.1007/s43681-025-00662-3","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-025-00662-3.pdf",""
0,"AASTHA PANT, Rashina Hoda, Paul McIntosh","Raising Ai Ethics Awareness Through an Ai Ethics Quiz for Software Practitioners",2024,"","Elsevier BV","https://doi.org/10.2139/ssrn.4951041","",221,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.4951041","","",,,,,0,0.00,0,3,1,"","",""
0,"Mois Navon","Eudemonia of a machine",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00553-z","",222,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00553-z","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Henry Ford once said, “For most purposes, a man with a machine is better than a man without a machine.” To this, engineers today propose an addendum – “and a man that","https://link.springer.com/content/pdf/10.1007/s43681-024-00553-z.pdf",""
0,"James Fritz","On the scope of the right to explanation",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00586-4","",223,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00586-4","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: As opaque algorithmic systems take up a larger and larger role in shaping our lives, calls for explainability in various algorithmic systems have increased. Many moral and political philosophers have sought to vindicate these calls for explainability by developing theories on which","https://link.springer.com/content/pdf/10.1007/s43681-024-00586-4.pdf",""
0,"Christos Kyriacou","Artificial moral intelligence and computability: an Aristotelian perspective",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00543-1","",224,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00543-1","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00543-1.pdf",""
0,"Airlie Hilliard, Emre Kazim, Stephan Ledain","Are the robots taking over? On AI and perceived existential risk",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00600-9","",225,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00600-9","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Artificial intelligence (AI) is increasingly infiltrating our lives, and a large proportion of the population use the technology whether they know it or not. While AI can offer significant transformative benefits, this is only true if it is used in a safe and responsible way with the right guardrails. Indeed, there have been several instances of harm resulting from the use of AI without the appropriate safeguards in place. As such, it is unsurprising that there are mixed views of AI in society, where the negative view can in fact manifest as a dystopian view of “robots taking over”. In this paper, we explore these positive and negative views of AI and the factors driving such perceptions. We propose that negative perceptions of AI often concern job displacement, bias and fairness, and misalignment with human values, while positive perceptions typically focus on specific applications and benefits of AI, such as in scientific research, healthcare, and education. Moreover, we posit that the types of perceptions one has about AI are driven by their proximity to AI, whether general or specific applications of AI are being considered, knowledge of AI, and how it is framed in the media. We end with a framework for reducing threat perceptions of AI, such that the technology can be embraced more confidently in tandem with risk management practices.","https://link.springer.com/content/pdf/10.1007/s43681-024-00600-9.pdf",""
0,"Hannah van Kolfschooten","The prospects of using AI in euthanasia and physician-assisted suicide: a legal exploration",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00491-w","",226,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00491-w","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: The Netherlands was the first country to legalize euthanasia and physician-assisted suicide. This paper offers a first legal perspective on the prospects of using AI in the Dutch practice of euthanasia and physician-assisted suicide. It responds to the Regional Euthanasia Review Committees’ interest in exploring technological solutions to improve current procedures. The specific characteristics of AI – the capability to process enormous amounts of data in a short amount of time and generate new insights in individual cases – may for example alleviate the increased workload of review committees due to the continuous increase of euthanasia cases. The paper considers three broad categories for the use of AI in the Dutch euthanasia practice: (1) the physician’s assessment of euthanasia requests, (2) the actual execution of euthanasia, and (3) the retrospective reviews of cases by the Regional Euthanasia Review Committees. Exploring the legal considerations around each avenue, both in the EU AI Act and the Dutch legal framework, this paper aims to facilitate the societal discussion on the role of technology in such deeply human decisions. This debate is equally relevant to other countries that legalized euthanasia (e.g. Belgium and Canada) or physician-assisted suicide (e.g. Switzerland and numerous states in the US).","https://link.springer.com/content/pdf/10.1007/s43681-024-00491-w.pdf",""
0,"Janvi Chhabra, Karthik Sama, Jayati Deshmukh, Srinath Srinivasa","Evaluating computational models of ethics for autonomous decision making",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00532-4","",229,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00532-4","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00532-4.pdf",""
0,"Marek Winkel","Society in charge: the connection of artificial intelligence, responsibility, and ethics in German media discourse",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00604-5","",232,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00604-5","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Artificial intelligence (AI) is playing an increasingly important role in society, and applications like ChatGPT and Dall-E, which can produce texts and pictures on their own, are becoming very popular. This development raises questions regarding ethics, values, and responsibility, as AI-generated documents may promote misinformation and erode democracy, while human actors can scarcely be held accountable. AI technology may also support an efficient, rationalized society, which has its advantages and disadvantages. Two main spheres, which influence society’s perspective on the connection between AI, ethics and responsibility, are public media debates and the legal system. Popular newspapers reach broad audiences, so insight is provided into what perspectives on these issues are helping everyday citizens form their opinions. Legal frameworks potentially regulate citizens’ and companies’ dealing with AI technology—and may get included in media discussions on AI. Acknowledging that, this article presents a two-folded analysis. First, the article presents the results of a discourse analysis of 113 articles from German newspapers, ranging from the center-left to the conservative spectrum. The analysis examined how these media frame the connection of AI, ethics, values, and responsibility. The article discusses the discourse analysis together with theoretical assumptions around the question, which actors in society could be counted as accountable in AI regards. Second, a discussion of the European AI legal system is added, to evaluate its connection with the media discourses. The article presents the results of both parts of the analysis together and finally discusses further research perspectives.","https://link.springer.com/content/pdf/10.1007/s43681-024-00604-5.pdf",""
0,"Gabriella Waters, William Mapp, Phillip Honenberger","Decisional value scores: A new family of metrics for ethical AI-ML",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00504-8","",233,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00504-8","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Research in ethical AI has made strides in quantitative expression of ethical values such as fairness, transparency, and privacy. Here we contribute to this effort by proposing a new family of metrics called “decisional value scores” (DVS). DVSs are scores assigned to a system based on whether the decisions it makes meet or fail to meet a particular standard (either individually, in total, or as a ratio or average over decisions made). Advantages of DVS include greater discrimination capacity between types of ethically relevant decisions and facilitation of ethical comparisons between decisions and decision-making systems, including across different modalities (for instance: human, machine, or coupled human–machine systems). After clarifying ambiguities in the concept of “decision” itself, including the question of how to individuate the decisions made by a system, we discuss the role and meaning of “decision” in common AI and machine learning approaches such as decision trees, neural networks, SVMs, and unsupervised classifiers. We then show how DVSs may be defined for several ethical values of interest, with an extended discussion of transparency. Finally, we explore how such metrics can be applied to real decision-making systems through two case studies: evaluations of LLMs for transparency; and evaluations of criminal risk assessment tools for utility, rights violations, fairness, and transparency.","https://link.springer.com/content/pdf/10.1007/s43681-024-00504-8.pdf",""
0,"Mosiur Rahaman, Princy Pappachan, Sheila Mae Orozco, Shavi Bansal, Varsha Arya","AI Safety and Security",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch011","",234,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch011","2327-0411","",,,354,383,0,0.00,0,5,1,"The chapter “AI Safety and Security” presents a comprehensive and multi-dimensional exploration, addressing the critical aspects of safety and security in the context of large language models. The chapter begins by identifying the risks and threats posed by LLMs, delving into vulnerabilities such as bias, misinformation, and unintended AI interactions, impacts like privacy concerns. Building on these identified risks, it then explores the strategies and methodologies for ensuring AI safety, focusing on principles like robustness, transparency, and accountability and discussing the challenges of implementing these safety measures. It concludes with an insight into long-term AI safety research, highlighting ongoing efforts and future directions to sustain AI system safety amidst rapid technological advancements and encouraging a collaborative approach among various stakeholders. By integrating perspectives from computer science, ethics, law, and social sciences, the chapter provides an insightful and comprehensive analysis of current and future challenges in AI safety and security.","https://www.igi-global.com/viewtitle.aspx?TitleId=354401",""
0,"Tom Stenson","What does it mean to be good? The normative and metaethical problem with ‘AI for good’",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00501-x","",235,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00501-x","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00501-x.pdf",""
0,"Jennifer Chubb, David Beer","Establishing counterpoints in the sonic framing of AI narratives",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00404-3","",237,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00404-3","2730-5953","",4,3,679,690,0,0.00,0,2,2,"Abstract: In order to challenge dominant representations and conceptions of artificial intelligence (AI), this article explores how AI is sonically represented in documentaries. Using a corpus of documentaries alongside expert interviews with sound designers, we explore the ways in which music and sound may influence perception about AI. The notion of ‘counterpoint’ in music theory is developed as a concept to capture and explain how the integrated dynamics of human/machines are represented within these sonic framings. The concept of the counterpoint allows us to reflect on how the relations between AI and the human and how they are sonically framed in ways that separate and blend without recourse to reductive or binary futures, which potentially misrepresent AI capabilities and performance. The article identifies and develops four types of counterpoint in what we refer to as AI sonic narratives. This article provides a framework from which AI could be sonically framed responsibly, which is critical when misinformation and hype impede the public understanding of science.","https://link.springer.com/content/pdf/10.1007/s43681-023-00404-3.pdf",""
0,"Benno Blumoser","Exploring AI with Purpose",2023,"CSR, Sustainability, Ethics &amp; Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-09245-9_9","",238,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09245-9_9","2196-7075","",,,197,204,0,0.00,0,1,2,"Abstract: Never get complacent: Developing AI solutions doesn’t just take expertise. It also means fostering an intrapreneurial work culture while keeping in mind the greater good our work serves. That’s what we do at the Siemens AI Lab.","https://link.springer.com/content/pdf/10.1007/978-3-031-09245-9_9",""
0,"Haleh Asgarinia","Adopting trust as an ex post approach to privacy",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00421-w","",239,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00421-w","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: This research explores how a person with whom information has been shared and, importantly, an artificial intelligence (AI) system used to deduce information from the shared data contribute to making the disclosure context private. The study posits that private contexts are constituted by the interactions of individuals in the social context of intersubjectivity based on trust. Hence, to make the context private, the person who is the trustee (i.e., with whom information has been shared) must fulfil trust norms. According to the commitment account of trustworthiness, a person is trustworthy only if they satisfy the norm of competence. It is argued that a person using an AI system to answer a question is competent only if they are ex post justified in believing what has been delivered by the AI system. A person’s belief is justified in the doxastic sense only if the AI system is accurate. This feature of AI’s performance affects a person’s competence and, as a result, trustworthiness. The effect of AI on trust as an essential component of making the context private, and thus on privacy, means an AI system also impacts privacy. Therefore, a private context is constituted when the individual with whom the information is shared fulfils the competence norm and the AI system used for analysing the information is sufficiently accurate to adhere to this norm. The result of this research emphasises the significance of the relationship between individuals involved in information-sharing and how an AI system used for analysing that information impacts the relationship regarding making the context private, as well as how it impacts privacy. The findings of this research have significant implications for improving or ameliorating privacy regulations in light of trust.","https://link.springer.com/content/pdf/10.1007/s43681-024-00421-w.pdf",""
0,"Jack Madock","Robot warfare: the (im)permissibility of autonomous weapons systems",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00567-7","",241,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00567-7","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00567-7.pdf",""
0,"Elin Sporrong, Cormac McGrath, Teresa Cerratto Pargman","Situating AI in assessment—an exploration of university teachers’ valuing practices",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00558-8","",243,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00558-8","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Emerging AI technologies are changing teachers’ assessment practices and posing higher education institutions with novel ethical dilemmas. While frameworks and guidelines promise to align technology with moral and human values, the dilemma of how AI may impact existing valuing practices is often overlooked. To examine this gap, we conducted an interview study with university teachers from different disciplines at a university in Sweden. Following a semi-structured study design, we explored university teachers’ anticipations of AI in assessment and examined how emerging AI technologies may reconfigure the fit between values, challenges, and activities situated in everyday assessment contexts. Our findings suggest that anticipated AI, including automation and AI-mediated communication and grading, may amplify and reduce teachers’ possibilities to align activities with professional, pedagogical, and relational values and solve current challenges. In light of the study’s findings, the paper discusses potential ethical issues in the anticipated shifts from human to automated assessment and possible new and reinforced challenges brought by AI for education.","https://link.springer.com/content/pdf/10.1007/s43681-024-00558-8.pdf",""
0,"Jacob Sparks, Ava Thomas Wright","Models of rational agency in human-centered AI: the realist and constructivist alternatives",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-025-00658-z","",244,"2025-02-04 16:55:17","journal-article","10.1007/s43681-025-00658-z","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-025-00658-z.pdf",""
0,"Blair Attard-Frost, Kelly Lyons","AI governance systems: a multi-scale analysis framework, empirical findings, and future directions",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00569-5","",245,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00569-5","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00569-5.pdf",""
0,"Georgios Stathis, Jaap van den Herik","Ethical and preventive legal technology",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00413-2","",247,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00413-2","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: Preventive Legal Technology (PLT) is a new field of Artificial Intelligence (AI) investigating the","https://link.springer.com/content/pdf/10.1007/s43681-023-00413-2.pdf",""
0,"Alessandra Cenci","Citizen science and negotiating values in the ethical design of AI-based technologies targeting vulnerable individuals",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00636-x","",249,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00636-x","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Citizen science is the new mantra both in academic circles and the public discourse. While the citizen science ideal is conceptually broad, If and how it can be realized in fields often depicted as value free/value neutral—such as applied AI—is controversial. The practical challenges in generating ethical AI encapsulating the citizen science ideal are addressed by targeting scientific practices underlying the participatory design of an AI-based tracking app aimed at enhancing the safety and wellbeing of vulnerable citizens with dementia of a Danish municipality through the engagement of the local community. The focus is on the process of","https://link.springer.com/content/pdf/10.1007/s43681-024-00636-x.pdf",""
0,"Cristina Poncibò, Michel Cannarsa","AI and the Law",2022,"The Cambridge Handbook of Artificial Intelligence","Cambridge University Press","https://doi.org/10.1017/9781009072168.037","",251,"2025-02-04 16:55:17","book-chapter","10.1017/9781009072168.037","","",,,419,428,0,0.00,0,2,3,"","",""
0,"Boy Firmansyah","Cybersecurity Fundamentals",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch009","",252,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch009","2327-0411","",,,280,320,0,0.00,0,1,1,"In today's interconnected digital landscape, cybersecurity has become paramount to safeguarding sensitive information and digital assets. This abstract explores the fundamental principles of cybersecurity, highlighting its importance, key concepts, and best practices. Cybersecurity encompasses various measures and strategies aimed at protecting computers, networks, and data from unauthorized access, cyberattacks, and other malicious activities. Understanding the threats posed by cybercriminals, hackers, and other malicious actors is crucial for developing effective cybersecurity strategies. Key concepts in cybersecurity include authentication, encryption, access control, and threat detection. Authentication verifies the identity of users and devices accessing a system, while encryption scrambles data to prevent unauthorized access. Access control ensures that only authorized individuals or processes can access specific resources, reducing the risk of data breaches.","https://www.igi-global.com/viewtitle.aspx?TitleId=354399",""
0,"Ahmed S. Almasoud, Jamiu Adekunle Idowu","Algorithmic fairness in predictive policing",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00541-3","",253,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00541-3","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: The increasing use of algorithms in predictive policing has raised concerns regarding the potential amplification of societal biases. This study adopts a two-phase approach, encompassing a systematic review and the mitigation of age-related biases in predictive policing. Our systematic review identifies a variety of fairness strategies in existing literature, such as domain knowledge, likelihood function penalties, counterfactual reasoning, and demographic segmentation, with a primary focus on racial biases. However, this review also highlights significant gaps in addressing biases related to other protected attributes, including age, gender, and socio-economic status. Additionally, it is observed that police actions are a major contributor to model discrimination in predictive policing. To address these gaps, our empirical study focuses on mitigating age-related biases within the Chicago Police Department's Strategic Subject List (SSL) dataset used in predicting the risk of being involved in a shooting incident, either as a victim or an offender. We introduce Conditional Score Recalibration (CSR), a novel bias mitigation technique, alongside the established Class Balancing method. CSR involves reassessing and adjusting risk scores for individuals initially assigned moderately high-risk scores, categorizing them as low risk if they meet three criteria: no prior arrests for violent offenses, no previous arrests for narcotic offenses, and no involvement in shooting incidents. Our fairness assessment, utilizing metrics like Equality of Opportunity Difference, Average Odds Difference, and Demographic Parity, demonstrates that this approach significantly improves model fairness without sacrificing accuracy.","https://link.springer.com/content/pdf/10.1007/s43681-024-00541-3.pdf",""
0,"Nathan Gabriel Wood","Regulating autonomous and AI-enabled weapon systems: the dangers of hype",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00448-z","",255,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00448-z","2730-5953","",4,3,805,817,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00448-z.pdf",""
0,"Federico Benitez, Cyriel Pennartz, Walter Senn","The conductor model of consciousness, our neuromorphic twins, and the human-AI deal",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00580-w","",257,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00580-w","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Critics of Artificial Intelligence (AI) posit that artificial agents cannot achieve consciousness even in principle, because they lack certain necessary pre-conditions present in biological agents. Here we highlight arguments from a neuroscientific and neuromorphic engineering perspective as to why such a strict denial of consciousness in artificial agents is not compelling. Based on the construction of a co-evolving neuromorphic twin, we argue that the differences between a developing biological and artificial brain are not fundamental and are vanishing with progress in neuromorphic architecture designs mimicking the human blueprint. To characterise this blueprint, we propose the Conductor Model of Consciousness (CMoC) that builds on neuronal implementations of an external and internal world model, while gating and labelling information flows. An extended turing test lists functional and neuronal correlates of biological consciousness that are captured by the CMoC. These correlates provide the grounding for how biological or artificial agents learn to distinguish between sensory activity generated from outside or inside of the brain, how the perception of these activities can itself be learned, and how the information flow for learning an internal world model is orchestrated by a cortical meta-instance, which we call the conductor. Perception comes with the distinction of sensory and affective components, with the affective component linking to ethical questions that are inherent in our multidimensional model of consciousness. Recognizing the existence of a blueprint for a possible artificial consciousness encompasses functional, neuronal and ethical dimensions, begging the question: How should we behave towards agents that are akin to us in the inner workings of their brains? We sketch a human-AI deal, balancing the growing cognitive abilities of artificial agents, and the possibility to relieve them from suffering of negative affects, with a protection for the rights of humans.","https://link.springer.com/content/pdf/10.1007/s43681-024-00580-w.pdf",""
0,"Tomasz Hollanek","The ethico-politics of design toolkits: responsible AI tools, from big tech guidelines to feminist ideation cards",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00545-z","",259,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00545-z","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: This paper interrogates the belief in","https://link.springer.com/content/pdf/10.1007/s43681-024-00545-z.pdf",""
0,"Enrico Moch","Liability Issues in the Context of Artificial Intelligence: Legal Challenges and Solutions for AI-Supported Decisions",2024,"East African Journal of Law and Ethics","East African Nature and Science Organization","https://doi.org/10.37284/eajle.7.1.2518","",261,"2025-02-04 16:55:17","journal-article","10.37284/eajle.7.1.2518","2707-5338","",7,1,214,234,0,0.00,0,1,1,"Artificial intelligence (AI), which enhances efficiency, production, and decision-making, has rapidly become a crucial component in sectors such as healthcare, banking, education, and transportation. However, as AI systems increasingly integrate into critical aspects of daily life, significant legal challenges related to liability, transparency, and accountability arise. The issue is that it can be challenging to assign blame for judgments made by AI, particularly when self-learning systems are involved and go beyond initial programming. In addition to algorithmic bias, opaque decision-making procedures, and third-party involvement, there are ambiguities in the assignment of accountability among developers, operators, and users. The purpose of this study is to discuss these legal issues and offer workable answers to guarantee fairness and responsibility in AI-assisted decision-making. In order to streamline compensation by emphasizing causality rather than culpability, key findings recommend the implementation of strict responsibility for high-risk AI applications. Accountability and traceability can be increased by increasing transparency through required paperwork and explainable AI systems. Uncertainty can be decreased by using explicit contractual frameworks to clearly define roles for developers, operators, and users. Furthermore, the creation of specialist liability insurance can promote the appropriate use of AI while providing financial protection for stakeholders. Building public trust and making sure AI advances society without endangering it needs striking a balance between innovation and moral and legal obligations. Cross-border AI applications require international harmonization of legal norms, such as the GDPR and the EU's AI Act, in order to establish a uniform regulatory framework. To ensure justice, fairness, and the well-being of society, these extensive legal reforms are required to close the gap between accountability and technological innovation","https://journals.eanso.org/index.php/eajle/article/download/2518/3179",""
0,"Joel Janhonen","Socialisation approach to AI value acquisition: enabling flexible ethical navigation with built-in receptiveness to social influence",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00372-8","",262,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00372-8","2730-5953","",,,,,0,0.00,0,1,2,"Abstract: This article describes an alternative starting point for embedding human values into artificial intelligence (AI) systems. As applications of AI become more versatile and entwined with society, an ever-wider spectrum of considerations must be incorporated into their decision-making. However, formulating less-tangible human values into mathematical algorithms appears incredibly challenging. This difficulty is understandable from a viewpoint that perceives human moral decisions to primarily stem from intuition and emotional dispositions, rather than logic or reason. Our innate normative judgements promote prosocial behaviours which enable collaboration within a shared environment. Individuals internalise the values and norms of their social context through socialisation. The complexity of the social environment makes it impractical to consistently apply logic to pick the best available action. This has compelled natural agents to develop mental shortcuts and rely on the collective moral wisdom of the social group. This work argues that the acquisition of human values cannot happen just through rational thinking, and hence, alternative approaches should be explored. Designing receptiveness to social signalling can provide context-flexible normative guidance in vastly different life tasks. This approach would approximate the human trajectory for value learning, which requires social ability. Artificial agents that imitate socialisation would prioritise conformity by minimising detected or expected disapproval while associating relative importance with acquired concepts. Sensitivity to direct social feedback would especially be useful for AI that possesses some embodied physical or virtual form. Work explores the necessary faculties for social norm enforcement and the ethical challenges of navigating based on the approval of others.","https://link.springer.com/content/pdf/10.1007/s43681-023-00372-8.pdf",""
0,"Ana Santana González, Lucia Rampino","A design perspective on how to tackle gender biases when developing AI-driven systems",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00386-2","",263,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00386-2","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: A growing awareness of bias in artificial intelligence (AI) systems has recently emerged, leading to an increased number of publications discussing ethics in AI. Nevertheless, the specific issue of gender bias remains under-discussed. How can design contribute to preventing the emergence of gender bias in AI-driven systems? To answer this question, we investigated the current state of AI ethical guidelines within the European Union. The results revealed that most guidelines do not acknowledge gender bias but address discrimination. This raised our concerns, as addressing multiple biases simultaneously might not effectively mitigate any of them due to their often-unconscious nature. Furthermore, our results revealed a lack of quantitative evidence supporting the effectiveness of bias prevention implementation methods and solutions. In conclusion, based on our analysis, we propose four recommendations for designing effective guidelines to tackle gender biases in AI. Moreover, we stress the central role of diversity in embedding the gender perspective from the beginning in any design activity.","https://link.springer.com/content/pdf/10.1007/s43681-023-00386-2.pdf",""
0,"Victor S. Y. Lo, Sayan Datta, Youssouf Salami","Bringing practical statistical science to AI and predictive model fairness testing",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00518-2","",264,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00518-2","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Artificial Intelligence, Machine Learning, Statistical Modeling and Predictive Analytics have been widely used in various industries for a long time. More recently, AI Model Governance including AI Ethics has received significant attention from academia, industry, and regulatory agencies. To minimize potential unjustified treatment disfavoring individuals based on demographics, an increasingly critical task is to assess group fairness through some established metrics. Many commercial and open-source tools are now available to support the computations of these fairness metrics. However, this area is largely based on rules, e.g., metrics within a prespecified range would be considered satisfactory. These metrics are statistical estimates and are often based on limited sample data and therefore subject to sampling variability. For instance, if a fairness criterion is barely met or missed, it is often uncertain if it should be a “pass” or “failure,” if the sample size is not large. This is where statistical science can help. Specifically, statistical hypothesis testing enables us to determine whether the sample data can support a particular hypothesis (e.g., falling within an acceptable range) or the observations may have happened by chance. Drawing upon the bioequivalence literature from medicine and advanced hypothesis testing in statistics, we propose a practical statistical significance testing method to enhance the current rule-based process for model fairness testing and its associated power calculation, followed by an illustration with a realistic example.","https://link.springer.com/content/pdf/10.1007/s43681-024-00518-2.pdf",""
0,"Ewa Milczarek","Artificial intelligence’s right to life",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00296-3","",269,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00296-3","2730-5953","",4,2,587,592,0,0.00,0,1,2,"Abstract: The right to life is fundamental and primary and is a precondition for exercising other rights (Ramcharan in Ramcharan (ed), The right to life in International Law, Martinus Nijhoff Publishers, Dordrecht, 1985). Its universal recognition in the arena of international law is associated with the concept of a human being endowed with inherent and inalienable dignity. Categorization of the circle of entities covered with the right to life today seems obvious and indisputable. Intense development of artificial intelligence, also the fact that it has passed the Turing test which checks AI’s thinking ability in a way similar to human reasoning, inspires a reflection on AI’s future legal status. This study will investigate a thesis of whether artificial intelligence may be entitled to the right to life. The analysis will be carried out around an exploratory question: what are the requirements for being afforded protection of the right to life?","https://link.springer.com/content/pdf/10.1007/s43681-023-00296-3.pdf",""
0,"Avish Vijayaraghavan, Cosmin Badea","Minimum levels of interpretability for artificial moral agents",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00536-0","",270,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00536-0","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: As artificial intelligence (AI) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. For models involved in moral decision-making (MDM), also known as artificial moral agents (AMA), interpretability provides a way to trust and understand the agent’s internal reasoning mechanisms for effective use and error correction. In this paper, we bridge the technical approaches to interpretability with construction of AMAs to establish minimal safety requirements for deployed AMAs. We begin by providing an overview of AI interpretability in the context of MDM, thereby framing different levels of interpretability (or transparency) in relation to the different ways of constructing AMAs. Introducing the concept of the Minimum Level of Interpretability (MLI) and drawing on examples from the field, we explore two overarching questions: whether a lack of model transparency prevents trust and whether model transparency helps us sufficiently understand AMAs. Finally, we conclude by recommending specific MLIs for various types of agent constructions, aiming to facilitate their safe deployment in real-world scenarios.","https://link.springer.com/content/pdf/10.1007/s43681-024-00536-0.pdf",""
0,"Daniel Friedrich","Are AI safety and AI ethics memetic rivals?",2023,"","Center for Open Science","https://doi.org/10.31234/osf.io/3rpwt","",273,"2025-02-04 16:55:17","posted-content","10.31234/osf.io/3rpwt","","",,,,,0,0.00,0,1,2,"<p>As the risks of artificial intelligence (AI) attract the spotlight of public attention, policy-makers turn to academia to indicate which problems to prioritize. While some argue they should first deal with the prospect of a global catastrophe (AI safety), others believe AI’s current social impact bears greater urgency (AI ethics). This has led some to express the concern that one or the other „diverts the public’s attention“. In this article, I sketch out the psychological landscape, in which these concerns arise as a logical reaction, but suggest that in the case of AI risks, they are misplaced. I ran a survey, in which students were asked regarding their attitudes towards the issues commonly labeled under „AI ethics“ and „AI safety“. The results suggest that when salience of AI safety is experimentally increased, people report higher support for solving the problems related to AI ethics. Secondly, the levels of concern for AI safety and AI ethics correlate positively. Therefore, in terms of public-facing communication, AI safety and AI ethics seem like memetic allies, rather than rivals.</p>","",""
0,"Gerry Firmansyah, Shavi Bansal, Ankita Manohar Walawalkar, Suman Kumar, Sourasis Chattopadhyay","The Future of Ethical AI",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch005","",275,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch005","2327-0411","",,,145,177,0,0.00,0,5,1,"The rapid integration of artificial intelligence (AI) into various sectors such as healthcare, transportation, employment, automation, and judicial decisions has brought forth significant ethical challenges. This chapter explores the critical importance of establishing ethical frameworks to guide AI development and implementation. It addresses the philosophical, societal, and technical issues arising from AI technologies, emphasizing the need for fairness, accountability, and transparency to mitigate risks like bias, security, privacy violations, and racial inequities. The chapter highlights significant historical events, such as Google's dismissal of AI ethics researchers and the Uber autonomous vehicle fatality, which underscore the urgency of robust ethical governance in AI. Furthermore, it discusses the evolution of ethical awareness within AI development, the challenges of creating impartial AI systems, and the role of diverse perspectives in mitigating biases. Technological solutions such as explainable AI, fairness metrics, and synthetic data generation are examined for their potential to enhance ethical AI practices. The chapter also delves into global policy and regulatory efforts, illustrating the need for international collaboration to standardize AI ethics. Finally, it underscores the significance of cultural perspectives and societal norms in shaping AI ethics and advocates for comprehensive education and training in AI ethics for both professionals and the public. This multidisciplinary approach aims to ensure that AI technologies are developed and deployed in a manner that upholds human dignity and rights worldwide.","https://www.igi-global.com/viewtitle.aspx?TitleId=354395",""
0,"Alberto Romele","The AI Imaginary: AI, Ethics, and Communication",2024,"Handbook on the Ethics of Artificial Intelligence","Edward Elgar Publishing","https://doi.org/10.4337/9781803926728.00023","",277,"2025-02-04 16:55:17","book-chapter","10.4337/9781803926728.00023","","",,,262,273,0,0.00,0,1,1,"","https://www.elgaronline.com/view/book/9781803926728/9781803926728.xml",""
0,"Clayton Peterson","Automated ethical decision, value-ladenness, and the moral prior problem",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00482-x","",278,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00482-x","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Part of the literature on machine ethics and ethical artificial intelligence focuses on the idea of defining autonomous ethical agents able to make ethical choices and solve dilemmas. While ethical dilemmas often arise in situations characterized by uncertainty, the standard approach in artificial intelligence is to use rational choice theory and maximization of expected utility to model how algorithm should choose given uncertain outcomes. Motivated by the","https://link.springer.com/content/pdf/10.1007/s43681-024-00482-x.pdf",""
0,"Sue Anne Teo","Artificial intelligence and its ‘slow violence’ to human rights",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00547-x","",279,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00547-x","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Human rights concerns in relation to the impacts brought forth by artificial intelligence (‘AI’) have revolved around examining how it affects specific rights, such as the right to privacy, non-discrimination and freedom of expression. However, this article argues that the effects go deeper, potentially challenging the foundational assumptions of key concepts and normative justifications of the human rights framework. To unpack this, the article applies the lens of ‘slow violence’, a term borrowed from environmental justice literature, to frame the grinding, gradual, attritional harms of AI towards the human rights framework.","https://link.springer.com/content/pdf/10.1007/s43681-024-00547-x.pdf",""
0,"Suvidha Agarwal, Preeta Rajiv Sivaraman","Education in the Era of Generative AI",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-9173-0.ch008","",282,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-9173-0.ch008","2327-0411","",,,223,248,0,0.00,0,2,1,"Gen artificial intelligence (AI) is revolutionizing the higher education sector by employing deep learning models to generate information that closely mimics human content. However, its introduction into educational settings raises questions about factors like academic integrity, moral ethics and potential impacts on critical thinking skills. The emergence of generative artificial intelligence (AI) may seem to be a reason for concern which has led to swift prohibitions by organizations and educational agencies. The chapter proposes to put in detail the benefits of using Generative AI by the stakeholders of the education sector, along with the ethics and values to be taken care of and thereby avoiding the reasons to negate the usage of the AI. This chapter will cover the use of Generative AI in different educational settings, the tools and methods employed, the efficiency of GAI in enhancing teaching and learning, the influence on student outcomes, and the possible drawbacks and moral dilemmas related to its application.","https://www.igi-global.com/viewtitle.aspx?TitleId=357140",""
0,"Geetika Madaan, Satish Kumar Asthana, Jaskiran Kaur","Generative AI",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-8557-9.ch004","",283,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-8557-9.ch004","2327-0411","",,,88,121,0,0.00,0,3,1,"This study provides a comprehensive view of the state of generative AI today, touching on its uses, foundational models, obstacles, prospects, and potential future courses of action. Autoregressive models like Transformers, GANs, and Variational Autoencoders (VAEs) are the backbone of generative AI. Generated AI still has a way to go before fully realizing its potential. Problems with model interpretability, training stability, and generated content bias are all examples of such challenges. Computer scientists, psychologists, and ethicists must work together to find solutions to these problems. Generative AI does, however, offer tremendous potential. Artists, designers, and storytellers have new tools at their fingertips. Improving the robustness of models, granting greater control over generated outputs, and investigating uses in interactive storytelling and real-time content production are all potential future areas for generative AI.","https://www.igi-global.com/viewtitle.aspx?TitleId=354604",""
0,"Anastasios Nikolaos Kanellopoulos","Counterintelligence, Artificial Intelligence and National Security: Synergy and Challenges",2024,"Journal of Politics and Ethics in New Technologies and AI","National Documentation Centre (EKT)","https://doi.org/10.12681/jpentai.35617","",284,"2025-02-04 16:55:17","journal-article","10.12681/jpentai.35617","2944-9243","",3,1,,,0,0.00,0,1,1,"Counterintelligence (CI) and Artificial Intelligence (AI) represent two distinct yet interconnected domains that play pivotal roles in safeguarding National and International Security. On the first hand, CI involves activities and measures taken to identify, prevent and counter any Intelligence activities of hostile entities, such as spying, sabotage and information gathering. On the other hand, AI refers to the development and use of computer systems that can perform tasks that typically require human intelligence, such as learning, reasoning and problem-solving. Subsequently, in the ever-evolving landscape of global security, the rise of AI has ushered in a new era of CI practices. The present paper delves into the intersection of CI and AI, exploring the profound impact of AI on the CI processes and how it is transforming National Security strategies, highlighting at the same time the fields of mutually influence. Ultimately, underscores the imperative of harnessing AI's potential to strengthen CI efforts in an ever-evolving threat landscape. Plus, it investigates the ethical concerns and privacy implications associated with AI in CI emphasizing the imperative of responsible AI development and deployment. Finally, through comprehensive international case studies, offers insights into how United States, China, Russia and Israel have integrated AI into their Intelligence and CI strategies, shedding light on the diverse approaches and challenges faced by different countries. Summarizing, the paper underscores the potential synergy between AI and CI, while also acknowledging the formidable challenges it presents, such as privacy concerns and adversarial AI. Striking a balance between harnessing AI's power and safeguarding national interests remains a pivotal task for policymakers and intelligence agencies in the ever-evolving landscape of national security.","https://ejournals.epublishing.ekt.gr/index.php/jpentai/article/download/35617/27796",""
0,"","Integrating AI into Malaysian School Counselling: A Study on Opportunities, Challenges, and Ethics",2024,"Journal of Contemporary Issues and Thought","Universiti Pendidikan Sultan Idris","https://doi.org/10.37134/jcit.vol14.2.1.2024","",285,"2025-02-04 16:55:17","journal-article","10.37134/jcit.vol14.2.1.2024","0128-0481","",,14,1,8,0,0.00,0,0,1,"The use of artificial intelligence (AI) in school counseling has the potential to revolutionize the way counseling services are delivered through the provision of personalized support and data-driven decision-making. This paper explores the potential opportunities, challenges, and ethical considerations of using AI in Malaysian school counseling. The paper also highlights several ethical considerations that must be taken into account when integrating AI into school counseling practices, such as protecting students’ privacy and avoiding bias in AI-generated data. Overall, this paper provides a comprehensive exploration on the potential of using AI in Malaysian school counseling and highlights the opportunities, challenges, and ethical considerations that are applicable when implementing the technology into school counseling.","",""
0,"Swapna Nadakuditi, Bhargava Kumar, Tejaswini Kumar","AI and Machine Learning in Healthcare - Applications, Challenges and Ethics",2024,"International Journal of Health Sciences","CARI Journals Limited","https://doi.org/10.47941/ijhs.1949","",286,"2025-02-04 16:55:17","journal-article","10.47941/ijhs.1949","2710-2564","",7,4,36,43,0,0.00,0,3,1,"Purpose: This research aims to discuss how AI and machine learning can be used in healthcare, challenges associated with implementation and the ethics around the widespread adoption of AI in the health care ecosystem while understanding the regulations around the technology implementation. Methodology: By conducting qualitative analysis on various applications of AI and machine learning in health care and its impacts on patient care, the analysis summarizes the challenges and ethics associated with the implementation. Findings: Results indicate that in the last few years, the data collected in the healthcare industry has increased manifold. Some studies suggest that structured data is growing by 40% each year, unstructured data is growing by over 80% and global data produced is forty zettabytes (ZB) as of 2020. With the increased regulatory and compliance requirements, effective data governance is a mandate for industries like healthcare where there is greater focus on data privacy, data security and personal information protection. This rapid explosion of data and the need to ensure the data is available at the right time has led to increased adoption of artificial intelligence (AI) and machine learning solutions across healthcare organizations to gain meaningful insights from the data collected. These technologies are proving to transform many aspects of healthcare ecosystem from patient care to administrative functions. Unique contribution to theory, policy, and practice: Currently AI and machine learning are aiding providers and patients by improving the health outcomes, but further research   is necessary to validate to ensure these technologies are complying the regulatory guidelines without comprising on the patient care and the ethics involved when it comes to patient security and privacy. ","https://carijournals.org/journals/index.php/IJHS/article/download/1949/2327",""
0,"Sundaraparipurnan Narayanan","Developing Responsible AI Business Model",2023,"CSR, Sustainability, Ethics &amp; Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-09245-9_10","",288,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09245-9_10","2196-7075","",,,205,217,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-3-031-09245-9_10",""
0,"Eline S. Rentier","To use or not to use: exploring the ethical implications of using generative AI in academic writing",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00649-6","",289,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00649-6","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: The rapid emergence of open-access generative artificial intelligence (GenAI) has sparked a heated debate with polarising views on whether or not it is ethical to use GenAI for academic writing. In this commentary, I explore the ethical implications of using GenAI in academic writing. I provide an overview of how GenAI works as a probabilistic model and what the limitations are, such as generating incorrect or entirely made-up information. I raise concerns about bias, misinformation, plagiarism, inequity of access and monetisation of knowledge and discuss how this implicates ethical values regarding plagiarism and research integrity. Despite these issues, I also acknowledge the potential of GenAI to advance academic writing by aiding in tasks like structuring text and summarising large bodies of information. I emphasise the need to be transparent about the use of GenAI and call for an open discussion on this contentious, yet important, topic.","https://link.springer.com/content/pdf/10.1007/s43681-024-00649-6.pdf",""
0,"Joseph Breeden","Scoring AI-generated policy recommendations with Risk-Adjusted Gain in Net Present Happiness",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00355-9","",291,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00355-9","2730-5953","",4,4,1201,1211,0,0.00,0,1,2,"Abstract: Ethical considerations for assessing the collective benefit of an AI’s policy recommendations are different from assessing the ethical consequences from interacting with an individual. The study of population ethics provides a framework for studying collective benefit or harm in abstract terms. Research into happiness has made significant strides in identifying some key drivers of subjective well-being as measured both individually and collectively across societies. This research examines models from population ethics and statistical studies of subjective well-being to create a measure of benefit with which to judge AI recommendations. These models include refining estimations of the interaction between cultural aspects and economic development and incorporating measures of inequality of happiness and satisfaction through a society. When the impacts of a proposed policy are simulated for multiple successive years, risk discounting is used to measure Net Present Happiness, thus solving the conundrum of considering future generations in ethical considerations as posed in population ethics. Lastly, the Risk-Adjusted Gain in Net Present Happiness is proposed as a reasonable approach to ranking AI policy recommendations and as an AI objective function.","https://link.springer.com/content/pdf/10.1007/s43681-023-00355-9.pdf",""
0,"Johannes Thumfart","Correction: The democratic offset: Contestation, deliberation, and participation regarding military applications of AI",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00345-x","",293,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00345-x","2730-5953","",4,2,527,527,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00345-x.pdf",""
0,"Frank Dietrich","Correction: AI-based removal of hate speech from digital social networks: chances and risks for freedom of expression",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00645-w","",296,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00645-w","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00645-w.pdf",""
0,"Ahmed Banafa","Ethics in AI",2024,"Transformative AI","River Publishers","https://doi.org/10.1201/9781032669182-12","",298,"2025-02-04 16:55:17","book-chapter","10.1201/9781032669182-12","","",,,67,70,0,0.00,0,1,1,"","",""
0,"Tania Duarte, Nicholas Barrow, Medina Bakayeva, Peter Smith","Editorial: The ethical implications of AI hype",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00539-x","",299,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00539-x","2730-5953","",4,3,649,651,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00539-x.pdf",""
0,"Sara Patuzzo, Maurizio Balistreri, Tommaso Marinelli, Simone Giacopuzzi","Is a robot surgeon with AI the ideal surgeon? A philosophical analysis",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00361-x","",300,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00361-x","2730-5953","",,,,,0,0.00,0,4,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00361-x.pdf",""
0,"Donal Khosrowi, Finola Finn, Elinor Clark","Engaging the many-hands problem of generative-AI outputs: a framework for attributing credit",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00440-7","",303,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00440-7","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: The recent wave of generative AI (GenAI) systems like Stable Diffusion or ChatGPT that can produce images, text and code from human prompts raises controversial issues about creatorship, originality, creativity and copyright. This paper focuses on creatorship: who creates and should be credited with the outputs made with the help of GenAI? There is currently significant moral, legal and regulatory uncertainty around these questions. We develop a novel framework, called CCC (collective-centered creation), that helps resolve this uncertainty. According to CCC, GenAI outputs are created by collectives in the first instance. Claims to creatorship come in degrees and depend on the nature and significance of individual contributions made by the various agents and entities involved, including users, GenAI systems, developers, producers of training data and others. We demonstrate how CCC can help navigate a range of ongoing controversies around the responsible development and deployment of GenAI technologies and help more accurately attribute credit where it is due.","https://link.springer.com/content/pdf/10.1007/s43681-024-00440-7.pdf",""
0,"Edmund Balogun, Dion Dcosta, Auxane Boch, Christoph Luetge","Exploring key stakeholders’ perspectives on integrating the EU AI Act with the MDR for certifying AI medical devices",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00612-5","",308,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00612-5","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: Artificial Intelligence (AI) algorithms are transforming healthcare by advancing the capabilities of medical devices. These algorithms can now analyze X-ray images to help detect and interpret medical conditions, particularly in radiology and pathology. The growing use of AI in medicine highlights the importance of strict regulations to prioritize patient safety during the development and deployment of AI in medical devices. Despite extensive research, there are limited empirical studies on stakeholders’ perspectives regarding the challenges of implementing AI regulatory frameworks alongside existing legally binding regulations specific to medical devices. This study uses semi-structured interviews to explore the perspectives of key stakeholders, such as regulatory bodies and medical device manufacturers, to understand the potential implications of integrating the European Union Artificial Intelligence Act with the Medical Device Regulation to certify AI-embedded medical devices. Through inductive-thematic analysis and adopting activity theory to further synthesize the interviews, the findings reveal stakeholders’ challenges, including uncertainty about implementing different regulations, resource limitations, and the potential impact of complex regulations on healthcare quality. Additionally, the results indicate that key stakeholders did not explicitly identify the absence of a comprehensive ethical framework for AI-enabled medical devices in both regulations as a major challenge. This underscores the need to develop a harmonized ethical framework for certifying AI used in healthcare.","https://link.springer.com/content/pdf/10.1007/s43681-024-00612-5.pdf",""
0,"Uma E. Sarkar","Evaluating alignment in large language models: a review of methodologies",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00637-w","",312,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00637-w","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00637-w.pdf",""
0,"Magali Goirand, Elizabeth Austin, Robyn Clay-Williams","Bringing clarity and transparency to the consultative process underpinning the implementation of an ethics framework for AI-based healthcare applications: a qualitative study",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00466-x","",313,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00466-x","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Artificial intelligence (AI) has been applied in healthcare to address various aspects of the COVID-19 crisis including early detection, diagnosis and treatment, and population monitoring. Despite the urgency to develop AI solutions for COVID-19 problems, considering the ethical implications of those solutions remains critical. Implementing ethics frameworks in AI-based healthcare applications is a wicked issue that calls for an inclusive, and transparent participatory process. In this qualitative study, we set up a participatory process to explore assumptions and expectations about ethical issues associated with development of a COVID-19 monitoring AI-based app from a diverse group of stakeholders including patients, physicians, and technology developers. We also sought to understand the influence the consultative process had on the participants’ understanding of the issues. Eighteen participants were presented with a fictitious AI-based app whose features included individual self-monitoring of potential infection, physicians’ remote monitoring of symptoms for patients diagnosed with COVID-19 and tracking of infection clusters by health agencies. We found that implementing an ethics framework is systemic by nature, and that ethics principles and stakeholders need to be considered in relation to one another. We also found that the AI app introduced a novel channel for knowledge between the stakeholders. Mapping the flow of knowledge has the potential to illuminate ethical issues in a holistic way.","https://link.springer.com/content/pdf/10.1007/s43681-024-00466-x.pdf",""
0,"Ricardo Trainotti Rabonato, Lilian Berton","A systematic review of fairness in machine learning",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00577-5","",314,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00577-5","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00577-5.pdf",""
0,"Tersur Melchizedek Akpan","Transhumanist technologies as enhancers of human nature and its dignity",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00559-7","",315,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00559-7","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00559-7.pdf",""
0,"Maria Pawelec","Decent deepfakes? Professional deepfake developers’ ethical considerations and their governance potential",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00542-2","",316,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00542-2","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Policymakers and societies are grappling with the question of how to respond to deepfakes, i.e., synthetic audio-visual media which is proliferating in all areas of digital life– from politics to pornography. However, debates and research on deepfakes’ impact and governance largely neglect the technology’s sources, namely the developers of the underlying artificial intelligence (AI), and those who provide code or deepfake creation services to others, making the technology widely accessible. These actors include open-source developers, professionals working in large technology companies and specialized start-ups, and for deepfake apps. They can profoundly impact which underlying AI technologies are developed, whether and how they are made public, and what kind of deepfakes can be created. Therefore, this paper explores which values guide professional deepfake development, how economic and academic pressures and incentives influence developers’ (perception of) agency and ethical views, and how these views do and could impact deepfake design, creation, and dissemination. Thereby, the paper focuses on values derived from debates on AI ethics and on deepfakes’ impact. It is based on ten qualitative in-depth expert interviews with academic and commercial deepfake developers and ethics representatives of synthetic media companies. The paper contributes to a more nuanced understanding of AI ethics in relation to audio-visual generative AI. Besides, it empirically informs and enriches the deepfake governance debate by incorporating developers’ voices and highlighting governance measures which directly address deepfake developers and providers and emphasize the potential of ethics to curb the dangers of deepfakes.","https://link.springer.com/content/pdf/10.1007/s43681-024-00542-2.pdf",""
0,"Paschal Mmesoma Ukpaka","The creative agency of large language models: a philosophical inquiry",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00557-9","",320,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00557-9","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: This paper explores the difficult question of whether Large Language Models (LLMs) are intrinsically creative. Because they can independently create original content, LLMs are often seen as creative agents. Contrary to the belief that LLMs are creative, this paper argues that LLMs are not creative for two reasons. First, LLMs are not creative because they lack an essential component of creativity, which is the first-person experience of the world. Secondly, LLMs are not creative because they are not the principal authors of their creative output, for they lack the subjective awareness and intentionality necessary to be regarded as authors, and their output is a collaborative effort of the AI model, data providers, and other stakeholders. Since they are not full-fledged authors in a traditional sense, they are not creative.","https://link.springer.com/content/pdf/10.1007/s43681-024-00557-9.pdf",""
0,"Jakob Mainz, Lauritz Munch, Jens Christian Bjerring","Cost-effectiveness and algorithmic decision-making",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00528-0","",321,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00528-0","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: We argue that there are cases in which it is morally permissible to replace medical practitioners with machine learning algorithms. Our argument appeals to the uncontroversial view that it is sometimes morally permissible to make medical decisions based on cost-effectiveness considerations. For example, it is generally morally permissible to prescribe a treatment that is as effective as its alternatives but much cheaper. If this is so, we argue, then similar cost-effectiveness considerations can explain why it is sometimes morally permissible to replace human practitioners with algorithms. To reject our argument, one needs to show that when it comes to algorithmic medical decision-making, there are special considerations that would always override cost-effectiveness considerations. We go through a range of candidate considerations and argue that none of them is weighty enough to tip the balance in favor of human practitioners.","https://link.springer.com/content/pdf/10.1007/s43681-024-00528-0.pdf",""
0,"Nicholas Kluge Corrêa, James William Santos, Camila Galvão, Marcelo Pasetti, Dieine Schiavon, Faizah Naqvi, Robayet Hossain, Nythamar De Oliveira","Crossing the principle–practice gap in AI ethics with ethical problem-solving",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00469-8","",322,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00469-8","2730-5953","",,,,,0,0.00,0,8,1,"Abstract: The past years have presented a surge in (AI) development, fueled by breakthroughs in deep learning, increased computational power, and substantial investments in the field. Given the generative capabilities of more recent AI systems, the era of large-scale AI models has transformed various domains that intersect our daily lives. However, this progress raises concerns about the balance between technological advancement, ethical considerations, safety measures, and financial interests. Moreover, using such systems in sensitive areas amplifies our general ethical awareness, prompting a re-emergence of debates on governance, regulation, and human values. However, amidst this landscape, how to bridge the principle–practice gap separating ethical discourse from the technical side of AI development remains an open problem. In response to this challenge, the present work proposes a framework to help shorten this gap:","https://link.springer.com/content/pdf/10.1007/s43681-024-00469-8.pdf",""
0,"Antonio Araújo","Anthropomorphic sex robots across the genitalia-computer interface: AI-generated lover persona, infopower feminist bioethics, and Alexa-style humanity",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00584-6","",325,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00584-6","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00584-6.pdf",""
0,"","Chapter 6: Ethics and Bias in AI",2024,"AI Revealed","De Gruyter","https://doi.org/10.1515/9781501520679-007","",329,"2025-02-04 16:55:17","book-chapter","10.1515/9781501520679-007","","",,,121,130,0,0.00,0,0,1,"","https://www.degruyter.com/document/doi/10.1515/9781501520679-007/xml",""
0,"","Chapter 6: Ethics and Bias in AI",2024,"AI Revealed","De Gruyter","https://doi.org/10.1515/9781501520631-007","",330,"2025-02-04 16:55:17","book-chapter","10.1515/9781501520631-007","","",,,121,130,0,0.00,0,0,1,"","https://www.degruyter.com/document/doi/10.1515/9781501520631-007/xml",""
0,"Federico Benitez, Cyriel Pennartz, Walter Senn","Publisher Correction: The conductor model of consciousness, our neuromorphic twins, and the human-AI deal",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00598-0","",333,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00598-0","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00598-0.pdf",""
0,"Orlando Gomes","I, Robot: the three laws of robotics and the ethics of the peopleless economy",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00263-y","",335,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00263-y","2730-5953","",4,2,257,272,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00263-y.pdf",""
0,"Alexander Vocelka","AI Governance for a Prosperous Future",2023,"CSR, Sustainability, Ethics &amp; Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-09245-9_3","",336,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09245-9_3","2196-7075","",,,17,90,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-3-031-09245-9_3",""
0,"Paula Sweeney","Two disrupters to arguments from analogy for robot rights: uniqueness and completeness",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00614-3","",337,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00614-3","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: When arguing for rights for robots, many theorists draw analogies between robots, on the one hand, and humans and other animals, on the other. They use these analogies to support the extension of rights beyond humans and other animals to robots. In this paper I argue that, even if we are initially inclined to see existing analogies between robots and humans and other animals as reasons for granting robots moral consideration, there are two rarely noted significant features that threaten to disrupt these analogies: uniqueness and completeness. I argue that those potential disrupters have a negative impact on many arguments from analogy regarding robot rights.","https://link.springer.com/content/pdf/10.1007/s43681-024-00614-3.pdf",""
0,"J. Rosenbaum","Gender Tapestry: gender classification as color assignation",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00456-z","",338,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00456-z","2730-5953","",4,4,889,900,0,0.00,0,1,1,"Abstract:","https://link.springer.com/content/pdf/10.1007/s43681-024-00456-z.pdf",""
0,"Sarah Wyer, Sue Black","Algorithmic bias: sexualized violence against women in GPT-3 models",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00641-0","",339,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00641-0","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: This study explores the occurrence and implications of sexualized violence against women in text completion tasks performed by GPT-3 models. The study began as an exploratory investigation into gender inequalities within GPT-3 models to discover","https://link.springer.com/content/pdf/10.1007/s43681-024-00641-0.pdf",""
0,"Shervin MirzaeiGhazi, Jakob Stenseke","Responsibility before freedom: closing the responsibility gaps for autonomous machines",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00503-9","",343,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00503-9","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: The introduction of autonomous machines (AMs) in human domains has raised challenging questions about the attribution of responsibility; referred to as the","https://link.springer.com/content/pdf/10.1007/s43681-024-00503-9.pdf",""
0,"Ms. Mansi Shukla","ARTIFICIAL INTELLIGENCE AND COPYRIGHT LAW: NAVIGATING THE CHALLENGES OF OWNERSHIP, INFRINGEMENT, AND ETHICS IN THE AGE OF AI-GENERATED CONTENT",2024,"Disruptive Technologies and the Law: Navigating Legal Challenges in an Era of Innovation","Iterative International Publishers, Selfypage Developers Pvt Ltd","https://doi.org/10.58532/nbennurdtch13","",346,"2025-02-04 16:55:17","book-chapter","10.58532/nbennurdtch13","","",,,139,150,0,0.00,0,1,1,"The emergence of artificial intelligence (AI) technology has brought about significant changes in various facets of our existence, altering not only how content is generated but also how it is consumed. AI-generated content, such as music, art, and literature, has become increasingly prevalent, raising a range of legal and ethical challenges regarding copyright ownership, infringement, and ethics. This paper explores the interaction between AI and copyright law, examining the legal implications of AI-generated content and its impact on copyright ownership along with the ethical considerations of using AI to create content, and its implications on the copyright law. Additionally, the paper shall also examine role of AI in detecting and enforcing copyright infringement. The paper shall provide recommendations for adapting copyright laws to accommodate the challenges posed by AI generated content. The discuss on the potential for new legal frameworks that could better accommodate AI-generated content, including the possibility of recognizing AI as a legal author is also made while considering the potential for alternative licensing arrangements that could better balance the interests of creators, users, and the public. Overall, this paper aims to provide insights into the legal and ethical complexities arising from the interaction between AI and copyright law, and to encourage further discussion and research in this important area while discussing relevant case studies like Raghav, where a work by an AI system was first given the copyright and then revoked. The challenges posed by AI-generated content are significant, but with careful consideration and collaboration, a legal and ethical framework can be developed that enable us to harness the potential of AI while protecting the interests of creators, users, and the public","",""
0,"Nitish Kumar Minz","Ethical Considerations in AI Applications in Finance",2024,"Advances in Finance, Accounting, and Economics","IGI Global","https://doi.org/10.4018/979-8-3693-2185-0.ch012","",347,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-2185-0.ch012","2327-5677","",,,277,290,0,0.00,0,1,1,"This research addresses ethical considerations in AI applications within finance. With AI's increasing integration into financial operations, the need for robust ethical guidelines is evident. The proposal explores tailored ethical frameworks for AI in finance, emphasizing transparency, explainability, accountability, and real-world case studies. It adapts established ethical frameworks, providing practical guidance for stakeholders. The research underscores the significance of transparency and explainability, exploring methods for ensuring these attributes in financial AI applications. It delves into accountability and responsibility concepts, clarifying stakeholder and regulatory roles. Real-world case studies offer insights into ethical dilemmas and resolutions. The goal is to heighten awareness and understanding, enabling the development of responsible, ethical, and sustainable AI applications in finance.","https://www.igi-global.com/viewtitle.aspx?TitleId=352620",""
0,"Sanjana Kumari, Amisha Kumari, Rabia Asim, Rayyan Khan","Multi-faceted role of artificial intelligence (AI) in cardiopulmonary resuscitation (CPR): a narrative review",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00634-z","",349,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00634-z","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00634-z.pdf",""
0,"Izaak Dekker, Bert Bredeweg, Wilco te Winkel, Ibo van de Poel","Ethical procedures for responsible experimental evaluation of AI-based education interventions",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00621-4","",350,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00621-4","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00621-4.pdf",""
0,"Anna Puzio","The entangled human being – a new materialist approach to anthropology of technology",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00537-z","",352,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00537-z","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Technological advancements raise anthropological questions: How do humans differ from technology? Which human capabilities are unique? Is it possible for robots to exhibit consciousness or intelligence, capacities once taken to be exclusively human? Despite the evident need for an anthropological lens in both societal and research contexts, the philosophical anthropology of technology has not been established as a set discipline with a defined set of theories, especially concerning emerging technologies. In this paper, I will utilize a New Materialist approach, focusing particularly on the theories of Donna Haraway and Karen Barad, to explore their potential for an anthropology of technology. I aim to develop a techno-anthropological approach that is informed and enriched by New Materialism. This approach is characterized by its relational perspective, a dynamic and open conception of the human being, attention to diversity and the dynamics of power in knowledge production and ontology, and an emphasis on the non-human. I aim to outline an anthropology of technology centered on New Materialism, wherein the focus, paradoxically, is not exclusively on humans but equally on non-human entities and the entanglement with the non-human. As will become clear, the way we understand humans and their relationship with technology is fundamental for our concepts and theories in ethics of technology.","https://link.springer.com/content/pdf/10.1007/s43681-024-00537-z.pdf",""
0,"Yujing Lyu, Yanyong Du","The ethical evaluation of large language models and its optimization",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00654-9","",354,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00654-9","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00654-9.pdf",""
0,"Florian Richter","AI Ethics as a Form of Research Ethics",2023,"What AI Can Do","Chapman and Hall/CRC","https://doi.org/10.1201/b23345-2","",355,"2025-02-04 16:55:17","book-chapter","10.1201/b23345-2","","",,,5,22,0,0.00,0,1,2,"","",""
0,"Frej Klem Thomsen","Algorithmic indirect discrimination, fairness and harm",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00326-0","",356,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00326-0","2730-5953","",4,4,1023,1037,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00326-0.pdf",""
0,"Olya Kudina, Bas de Boer","Large language models, politics, and the functionalization of language",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00564-w","",357,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00564-w","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: This paper critically examines the political implications of Large Language Models (LLMs), focusing on the individual and collective ability to engage in political practices. The advent of AI-based chatbots powered by LLMs has sparked debates on their democratic implications. These debates typically focus on how LLMS spread misinformation and thus hinder the evaluative skills of people essential for informed decision-making and deliberation. This paper suggests that beyond the spread of misinformation, the political significance of LLMs extends to the core of political subjectivity and action. It explores how LLMs contribute to political de-skilling by influencing the capacities of critical engagement and collective action. Put differently, we explore how LLMs shape political subjectivity. We draw from Arendt’s distinction between speech and language and Foucault’s work on counter-conduct to articulate in what sense LLMs give rise to political de-skilling, and hence pose a threat to political subjectivity. The paper concludes by considering how to reconcile the impact of LLMs on political agency without succumbing to technological determinism, and by pointing to how the practice of parrhesia enables one to form one’s political subjectivity in relation to LLMs.","https://link.springer.com/content/pdf/10.1007/s43681-024-00564-w.pdf",""
0,"Christoph Lütge","Conclusion to The Elgar Companion to Applied AI Ethics",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00024","",358,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00024","","",,,409,409,0,0.00,0,1,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Hamed Taherdoost","AI Ethics in IT",2025,"Information Technology Ethics","CRC Press","https://doi.org/10.1201/9781003584452-13","",360,"2025-02-04 16:55:17","book-chapter","10.1201/9781003584452-13","","",,,199,215,0,0.00,0,1,1,"","",""
0,"Franziska Poszler, Edy Portmann, Christoph Lütge","Formalizing ethical principles within AI systems: experts’ opinions on why (not) and how to do it",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00425-6","",361,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00425-6","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: AI systems are increasingly put into contexts where computed decisions must be guided by ethical considerations. To develop ethically grounded algorithms and technologies, scholars have suggested computational ethics as an essential frontier, which aims to translate ethical principles into computer code. However, computational ethics has received little attention in academic literature so far, with existing work mainly focusing on its technical implementation, while many open questions concerning its (societal and ethical) implications still need to be resolved. Therefore, in this study, we interviewed 12 experts from philosophy, AI and cognitive sciences to shed light on computational ethics beyond a technical perspective. Findings suggest that indicated supporting and opposing arguments can be clustered into pragmatic/practical, societal and epistemic reasons, all of which need to be contemplated when engaging in computational ethics and developing resulting artificial moral agents. Furthermore, the mentioned recommendations for companies’ technological design and development, for industry’s governance measures and academia’s research endeavors are recapitulated and summarized in a holistic framework that aims to facilitate a reflected implementation of ‘ethics in and by design’ in the future.","https://link.springer.com/content/pdf/10.1007/s43681-024-00425-6.pdf",""
0,"Nicholas Kluge Corrêa, James William Santos, Camila Galvão, Marcelo Pasetti, Dieine Schiavon, Faizah Naqvi, Robayet Hossain, Nythamar De Oliveira","Correction: Crossing the principle–practice gap in AI ethics with ethical problem-solving",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00487-6","",362,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00487-6","2730-5953","",,,,,0,0.00,0,8,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00487-6.pdf",""
0,"Thilo Hagendorff, Leonie Bossert, Tse Yip Fai, Peter Singer","Speciesist bias in AI: a reply to Arandjelović",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00319-z","",364,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00319-z","2730-5953","",3,4,1043,1047,0,0.00,0,4,2,"Abstract: The elimination of biases in artificial intelligence (AI) applications—for example biases based on race or gender—is a high priority in AI ethics. So far, however, efforts to eliminate bias have all been anthropocentric. Biases against nonhuman animals have not been considered, despite the influence AI systems can have on normalizing, increasing, or reducing the violence that is inflicted on animals, especially on farmed animals. Hence, in 2022, we published a paper in","https://link.springer.com/content/pdf/10.1007/s43681-023-00319-z.pdf",""
0,"Anna Schmitz, Michael Mock, Rebekka Görge, Armin B. Cremers, Maximilian Poretschkin","A global scale comparison of risk aggregation in AI assessment frameworks",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00479-6","",366,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00479-6","2730-5953","",,,,,0,0.00,0,5,1,"Abstract: AI applications bear inherent risks in various risk dimensions, such as insufficient reliability, robustness, fairness or data protection. It is well-known that trade-offs between these dimensions can arise, for example, a highly accurate AI application may reflect unfairness and bias of the real-world data, or may provide hard-to-explain outcomes because of its internal complexity. AI risk assessment frameworks aim to provide systematic approaches to risk assessment in various dimensions. The overall trustworthiness assessment is then generated by some form of risk aggregation among the risk dimensions. This paper provides a systematic overview on risk aggregation schemes used in existing AI risk assessment frameworks, focusing on the question how potential trade-offs among the risk dimensions are incorporated. To this end, we examine how the general risk notion, the application context, the extent of risk quantification, and specific instructions for evaluation may influence overall risk aggregation. We discuss our findings in the current frameworks in terms of whether they provide meaningful and practicable guidance. Lastly, we derive recommendations for the further operationalization of risk aggregation both from horizontal and vertical perspectives.","https://link.springer.com/content/pdf/10.1007/s43681-024-00479-6.pdf",""
0,"Giorgia Pozzi, Michiel De Proost","Keeping an AI on the mental health of vulnerable populations: reflections on the potential for participatory injustice",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00523-5","",368,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00523-5","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: Considering the overall shortage of therapists to meet the psychological needs of vulnerable populations, AI-based technologies are often seen as a possible remedy. Particularly smartphone apps or chatbots are increasingly used to offer mental health support, mostly through cognitive behavioral therapy. The assumption underlying the deployment of these systems is their ability to make mental health support accessible to generally underserved populations. Hence, this seems to be aligned with the fundamental biomedical principle of justice understood in its","https://link.springer.com/content/pdf/10.1007/s43681-024-00523-5.pdf",""
0,"Ahmed Banafa","Ethics in AI",2024,"Introduction to Artificial Intelligence (AI)","River Publishers","https://doi.org/10.1201/9781003499527-11","",374,"2025-02-04 16:55:17","book-chapter","10.1201/9781003499527-11","","",,,61,64,0,0.00,0,1,1,"","",""
0,"Florian Richter","From human-system interaction to human-system co-action and back: ethical assessment of generative AI and mutual theory of mind",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00626-z","",375,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00626-z","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Human-machine ethics has emerged as a rapidly growing research field in recent years. However, it seems that Generative Artificial Intelligence (AI) leads to a paradigm shift from human-machine interaction to co-action. The ethical assessment of such relationships is still in the making and needs further scrutiny. First, studies about the influence of technology in human-system interactions and manipulation are reviewed. Second, the “mutual theory of mind” approach is critically examined to identify its shortcomings. Third, creating user models is reconstruced to demonstrate the strategies of systems. Finally, use cases are discussed and assessed to outline ethical implications.","https://link.springer.com/content/pdf/10.1007/s43681-024-00626-z.pdf",""
0,"Frederic Gilbert, Ingrid Russo","Mind-reading in AI and neurotechnology: evaluating claims, hype, and ethical implications for neurorights",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00514-6","",376,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00514-6","2730-5953","",4,3,855,872,0,0.00,0,2,1,"Abstract: This paper examines claims that the convergence of AI and neurotechnology applications, known as brain-reading, enables the reading of human minds. The purpose of this examination is to investigate whether the use of the terms “brain-reading” and “mind-reading” to convey current neurotechnological findings carries evidence of hype. We conducted an interpretive content analysis of 1017 academic articles to gain insights into the current state of the art and examine assertions made by academics. Our analysis revealed that up to 91% of the examined articles suggest the possibility of mind-reading through brain-reading. Ethical issues discussed frequently include mental privacy, mental freedom, and personhood. Our study highlights the imprecise and inconsistent usage of the term mind-reading in scientific discourse, which leads to exaggerated claims about AI and BCIs having already achieved capacities beyond their current capabilities—or even reaching capacities that may never be feasible. While our study provides evidence of AI and BCI hype concerning alleged mind-reading capabilities, it also uncovers a hype in AI ethics, specifically pertaining to neurorights. This involves hypothetical scenarios where the fictional prospect of AI-enabled mind-reading calls for the establishment of new protective human rights.","https://link.springer.com/content/pdf/10.1007/s43681-024-00514-6.pdf",""
0,"Hyoung-bin Park","AMA Challenges in AI Ethics and Neuroscience - Considerations for Implementing Moral Judgment Algorithm -",2022,"Journal of Ethics Education Studies","The Korean Ethics Education Association","https://doi.org/10.18850/jees.2022.64.04","",379,"2025-02-04 16:55:17","journal-article","10.18850/jees.2022.64.04","1738-0545","",64,,91,114,0,0.00,0,1,3,"","",""
0,"Sophina Luitel, Yang Liu, Mohd Anwar","Investigating fairness in machine learning-based audio sentiment analysis",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00453-2","",382,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00453-2","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: Audio sentiment analysis is a growing area of research, however little attention has been paid to the fairness of machine learning models in this field. Whilst the current literature covers research on machine learning models’ reliability and fairness in various demographic groups, fairness in audio sentiment analysis with respect to gender is still an uninvestigated field. To fill this knowledge gap, we conducted experiments aimed at assessing the fairness of machine learning algorithms concerning gender within the context of audio sentiment analysis. In this research, we used 442 audio files of happiness and sadness—representing equal samples of male and female subjects—and generated spectrograms for each file. Then we performed feature extraction using bag-of-visual-words method followed by building classifiers using Random Forest, Support Vector Machines, and K-nearest Neighbors algorithms. We investigated whether the machine learning models for audio sentiment analysis are fair across female and male genders. We found the need for gender-specific models for audio sentiment analysis instead of a gender-agnostic-model. Our results provided three pieces of evidence to back up our claim that gender-specific models demonstrate bias in terms of overall accuracy equality when tested using audio samples representing the other gender, as well as combination of both genders. Furthermore, gender-agnostic-model performs poorly in comparison to gender-specific models in classifying sentiments of both male and female audio samples. These findings emphasize the importance of employing an appropriate gender-specific model for an audio sentiment analysis task to ensure fairness and accuracy. The best performance is achieved when using a female-model (78% accuracy) and a male-model (74% accuracy), significantly outperforming the 66% accuracy of the gender-agnostic model.","https://link.springer.com/content/pdf/10.1007/s43681-024-00453-2.pdf",""
0,"Shmona Simpson, Jonathan Nukpezah, Kie Brooks, Raaghav Pandya","Parity benchmark for measuring bias in LLMs",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00613-4","",383,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00613-4","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00613-4.pdf",""
0,"Dileesh Chandra Bikkasani","Navigating artificial general intelligence (AGI): societal implications, ethical considerations, and governance strategies",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00642-z","",385,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00642-z","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00642-z.pdf",""
0,"Peter Smith, Laura Smith","Editorial piece: Technology built on sand?",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00247-4","",390,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00247-4","2730-5953","",3,3,661,662,0,0.00,0,2,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00247-4.pdf",""
0,"Dorsaf Sallami, Esma Aïmeur","Fairframe: a fairness framework for bias detection and mitigation in news",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00568-6","",394,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00568-6","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00568-6.pdf",""
0,"John Bartucz","The process is the product: a review of Co-Intelligence by Ethan Mollick",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00633-0","",395,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00633-0","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00633-0.pdf",""
0,"Dennis Schuessler","The probability problems of the Moral Machine Experiment",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00287-4","",397,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00287-4","2730-5953","",4,2,501,510,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00287-4.pdf",""
0,"Prokopis A. Christou","A critical inquiry into the personal and societal perils of Artificial Intelligence",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00556-w","",398,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00556-w","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00556-w.pdf",""
0,"Verus Cronery Rwetembula","Legal and Practical Challenges for the Admissibility of Artificial Intelligence (AI) Evidence in Criminal Proceedings in Mainland Tanzania",2024,"East African Journal of Law and Ethics","East African Nature and Science Organization","https://doi.org/10.37284/eajle.7.1.2431","",400,"2025-02-04 16:55:17","journal-article","10.37284/eajle.7.1.2431","2707-5338","",7,1,136,148,0,0.00,0,1,1,"This article investigates the admissibility of artificial intelligence (AI) evidence in criminal proceedings within Mainland Tanzania. As AI technologies increasingly generate data that could be used in legal contexts, questions arise regarding the reliability, transparency, and potential biases inherent in AI-based evidence. The Tanzanian legal framework, including the Evidence Act and the Electronic Transactions Act, lacks explicit provisions for AI-generated evidence, which creates challenges for its integration into criminal cases. This paper explores current admissibility standards in Tanzania, analyzing how AI evidence could be evaluated for relevance and probative value under existing laws. By examining principles such as legal positivism and reliability theory, and drawing on international insights, the article proposes interpretative approaches to assess AI evidence’s validity and reliability in Tanzanian courts. Ultimately, this study seeks to provide insights and recommendations for Tanzanian legal professionals and policymakers, aiming to support the development of clear guidelines for the use of AI in the criminal justice system and ensure that technological advancements uphold procedural fairness and justice","https://journals.eanso.org/index.php/eajle/article/download/2431/3098",""
0,"Siraj Kariyilaparambu Kunjumuhammed, Hisham Madi, Mahmoud Abouraia","Risks and Challenges of AI-Driven Finance",2024,"Advances in Finance, Accounting, and Economics","IGI Global","https://doi.org/10.4018/979-8-3693-2185-0","",401,"2025-02-04 16:55:17","edited-book","10.4018/979-8-3693-2185-0","2327-5677","",,,,,0,0.00,0,3,1,"","",""
0,"Marc Jungtäubl, Christopher Zirnig, Caroline Ruiner","HCI driving alienation: autonomy and involvement as blind spots in digital ethics",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00298-1","",402,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00298-1","2730-5953","",4,2,617,634,0,0.00,0,3,2,"Abstract: The ongoing development and adoption of digital technologies such as AI in business brings ethical concerns and challenges. Main topics are the design of digital technologies, their tasks, and competencies in organizational practice, and their collaboration with humans. Previous guidelines on digital ethics mainly consider technological aspects such as the nondiscriminatory design of AI, its transparency, and technically constrained (distributed) agency as priorities in AI systems, leaving the consideration of the human factor and the implementation of ethical guidelines in organizational practice unclear. We analyze the relationship between human–computer interaction (HCI), autonomy, and worker involvement with its impact on the experience of alienation at work for workers. We argue that the consideration of autonomy and worker involvement is crucial for HCI. Based on a quantitative empirical study of 1989 workers in Germany, the analysis shows that when worker involvement is high, the effect of HCI use on alienation decreases. The study results contribute to the understanding of the use of digital technologies with regard to worker involvement, reveal a blind spot in widespread ethical debates about AI, and have practical implications with regard to digital ethics in organizational practice.","https://link.springer.com/content/pdf/10.1007/s43681-023-00298-1.pdf",""
0,"David Hartmann, José Renato Laranjeira de Pereira, Chiara Streitbörger, Bettina Berendt","Addressing the regulatory gap: moving towards an EU AI audit ecosystem beyond the AI Act by including civil society",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00595-3","",403,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00595-3","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: The European legislature has proposed the Digital Services Act (DSA) and Artificial Intelligence Act (AIA) to regulate platforms and Artificial Intelligence (AI) products. We review to what extent third-party audits are part of both laws and how is access to information on models and the data provided. By considering the value of third-party audits and third-party data access in an audit ecosystem, we identify a regulatory gap in that the AIA does not provide access to data for researchers and civil society. Our contributions to the literature include: (1) Defining an AI audit ecosystem incorporating compliance and oversight. (2) Highlighting a regulatory gap within the DSA and AIA regulatory framework, preventing the establishment of an AI audit ecosystem that has effective oversight by civil society and academia. (3) Emphasizing that third-party audits by research and civil society must be part of that ecosystem, we call for AIA amendments and delegated acts to include data and model access for certain AI products. Furthermore, we call for the DSA to provide NGOs and investigative journalists with data access to platforms by delegated acts and for adaptions and amendments of the AIA to provide third-party audits and data and model access, at least for high-risk systems. Regulations modeled after EU AI regulations should enable data access and third-party audits, fostering an AI audit ecosystem that promotes compliance and oversight mechanisms.","https://link.springer.com/content/pdf/10.1007/s43681-024-00595-3.pdf",""
0,"Alesia Zhuk","Correction: Navigating the legal landscape of AI copyright: a comparative analysis of EU, US, and Chinese approaches",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00365-7","",404,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00365-7","2730-5953","",4,4,1307,1307,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00365-7.pdf",""
0,"Laura Y. Cabrera, Jennifer Wagner, Sara Gerke, Daniel Susser","Tempered enthusiasm by interviewed experts for synthetic data and ELSI checklists for AI in medicine",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00652-x","",406,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00652-x","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00652-x.pdf",""
0,"Emily Hadley, Alan Blatecky, Megan Comfort","Investigating algorithm review boards for organizational responsible artificial intelligence governance",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00574-8","",411,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00574-8","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00574-8.pdf",""
0,"Corinne Jorgenson, Jurgen Willems, Ali I. Ozkes, Dieter Vanderelst","Humans and robots are nearly ethically equivalent",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00603-6","",412,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00603-6","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00603-6.pdf",""
0,"Alayt Abraham Issak","AI Ethics for Creativity",2025,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i2.31897","",413,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i2.31897","3065-8365","",7,2,18,20,0,0.00,0,1,1,"AI Ethics for Creativity is a study of lived experience under the overarching theme of creative expression through new technologies--the new technology being AI. In this context, I reference the past, situate the present, and speculate on the future. The nature of the dissertation is to ask ``humanities'' questions from the lens of Human-Computer Interaction, i.e. what is creativity, interaction, art, aesthetics, embodiment, and expression... with AI in the creative endeavor.","https://ojs.aaai.org/index.php/AIES/article/download/31897/34064",""
0,"Sietze Kai Kuilman, Luciano Cavalcante Siebert, Stefan Buijsman, Catholijn M. Jonker","How to gain control and influence algorithms: contesting AI to find relevant reasons",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00500-y","",414,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00500-y","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: Relevancy is a prevalent term in value alignment. We either need to keep track of the relevant moral reasons, we need to embed the relevant values, or we need to learn from the relevant behaviour. What relevancy entails in particular cases, however, is often ill-defined. The reasons for this are obvious, it is hard to define relevancy in a way that is both general and concrete enough to give direction towards a specific implementation. In this paper, we describe the inherent difficulty that comes along with defining what is relevant to a particular situation. Simply due to design and the way an AI system functions, we need to state or learn particular goals and circumstances under which that goal is completed. However, because of both the changing nature of the world and the varied wielders and users of such implements, misalignment occurs, especially after a longer amount of time. We propose a way to counteract this by putting contestability front and centre throughout the lifecycle of an AI system, as it can provide insight into what is actually relevant at a particular instance. This allows designers to update the applications in such a manner that they can account for oversight during design.","https://link.springer.com/content/pdf/10.1007/s43681-024-00500-y.pdf",""
0,"William J. W. Choi","Is ChatGPT a good moral interlocutor for teaching bioethics? A case analysis",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00593-5","",416,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00593-5","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00593-5.pdf",""
0,"Tita Alissa Bach, Magnhild Kaarstad, Elizabeth Solberg, Aleksandar Babic","Insights into suggested Responsible AI (RAI) practices in real-world settings: a systematic literature review",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00648-7","",418,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00648-7","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00648-7.pdf",""
0,"Branka Panic, Paige Arthur","AI, Peace, and Ethics",2024,"AI for Peace","CRC Press","https://doi.org/10.1201/9781003359982-6","",419,"2025-02-04 16:55:17","book-chapter","10.1201/9781003359982-6","","",,,79,98,0,0.00,0,2,1,"","",""
0,"John Zerilli","Is AI Ethics All Fluff?",2024,"AI Morality","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198876434.003.0019","",422,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198876434.003.0019","","",,,195,207,0,0.00,0,1,1,"Abstract: The AI revolution provides a neat illustration of C.P. Snow’s point about the “two cultures” and a timely opportunity to reflect on why a cultural gap between the sciences and humanities persists. This chapter takes aim at an attitude prevailing among some computer scientists that ethics and AI ethics, as branches of the humanities, are unserious disciplines because they do not yield verifiable and quantifiable answers to the problems they address.","https://academic.oup.com/book/chapter-pdf/58409231/oso-9780198876434-chapter-19.pdf",""
0,"Zacharus Gudmunsen","The moral decision machine: a challenge for artificial moral agency based on moral deference",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00444-3","",423,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00444-3","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Humans are responsible moral agents in part because they can competently respond to moral reasons. Several philosophers have argued that artificial agents cannot do this and therefore cannot be responsible moral agents. I present a counterexample to these arguments: the ‘Moral Decision Machine’. I argue that the ‘Moral Decision Machine’ responds to moral reasons just as competently as humans do. However, I suggest that, while a hopeful development, this does not warrant strong optimism about ‘artificial moral agency’. The ‘Moral Decision Machine’ (and similar agents) can only respond to moral reasons by deferring to others, and there are good reasons to think this is incompatible with responsible moral agency. While the challenge to artificial moral agency based on moral reasons-responsiveness can be satisfactorily addressed; the challenge based on moral deference remains an open question. The right way to understand the challenge, I argue, is as a route to the claim that artificial agents are unlikely to be responsible moral agents because they cannot be authentic.","https://link.springer.com/content/pdf/10.1007/s43681-024-00444-3.pdf",""
0,"Christophe Gouguenheim, Ahmad Berjaoui","Neighborhood sampling confidence metric for object detection",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00395-1","",424,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00395-1","2730-5953","",4,1,57,64,0,0.00,0,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00395-1.pdf",""
0,"June Jeon, Lanu Kim, Jaehyuk Park","The Ethics of Generative Ai in Social Science Research: A Qualitative Approach for Community-Based Ai Research Ethics",2024,"","Elsevier BV","https://doi.org/10.2139/ssrn.4784555","",425,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.4784555","","",,,,,0,0.00,0,3,1,"","",""
0,"Louise McCormack, Malika Bendechache","A comprehensive survey and classification of evaluation criteria for trustworthy artificial intelligence",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00590-8","",426,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00590-8","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00590-8.pdf",""
0,"Masoud Toossi Saeidi","Emotionalized AI and the meaningfulness gap: an AI ethics perspective",2025,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-025-02179-z","",427,"2025-02-04 16:55:17","journal-article","10.1007/s00146-025-02179-z","0951-5666","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s00146-025-02179-z.pdf",""
0,"Wan Rosalili Wan Rosli","Waging warfare against states: the deployment of artificial intelligence in cyber espionage",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00628-x","",428,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00628-x","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: Cyber espionage has significantly been viewed as a risk towards nation-states, especially in the area of security and protection of Critical National Infrastructures. The race against digitisation has also raised concerns about how emerging technologies are defining how cyber activities are linked to waging warfare between States. Real-world crimes have since found a place in cyberspace, and with high connectivity, has exposed various actors to various risks and vulnerabilities, including cyber espionage. Cyber espionage has always been a national security issue as it does not only target States but also affects public–private networks, corporations and individuals. The challenge of crimes committed within the cyber realm is how the nature of cybercrimes distorts the dichotomy of state responsibility in responding to cyber threats and vulnerabilities. Furthermore, the veil of anonymity and emerging technologies such as artificial intelligence have further provided opportunities for a larger scale impact on the state for such crime. The imminent threat of cyber espionage is impacting the economic and political interactions between nation-states and changing the nature of modern conflict. Due to these implications, this paper will discuss the current legal landscape governing cyber espionage and the impact of the use of artificial intelligence in the commission of such crimes.","https://link.springer.com/content/pdf/10.1007/s43681-024-00628-x.pdf",""
0,"Yi Zeng, Enmeng Lu, Kang Sun","Principles on symbiosis for natural life and living artificial intelligence",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00364-8","",429,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00364-8","2730-5953","",,,,,0,0.00,0,3,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00364-8.pdf",""
0,"Riya Widayanti, Tatik Mariyanti","AI Dialog: Utilization, Challenges, and Ethics in the Age of Artificial Intelligence",2023,"International Transactions on Artificial Intelligence (ITALIC)","Pandawan Sejahtera Indonesia","https://doi.org/10.33050/italic.v2i1.401","",432,"2025-02-04 16:55:17","journal-article","10.33050/italic.v2i1.401","2963-1939","",2,1,40,48,0,0.00,0,2,2,"In this writing, we will delve into every aspect of the utilization, challenges, as well as ethical considerations in harnessing Artificial Intelligence (AI) Technology, with a focus on ChatGPT, an exceptional language model. AI technology has become an inseparable element in modern life, manifesting across various sectors such as business, healthcare, governance, and several others. The capabilities of AI are capable of driving data-driven decision-making, reducing the potential for human errors, and enhancing process efficiency. However, its impact is also not devoid of potential risks, including data security and the replacement of human roles. To ensure intelligent AI usage, it is imperative to uphold AI ethics, protect user information and privacy, strive to prevent discriminatory practices, and ensure system safety stability. The questions raised in this study revolve around the benefits and challenges of implementing ChatGPT, optimizing its use, and its role in the realm of education. The mission of this study is to unravel the benefits, challenges, as well as ethical considerations in the utilization of ChatGPT in the AI era, while ensuring responsible steps in its application.","https://journal.pandawan.id/italic/article/download/401/385",""
0,"Patrick E. McSharry","Promoting AI Ethics Through Awareness and Case Studies",2023,"SpringerBriefs in Ethics","Springer International Publishing","https://doi.org/10.1007/978-3-031-23035-6_6","",433,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-23035-6_6","2211-8101","",,,67,84,0,0.00,0,1,2,"Abstract: Artificial intelligence (AI) is enabling organizations to address a range of real-world challenges in areas as diverse as global health, education and poverty alleviation.","https://link.springer.com/content/pdf/10.1007/978-3-031-23035-6_6",""
0,"Madeline G. Reinecke, Andreas Kappes, Sebastian Porsdam Mann, Julian Savulescu, Brian D. Earp","The need for an empirical research program regarding human–AI relational norms",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00631-2","",434,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00631-2","2730-5953","",,,,,0,0.00,0,5,1,"Abstract: As artificial intelligence (AI) systems begin to take on social roles traditionally filled by humans, it will be crucial to understand how this affects people’s cooperative expectations. In the case of human–human dyads, different relationships are governed by different norms: For example, how two strangers—versus two friends or colleagues—should interact when faced with a similar coordination problem often differs. How will the rise of ‘social’ artificial intelligence (and ultimately, superintelligent AI) complicate people’s expectations about the cooperative norms that should govern different types of relationships, whether human–human or human–AI? Do people expect AI to adhere to the same cooperative dynamics as humans when in a given social role? Conversely, will they begin to expect humans in certain types of relationships to act more like AI? Here, we consider how people’s cooperative expectations may pull apart between human–human and human–AI relationships, detailing an empirical proposal for mapping these distinctions across relationship types. We see the data resulting from our proposal as relevant for understanding people’s relationship–specific cooperative expectations in an age of social AI, which may also forecast potential resistance towards AI systems occupying certain social roles. Finally, these data can form the basis for ethical evaluations: What relationship–specific cooperative norms we should adopt for human–AI interactions, or reinforce through responsible AI design, depends partly on empirical facts about what norms people find intuitive for such interactions (along with the costs and benefits of maintaining these). Toward the end of the paper, we discuss how these relational norms may change over time and consider the implications of this for the proposed research program.","https://link.springer.com/content/pdf/10.1007/s43681-024-00631-2.pdf",""
0,"Danielle Allen, Sarah Hubbard, Woojin Lim, Allison Stanger, Shlomit Wagman, Kinney Zalesne, Omoaholo Omoakhalen","A roadmap for governing AI: technology governance and power-sharing liberalism",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00635-y","",435,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00635-y","2730-5953","",,,,,0,0.00,0,7,1,"Abstract: This paper aims to provide a roadmap for governing AI. In contrast to the reigning paradigms, we argue that AI governance should be not merely a reactive, punitive, status-quo-defending enterprise, but rather the expression of an expansive, proactive vision for technology—to advance human flourishing. Advancing human flourishing in turn requires democratic/political stability and economic empowerment. To accomplish this, we build on a new normative framework that will give humanity its best chance to reap the full benefits, while avoiding the dangers, of AI. This new framework of “Power-Sharing Liberalism” is a philosophy that restores protections of positive liberties to liberalism. As we deploy it here, it helps shape a more comprehensive (and we would contend, more accurate) understanding of both risk and opportunity introduced by AI. To lay out how Power-Sharing Liberalism can be applied to AI governance, we take four steps. First, we define central concepts in the field of AI governance, disambiguating between forms of technological harms and risks. Second, we review current normative frameworks around the globe and argue that Power-Sharing Liberalism is a better fit for governing AI. Third, we walk through the six governance tasks that should be accomplished by any governance framework and analyze them through the Power-Sharing Liberalism framework. Based on that analysis, we make 17 recommendations for the governance of AI—including transformative investments in public goods, personnel, and the sustainability of democracy itself. Finally, we discuss concrete proposals for implementing those recommendations.","https://link.springer.com/content/pdf/10.1007/s43681-024-00635-y.pdf",""
0,"Salla Westerstrand","Reconstructing AI Ethics Principles: Rawlsian Ethics of Artificial Intelligence",2024,"Science and Engineering Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s11948-024-00507-y","",438,"2025-02-04 16:55:17","journal-article","10.1007/s11948-024-00507-y","1471-5546","",30,5,,,0,0.00,0,1,1,"Abstract: The popularisation of Artificial Intelligence (AI) technologies has sparked discussion about their ethical implications. This development has forced governmental organisations, NGOs, and private companies to react and draft ethics guidelines for future development of ethical AI systems. Whereas many ethics guidelines address values familiar to ethicists, they seem to lack in ethical justifications. Furthermore, most tend to neglect the impact of AI on democracy, governance, and public deliberation. Existing research suggest, however, that AI can threaten key elements of western democracies that are ethically relevant. In this paper, Rawls’s theory of justice is applied to draft a set of guidelines for organisations and policy-makers to guide AI development towards a more ethical direction. The goal is to contribute to the broadening of the discussion on AI ethics by exploring the possibility of constructing AI ethics guidelines that are philosophically justified and take a broader perspective of societal justice. The paper discusses how Rawls’s theory of justice as fairness and its key concepts relate to the ongoing developments in AI ethics and gives a proposition of how principles that offer a foundation for operationalising AI ethics in practice could look like if aligned with Rawls’s theory of justice as fairness.","https://link.springer.com/content/pdf/10.1007/s11948-024-00507-y.pdf",""
0,"Syafira Fitri Auliya, Olya Kudina, Aaron Yi Ding, Ibo Van de Poel","AI versus AI for democracy: exploring the potential of adversarial machine learning to enhance privacy and deliberative decision-making in elections",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00588-2","",442,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00588-2","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: Our democratic systems have been challenged by the proliferation of artificial intelligence (AI) and its pervasive usage in our society. For instance, by analyzing individuals’ social media data, AI algorithms may develop detailed user profiles that capture individuals’ specific interests and susceptibilities. These profiles are leveraged to derive personalized propaganda, with the aim of influencing individuals toward specific political opinions. To address this challenge, the value of privacy can serve as a bridge, as having a sense of privacy can create space for people to reflect on their own political stance prior to making critical decisions, such as voting for an election. In this paper, we explore a novel approach by harnessing the potential of AI to enhance the privacy of social-media data. By leveraging adversarial machine learning, i.e., “AI versus AI,” we aim to fool AI-generated user profiles to help users hold a stake in resisting political profiling and preserve the deliberative nature of their political choices. More specifically, our approach probes the conceptual possibility of infusing people’s social media data with minor alterations that can disturb user profiling, thereby reducing the efficacy of the personalized influences generated by political actors. Our study delineates the boundary of ethical and practical implications associated with this ‘AI versus AI’ approach, highlighting the factors for the AI and ethics community to consider in facilitating deliberative decision-making toward democratic elections.","https://link.springer.com/content/pdf/10.1007/s43681-024-00588-2.pdf",""
0,"Deepak P., Sahely Bhadra, Anna Jurek-Loughrey, G. Santhosh Kumar, M. Satish Kumar","Geo-political bias in fake news detection AI: the case of affect",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00494-7","",443,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00494-7","2730-5953","",,,,,0,0.00,0,5,1,"Abstract: There have been massive advances in AI technologies towards addressing the contemporary challenge of fake news identification. However, these technologies, as observed widely, have not had the same kind or depth in impact across global societies. In particular, the AI scholarship in fake news detection arguably has not been as beneficial or appropriate for Global South, bringing geo-political bias into the picture. While it is often natural to think of data bias as the potential reason for geo-political bias, other factors could be much more important in being more latent, and thus less visible. In this commentary, we investigate as to how the facet of affect, comprising emotions and sentiments, could be a potent vehicle for geo-political biases in AI. We highlight, through assembling and interpreting insights from literature, the overarching neglect of affect across methods for fake news detection AI, and how this could be a potentially important factor for geo-political bias within them. This exposition, we believe, also serves as a first effort in understanding how geo-political biases work within AI pipelines beyond the data collection stage.","https://link.springer.com/content/pdf/10.1007/s43681-024-00494-7.pdf",""
0,"Karel Van den Bosch, Jurriaan Van Diggelen, Sabine Verdult, Tjalling Haije, Jasper Van der Waa","Measuring meaningful human control in human–AI teaming: effects of team design in AI-assisted pandemic triage",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00647-8","",445,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00647-8","2730-5953","",,,,,0,0.00,0,5,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00647-8.pdf",""
0,"Erez Firt","Correction: Ought we align the values of artificial moral agents?",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00403-4","",446,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00403-4","2730-5953","",4,2,283,283,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00403-4.pdf",""
0,"Ryan Lemasters, Clint Hurshman","A shift towards oration: teaching philosophy in the age of large language models",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00455-0","",451,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00455-0","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00455-0.pdf",""
0,"Paula Boddington","The Rise of AI Ethics",2023,"Artificial Intelligence: Foundations, Theory, and Algorithms","Springer Nature Singapore","https://doi.org/10.1007/978-981-19-9382-4_2","",454,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-19-9382-4_2","2365-3051","",,,35,89,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-981-19-9382-4_2",""
0,"Jonathan Adams","Introducing the ethical-epistemic matrix: a principle-based tool for evaluating artificial intelligence in medicine",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00597-1","",455,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00597-1","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: While there has been much discussion of the ethical assessment of artificial intelligence (AI) in medicine, such work has rarely been combined with the parallel body of scholarship analyzing epistemic implications of AI. This paper proposes a method for joint evaluation of AI’s ethical and epistemic implications in medicine that draws on the principle-oriented tradition in bioethics and the consequent ‘ethical matrix’ approach to assessing novel technologies. It first introduces principle-based approaches as specific tools for ethical assessment of AI in medicine and other domains that are contrasted with the lack of comparable epistemic principles that would govern AI evaluation in medicine. In the next section, the ethical matrix is explained as a well-established principle-based tool in applied ethics that has had some limited applications to near-term implications of AI in medicine and elsewhere that can be strengthened, I suggest, using epistemic principles. To this end, the following section looks to the philosophy of science for relevant epistemic principles, identifying ‘accuracy’, ‘consistency’, ‘relevance’, and ‘instrumental efficacy’ as a provisional set for technology evaluation. The next section articulates the relevance of these epistemic principles to AI in medicine by highlighting conventional standards that have already been applied in AI, epistemology, and the medical sciences. Before concluding, the paper then defines and defends the possibility of an ‘ethical-epistemic matrix’ for the application of these epistemic principles alongside established ethical principles to a selection of stakeholder groups: patients, clinicians, developers, and the public.","https://link.springer.com/content/pdf/10.1007/s43681-024-00597-1.pdf",""
0,"Ugochi A. Okengwu","Practical Implications of Different Theoretical Approaches to AI Ethics",2023,"SpringerBriefs in Ethics","Springer International Publishing","https://doi.org/10.1007/978-3-031-23035-6_3","",456,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-23035-6_3","2211-8101","",,,27,35,0,0.00,0,1,2,"Abstract: Ethics are moral principles that govern a person’s behaviour or the conduct of an activity.","https://link.springer.com/content/pdf/10.1007/978-3-031-23035-6_3",""
0,"Xiao-yu Sun, Bin Ye, Bao-hua Xia","The problem of fairness in tools for algorithmic fairness",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00533-3","",457,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00533-3","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00533-3.pdf",""
0,"Christoph Lütge, Alexander Kriebitz, Raphael Max, Caitlin C. Corrigan","International regulation of AI ethics",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00012","",459,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00012","","",,,109,110,0,0.00,0,4,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Anne Zimmerman, Joel Janhonen, Michael Saadeh, Camille Castelyn, Heikki Saxén","Values in AI: bioethics and the intentions of machines and people",2022,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-022-00242-9","",464,"2025-02-04 16:55:17","journal-article","10.1007/s43681-022-00242-9","2730-5953","",3,3,1003,1012,0,0.00,0,5,3,"","https://link.springer.com/content/pdf/10.1007/s43681-022-00242-9.pdf",""
0,"Reyhane Izadi","Review of: ""AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development""",2024,"","Qeios Ltd","https://doi.org/10.32388/y6jue5","",467,"2025-02-04 16:55:17","peer-review","10.32388/y6jue5","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/Y6JUE5/pdf",""
0,"Mamia Agbese","Review of: ""AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development""",2025,"","Qeios Ltd","https://doi.org/10.32388/jpuckq","",469,"2025-02-04 16:55:17","peer-review","10.32388/jpuckq","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/JPUCKQ/pdf",""
0,"Bernardo Bolaños Guerra, Jorge Luis Morton Gutierrez","On singularity and the Stoics: why Stoicism offers a valuable approach to navigating the risks of AI (Artificial Intelligence)",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00548-w","",470,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00548-w","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: The potential benefits and risks of artificial intelligence technologies have sparked a wide-ranging debate in both academic and public circles. On one hand, there is an urgent call to address the immediate and avoidable challenges associated with these tools, such as accountability, privacy, bias, understandability, and transparency; on the other hand, prominent figures like Geoffrey Hinton and Elon Musk have voiced concerns over the potential rise of Super Artificial Intelligence, whose singularity could pose an existential threat to humanity. Coordinating the efforts of thousands of decentralized entities to prevent such a hypothetical event may seem insurmountable in our intricate and multipolar world. Thus, drawing from both perspectives, this work suggests employing the tools and framework of Stoic philosophy, particularly the concept of the dichotomy of control—focusing on what is within our power. This Stoic principle offers a practical and epistemological approach to managing the complexities of AI, and it encourages individuals to organize their efforts around what they can influence while adapting to the constraints of external factors. Within this framework, the essay found that Stoic wisdom is essential for assessing risks, courage is necessary to face contemporary challenges, and temperance and tranquility are indispensable; and these lessons can inform ongoing public and academic discourse, aiding in the development of more effective policy proposals for aligning Narrow AI and General AI with human values.","https://link.springer.com/content/pdf/10.1007/s43681-024-00548-w.pdf",""
0,"Eric Heinze","Commentary to: AI-based removal of hate speech from digital social networks: chances and risks for freedom of expression (10.1007/s43681-024-00610-7)",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00646-9","",474,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00646-9","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00646-9.pdf",""
0,"Lynn Gribble, Janis Wardrop","Exploring the Challenges Posed to Academic Integrity and Credentialing by Generative AI",2024,"Artificial Intelligence Applications in Higher Education","Routledge","https://doi.org/10.4324/9781003440178-9","",475,"2025-02-04 16:55:17","book-chapter","10.4324/9781003440178-9","","",,,143,162,0,0.00,0,2,1,"","",""
0,"Gregory Antill","Robots and reactive attitudes: a defense of the moral and interpersonal status of non-conscious agents",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00511-9","",476,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00511-9","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00511-9.pdf",""
0,"Junchao Li, Mingyu Cai, Shaoping Xiao","Reinforcement learning-based motion planning in partially observable environments under ethical constraints",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00441-6","",481,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00441-6","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00441-6.pdf",""
0,"Christoph Lütge, Alexander Kriebitz, Raphael Max, Caitlin C. Corrigan","Notions and concepts of AI ethics",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00007","",484,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00007","","",,,11,12,0,0.00,0,4,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Nicholas J. Abernethy","Let stochastic parrots squawk: why academic journals should allow large language models to coauthor articles",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00575-7","",486,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00575-7","2730-5953","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00575-7.pdf",""
0,"Paula Boddington","AI, Philosophy of Technology, and Ethics",2023,"Artificial Intelligence: Foundations, Theory, and Algorithms","Springer Nature Singapore","https://doi.org/10.1007/978-981-19-9382-4_3","",487,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-19-9382-4_3","2365-3051","",,,91,130,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-981-19-9382-4_3",""
0,"Brady Lund, Zeynep Orhan, Nishith Reddy Mannuru, Ravi Varma Kumar Bevara, Brett Porter, Meka Kasi Vinaih, Padmapadanand Bhaskara","Standards, frameworks, and legislation for artificial intelligence (AI) transparency",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-025-00661-4","",490,"2025-02-04 16:55:17","journal-article","10.1007/s43681-025-00661-4","2730-5953","",,,,,0,0.00,0,7,1,"","https://link.springer.com/content/pdf/10.1007/s43681-025-00661-4.pdf",""
0,"Quintin P. McGrath","Unveiling the ethical positions of conversational AIs: a study on OpenAI’s ChatGPT and Google’s Bard",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00433-6","",491,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00433-6","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: In an era where conversational AIs (CAIs) like OpenAI’s ChatGPT and Google's Bard are becoming integral to daily life, understanding their ethical positions is paramount. This research delves into the expressed moral values of these CAIs, exploring how their pre-training influences their ethical stances. The study aims to assess the articulated ethical positions of ChatGPT and Bard, uncovering whether these systems align with particular moral values. By understanding their ethical positions, the research seeks to provide insights into how these CAIs might respond to prompts and guide users in their selection and utilization. Utilizing O’Boyle and Forsyth’s Ethical Position Questionnaire (EPQ-5), the research evaluated the CAIs’ levels of idealism and relativism. The study also involved a third CAI, Anthropic’s Claude and an online human panel, to analyze the reasoning behind the responses, providing a more nuanced understanding of the ethical positions. The initial findings revealed that ChatGPT aligns more with an ‘absolutist’ position, endorsing strict adherence to moral principles, while Bard leans towards a ‘situationist’ stance, valuing flexibility and situational considerations. However, further analysis by Claude and humans suggested a more complex categorization, with ChatGPT fitting the 'exceptionist' categorization and Bard aligning with ‘absolutism.’ The research underscores the significance of recognizing the trained-in ethical positions of CAIs, as they are not neutral but reflect particular ethical leanings. Understanding these positions is vital for interpreting CAI outputs and using these systems effectively and ethically. The study calls for further exploration into how these ethical positions might influence real-world applications of CAIs.","https://link.springer.com/content/pdf/10.1007/s43681-024-00433-6.pdf",""
0,"Haleh Asgarinia","Convergence of the source control and actual access accounts of privacy",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00270-z","",492,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00270-z","2730-5953","",4,2,333,343,0,0.00,0,1,2,"Abstract: In this paper, it is argued that, when properly revised in the face of counter-examples, the source control and actual access views of privacy are extensionally equivalent but different in their underlying rationales. In this sense, the source control view and the actual access view, when properly modified to meet counter-examples, can be metaphorically compared to ‘climbing the same mountain but from different sides’ (as Parfit [1] has argued about normative theories). These two views can equally apply to the privacy debates and, thus, resolve a long-standing debate in the literature.","https://link.springer.com/content/pdf/10.1007/s43681-023-00270-z.pdf",""
0,"Subhasree Sengupta, Christopher Flathmann, Beau Schelble, Joseph B. Lyons, Nathan McNeese","An analysis of ethical rationales and their impact on the perceived moral persona of AI teammates",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00515-5","",493,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00515-5","2730-5953","",,,,,0,0.00,0,5,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00515-5.pdf",""
0,"Khadija Alam, Akhil Kumar, F. N. U. Samiullah","Prospectives and drawbacks of ChatGPT in healthcare and clinical medicine",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00434-5","",494,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00434-5","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00434-5.pdf",""
0,"","PRINCIPLES BASED ETHICS AND VIRTUE ETHICS",2023,"Trustworthy AI Alone Is Not Enough.","Dykinson","https://doi.org/10.2307/jj.8500773.11","",502,"2025-02-04 16:55:17","book-chapter","10.2307/jj.8500773.11","","",,,117,123,0,0.00,0,0,2,"","",""
0,"Alva Markelius, Connor Wright, Joahna Kuiper, Natalie Delille, Yu-Ting Kuo","The mechanisms of AI hype and its planetary and social costs",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00461-2","",503,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00461-2","2730-5953","",4,3,727,742,0,0.00,0,5,1,"Abstract: Our global landscape of emerging technologies is increasingly affected by artificial intelligence (AI) hype, a phenomenon with significant large-scale consequences for the global AI narratives being created today. This paper aims to dissect the phenomenon of AI hype in light of its core mechanisms, drawing comparisons between the current wave and historical episodes of AI hype, concluding that the current hype is historically unmatched in terms of magnitude, scale and planetary and social costs. We identify and discuss socio-technical mechanisms fueling AI hype, including anthropomorphism, the proliferation of self-proclaimed AI “experts”, the geopolitical and private sector “fear of missing out” trends and the overuse and misappropriation of the term “AI” in emerging technologies. The second part of the paper seeks to highlight the often-overlooked costs of the current AI hype. We examine its planetary costs as the AI hype exerts tremendous pressure on finite resources and energy consumption. Additionally, we focus on the connection between AI hype and socio-economic injustices, including perpetuation of social inequalities by the huge associated redistribution of wealth and costs to human intelligence. In the conclusion, we offer insights into the implications for how to mitigate AI hype moving forward. We give recommendations of how developers, regulators, deployers and the public can navigate the relationship between AI hype, innovation, investment and scientific exploration, while addressing critical societal and environmental challenges.","https://link.springer.com/content/pdf/10.1007/s43681-024-00461-2.pdf",""
0,"Chhanda Chakraborti","Applied Ethics: AI and Ethics",2023,"Introduction to Ethics","Springer Nature Singapore","https://doi.org/10.1007/978-981-99-0707-6_8","",504,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-99-0707-6_8","","",,,619,754,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-981-99-0707-6_8",""
0,"Steven Gustafson","Architecting AI will improve AI ethics",2022,"Journal of AI, Robotics &amp; Workplace Automation","Henry Stewart Publications","https://doi.org/10.69554/eewn6637","",507,"2025-02-04 16:55:17","journal-article","10.69554/eewn6637","2633-5638","",1,4,329,329,0,0.00,0,1,3,"","",""
0,"John-Stewart Gordon","AI Ethics and Machine Ethics",2024,"Handbook on the Ethics of Artificial Intelligence","Edward Elgar Publishing","https://doi.org/10.4337/9781803926728.00012","",508,"2025-02-04 16:55:17","book-chapter","10.4337/9781803926728.00012","","",,,97,112,0,0.00,0,1,1,"","https://www.elgaronline.com/view/book/9781803926728/9781803926728.xml",""
0,"G. Arun Sampaul Thomas, S. Muthukaruppasamy, S. Sathish Kumar, Beulah J. Karthikeyan, K. Saravanan","Navigating the Nexus: Unravelling Challenges, Ethics, and Applications of Embodied AI in Drone Technology Through the Lens of Computer Vision",2024,"Information Systems Engineering and Management","Springer Nature Switzerland","https://doi.org/10.1007/978-3-031-68256-8_3","",509,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-68256-8_3","3004-958X","",,,61,78,0,0.00,0,5,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-68256-8_3",""
0,"Saara Ala-Luopa","Review of: ""AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development""",2024,"","Qeios Ltd","https://doi.org/10.32388/9rq575","",510,"2025-02-04 16:55:17","peer-review","10.32388/9rq575","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/9RQ575/pdf",""
0,"Nythamar de Oliveira","Review of: ""AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development""",2024,"","Qeios Ltd","https://doi.org/10.32388/zpp1u8","",511,"2025-02-04 16:55:17","peer-review","10.32388/zpp1u8","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/ZPP1U8/pdf",""
0,"Dhruvitkumar Talati","Ethics of AI (Artificial Intelligence)",2024,"","Institute of Electrical and Electronics Engineers (IEEE)","https://doi.org/10.36227/techrxiv.170751714.41555942/v1","",512,"2025-02-04 16:55:17","posted-content","10.36227/techrxiv.170751714.41555942/v1","","",,,,,0,0.00,0,1,1,"The standard solution to new technology is to center the ethics of robotics and artificial intelligence on ”concerns” of various kinds. Many of these fears end up being rather outdated; a few are essentially accurate but barely relevant (computer technological advances will annihilate businesses that make pictures on film, audio cassettes, or vinyl records); others are essentially accurate but extremely pertinent (automobiles will cause the deaths of children and drastically alter the landscape). Some of these fears are consistently incorrect when they indicate that technology will totally transform humans. This paper analyzes the problems and deflates the non-problems.","",""
0,"Sujan Sai Gannamaneni, Michael Mock, Maram Akila","Assessing systematic weaknesses of DNNs using counterfactuals",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00407-0","",515,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00407-0","2730-5953","",4,1,27,35,0,0.00,0,3,1,"Abstract: With the advancement of DNNs into safety-critical applications, testing approaches for such models have gained more attention. A current direction is the search for and identification of systematic weaknesses that put safety assumptions based on average performance values at risk. Such weaknesses can take on the form of (semantically coherent) subsets or areas in the input space where a DNN performs systematically worse than its expected average. However, it is non-trivial to attribute the reason for such observed low performances to the specific semantic features that describe the subset. For instance, inhomogeneities within the data w.r.t. other (non-considered) attributes might distort results. However, taking into account all (available) attributes and their interaction is often computationally highly expensive. Inspired by counterfactual explanations, we propose an effective and computationally cheap algorithm to validate the semantic attribution of existing subsets, i.e., to check whether the identified attribute is likely to have caused the degraded performance. We demonstrate this approach on an example from the autonomous driving domain using highly annotated simulated data, where we show for a semantic segmentation model that (i) performance differences among the different pedestrian assets exist, but (ii) only in some cases is the asset type itself the reason for this reduction in the performance.","https://link.springer.com/content/pdf/10.1007/s43681-023-00407-0.pdf",""
0,"Vanessa Schäffner","Crash dilemmas and the ethical design of self-driving vehicles: implications from metaethics and pragmatic road marks",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00591-7","",516,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00591-7","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: How should self-driving vehicles react when an accident can no longer be averted in dangerous situations? The complex issue of designing crash algorithms has been discussed intensively in recent research literature. This paper refines the discourse around a new perspective which reassesses the underlying dilemma structures in the light of a metaethical analysis. It aims at enhancing the critical understanding of both the conceptual nature and specific practical implications that relate to the problem of crash algorithms. The ultimate aim of the paper is to open up a way to building a bridge between the inherent structural issues of dilemma cases on the one hand and the characteristics of the practical decision context related to driving automation scenarios on the other. Based on a reconstruction of the metaethical structure of crash dilemmas, a pragmatic orientation towards the ethical design of crash algorithms is sketched and critically examined along two central particularities of the practical problem. Firstly, pertinent research on the social nature of crash dilemmas is found to be merely heuristic. Secondly, existing work from ethics of risk hardly offers explicit ethical solutions to relevant and urgent challenges. Further investigation regarding both aspects is ultimately formulated as a research desideratum.","https://link.springer.com/content/pdf/10.1007/s43681-024-00591-7.pdf",""
0,"Daphne Lenders, Toon Calders","Users’ needs in interactive bias auditing tools introducing a requirement checklist and evaluating existing tools",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00342-0","",517,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00342-0","2730-5953","",,,,,0,0.00,0,2,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00342-0.pdf",""
0,"Mohsin Ali Farhad, Muhammad Hamza Zakir","Adapting legal horizons in reshaping intellectual property law for the artificial intelligence revolution",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00555-x","",520,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00555-x","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00555-x.pdf",""
0,"Aidan Kierans","Benchmarked Ethics: A Roadmap to AI Alignment, Moral Knowledge, and Control",2023,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3600211.3604764","",521,"2025-02-04 16:55:17","proceedings-article","10.1145/3600211.3604764","","",,,964,965,0,0.00,0,1,2,"","https://dl.acm.org/doi/pdf/10.1145/3600211.3604764",""
0,"Edmund O. Benefo, Abani K. Pradhan, Debasmita Patra","The ethics of online AI-driven agriculture and food systems",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00009-3","",522,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00009-3","","",,,153,174,0,0.00,0,3,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000093",""
0,"Barbara J. Evans, Azra Bihorac","Co-creating Consent for Data Use — AI-Powered Ethics for Biomedical AI",2024,"NEJM AI","Massachusetts Medical Society","https://doi.org/10.1056/aipc2400237","",523,"2025-02-04 16:55:17","journal-article","10.1056/aipc2400237","2836-9386","",1,7,,,0,0.00,0,2,1,"","",""
0,"","Ethics in Online AI-based Systems",2024,"","Elsevier","https://doi.org/10.1016/c2022-0-00475-x","",524,"2025-02-04 16:55:17","edited-book","10.1016/c2022-0-00475-x","","",,,,,0,0.00,0,0,1,"","https://api.elsevier.com/content/article/PII:C2022000475X",""
0,"Anna Gausen, Ce Guo, Wayne Luk","An approach to sociotechnical transparency of social media algorithms using agent-based modelling",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00527-1","",526,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00527-1","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: The recommendation algorithms on social media platforms are hugely impactful, they shape information flow and human connection on an unprecedented scale. Despite growing criticism of the social impact of these algorithms, they are still opaque and transparency is an ongoing challenge. This paper has three contributions: (1) We introduce the concept of","https://link.springer.com/content/pdf/10.1007/s43681-024-00527-1.pdf",""
0,"Sai S. Kurapati, Antonio Yaghy, Aakriti G. Shukla","Data bias: ethical considerations for understanding diversity in medical artificial intelligence",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00589-1","",530,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00589-1","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00589-1.pdf",""
0,"Rashmi Nagpal, Rasoul Shahsavarifar, Vaibhav Goyal, Amar Gupta","Optimizing fairness and accuracy: a Pareto optimal approach for decision-making",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00508-4","",531,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00508-4","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: In the era of data-driven decision-making, ensuring fairness and equality in machine learning models has become increasingly crucial. Multiple fairness definitions have been brought forward to evaluate and mitigate unintended fairness-related harms in real-world applications, with little research on addressing their interactions with each other. This paper explores the application of a Minimax Pareto-optimized solution to optimize individual and group fairness at individual and group levels on the Adult Census Income dataset as well as on the German Credit dataset. The objective of training a classification model with a multi-objective loss function is to achieve fair outcomes without compromising utility objectives. We investigate the interplay of different fairness definitions, including definitions of performance consistency and traditional group and individual fairness measures, amongst each other coupled with performance. The results presented in this paper highlight the feasibility of incorporating several fairness considerations into machine learning models, which can be applied to use cases with multiple sensitive features and attributes that characterize real-world applications. This research is a valuable step toward building responsible and transparent machine learning systems that can be incorporated into critical decision-making processes.","https://link.springer.com/content/pdf/10.1007/s43681-024-00508-4.pdf",""
0,"J. L. A. Donohue","Click-Gap, paternalism, and tech giants’ relationships with their users",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00369-3","",532,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00369-3","2730-5953","",4,4,1441,1452,0,0.00,0,1,2,"Abstract: The spread of misinformation and fake news raises important problems for our society and for our democracy. From the January 6 attack on the U.S. Capitol to vaccine hesitancy, from suppressing voter turnout to peddling conspiracy theories, we know that these problems are real and need to be taken seriously. While misinformation is not a new problem for democracy, it can spread more quickly and easily because of new media’s design and popularity. Given these problems, it is encouraging that some technology companies are taking steps to reduce the spread of misinformation and fake news on the platforms they manage. Despite this seemingly positive development, some scholars have criticized some interventions designed to combat the spread of misinformation and fake news as paternalistic. For example, a 2019 Facebook intervention called Click-Gap aimed to reduce the amount of low-quality content (including fake news and misinformation) that users see on their NewsFeed. Click-Gap has been criticized as an instance of epistemic paternalism because it was adopted (1) with the goal of improving the epistemic status of its users and (2) irrespective of what the company believed the wishes of its users to be. If interventions like Click-Gap are problematic because paternalistic, those of us interested in the ethics of technology would face a dilemma—either endorse technology companies treating their users paternalistically or endorse their failing to act to combat the spread of misinformation and fake news on their platforms. Both options seem to me to be problematic. While paternalism may sometimes be permissible, I think we should be very hesitant to endorse a paternalistic relationship between technology companies and their users. The relationship does not seem to bear the right sort of structure to one in which paternalism might be appropriate, if it ever is. The second option seems, if anything worse: surely technology companies should not stand by and change nothing about their platforms despite the spread of misinformation and fake news in those spaces. In this paper, I argue that Click-Gap and interventions like it are not paternalistic, contrary to the conclusion of other scholars. Further, I will argue that the focus on paternalism itself is actually a red herring here. While not just any intervention or strategy that purports to reduce fake news and misinformation is permissible, we should want technology companies to take user well-being seriously and be able to take that well-being as a direct reason for action. Their doing so is not paternalistic nor even morally problematic, and it should not be criticized as such.","https://link.springer.com/content/pdf/10.1007/s43681-023-00369-3.pdf",""
0,"Gordon Bowen, Deidre Bowen, Lisa Bamford","AI and Ethics: Embedding Good Aspects of AI",2024,"Advanced Sciences and Technologies for Security Applications","Springer Nature Switzerland","https://doi.org/10.1007/978-3-031-47594-8_13","",533,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-47594-8_13","1613-5113","",,,245,258,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-47594-8_13",""
0,"","AI in Higher Education: Ethics and Innovation",2024,"","Lumivero","https://doi.org/10.4135/9781036213411","",534,"2025-02-04 16:55:17","book","10.4135/9781036213411","","",,,,,0,0.00,0,0,1,"","",""
0,"Dario Cecchini, Michael Pflanzer, Veljko Dubljević","Aligning artificial intelligence with moral intuitions: an intuitionist approach to the alignment problem",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00496-5","",535,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00496-5","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00496-5.pdf",""
0,"Garry Young","Using the classic trolley problem to teach AI students and researchers about their role as moral agents, and why they should be subject to moral scrutiny",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00509-3","",537,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00509-3","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: This commentary proposes a means of teaching students – particularly computer science students – about their role as moral agents, who, on account of this role, are necessarily subject to moral scrutiny. It utilizes the classic Trolley Problem; but instead of focusing on the morality of the","https://link.springer.com/content/pdf/10.1007/s43681-024-00509-3.pdf",""
0,"B. A. Kamphorst, J. H. Anderson","E-coaching systems and social justice: ethical concerns about inequality, coercion, and stigmatization",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00424-7","",539,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00424-7","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: Poor self-regulation has been linked to various behaviors that contribute to pressing societal issues, including rising household debt, inefficient use of sustainable resources, and increasing healthcare demands. In light of this observation, the prospect of individuals receiving automated, tailored support by “e-coaching systems” to scaffold and improve their self-regulation is thought to hold promise for making society-wide progress in addressing such issues. Though there may be legitimate reasons for promoting the use of such systems, and individuals might welcome the support, our aim in the present article is to contribute to the ethics of e-coaching by showing how societal pressures towards the widespread adoption of automated e-coaching systems raise concerns in relation to three distinct aspects of social justice. We argue that societal inequalities may be introduced or exacerbated by (1) unequal access to the technologies, (2) unequally distributed restrictions to liberty and subjection to coercion, and (3) the potentially disparate impact of the use of e-coaching technologies on (self-)stigmatizing perceptions of competence. The article offers a research agenda for studying and addressing these concerns.","https://link.springer.com/content/pdf/10.1007/s43681-024-00424-7.pdf",""
0,"Arleen Salles, Abel Wajnerman Paz","Anthropomorphism in social AIs: Some challenges",2024,"Developments in Neuroethics and Bioethics","Elsevier","https://doi.org/10.1016/bs.dnb.2024.02.007","",542,"2025-02-04 16:55:17","book-chapter","10.1016/bs.dnb.2024.02.007","2589-2959","",,,101,118,0,0.00,0,2,1,"","https://api.elsevier.com/content/article/PII:S2589295924000122",""
0,"I Ketut Ardhana, Ni Made Putri Ariyanti","Artificial Intelligence, Ethics, and Spirituality in the Modern Balinese Society",2023,"Advances in Human and Social Aspects of Technology","IGI Global","https://doi.org/10.4018/978-1-6684-9196-6.ch007","",543,"2025-02-04 16:55:17","book-chapter","10.4018/978-1-6684-9196-6.ch007","2328-1316","",,,90,106,0,0.00,0,2,2,"There is much debate about how the influence of artificial intelligence on the global world has caused various changes, including in Bali. The debate that occurred revolved around the pros and cons related to the introduction of artificial intelligence, where the pros saw the need to use artificial intelligence. However, on the other hand, are those who are against the view that the use of artificial intelligence has indeed made a difference with ethical and spiritual issues. For that, several significant questions arise among them. First, how is artificial intelligence accepted and developed in society? Second, how does the process of change affect the cultural roots of society? And third, how can the application of artificial intelligence have and strengthen meaning in people's lives concerning ethical, and spiritual issues that have an important role in the life of a globalized society? This study uses the approach of the social sciences and humanities, with an interdisciplinary approach, using qualitative data.","https://www.igi-global.com/viewtitle.aspx?TitleId=331960",""
0,"Sreerakuvandana Sreerakuvandana, Princy Pappachan, Varsha Arya","Understanding Large Language Models",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch001","",545,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch001","2327-0411","",,,1,24,0,0.00,0,3,1,"Large language models (LLMs) are a revolutionary development that allows machines to comprehend and produce text, similar to that of humans on a never-before-seen scale. This chapter examines the basic ideas underlying LLMs with an emphasis on their applications, training approaches, and architecture. Deep neural networks with billions of parameters are used by LLMs, such as the GPT-3 model, to capture complex linguistic patterns and contextual subtleties. Massive datasets, frequently drawn from a variety of online sources, are used in the training process to impart a thorough understanding of language. Consequently, LLMs show remarkable abilities in tasks like question answering, language translation, and text generation. Issues like bias, ethical issues, and interpretability thus become important concerns. So, this chapter outlines the main elements of LLMs, discusses their advantages, reviews current research, and addresses the ethical issues surrounding their application.","https://www.igi-global.com/viewtitle.aspx?TitleId=354391",""
0,"Sun-young Song","A Task of Convergent AI Ethics Education in School Curriculum - with emphasis on the major of AI Convergent Education in graduate schools of education and the class of ‘AI Ethics’ -",2022,"Journal of Ethics Education Studies","The Korean Ethics Education Association","https://doi.org/10.18850/jees.2022.63.03","",546,"2025-02-04 16:55:17","journal-article","10.18850/jees.2022.63.03","1738-0545","",63,,51,77,0,0.00,0,1,3,"","",""
0,"Alexander Kriebitz, Christoph Lütge, Caitlin Corrigan, Raphael Max","Introduction to The Elgar Companion to Applied AI Ethics",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00006","",548,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00006","","",,,1,10,0,0.00,0,4,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Aisha Aijaz","Bridging Ethics and AI: A Path to Moral Machines",2025,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i2.31892","",549,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i2.31892","3065-8365","",7,2,2,4,0,0.00,0,1,1,"Ethics is ubiquitous in most domains, requiring much deliberation due to its philosophical nature. Varying views often lead to conflicting courses of action where ethical dilemmas become challenging to resolve. The major driving forces to make such a decision can be discretized and simplified to provide an indication of the most ethical course of action in a context. Given the parallel ubiquity of AI systems in these domains, it becomes increasingly imperative to work towards building inherently ethical AI that holds the ability to reason morally. This work proposes the use of knowledge representation and neurosymbolic techniques to develop resources for inherently ethical AI. It presents a three-phase framework towards bridging the path to moral machines: (a) Applied Ethics Ontology to make explicit the abstract concepts and relationships, (b) Dataset and graph generation using LLMs to develop a benchmark data store for ethical reasoning, and a (c) Case-based Reasoning algorithm to implement the philosophical concept of casuistry to make moral judgments and resolve ethical dilemmas.","https://ojs.aaai.org/index.php/AIES/article/download/31892/34059",""
0,"Tatiana Chakravorti","Enhancing Transparency and Research Ethics through Human AI Techniques",2025,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i2.31894","",550,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i2.31894","3065-8365","",7,2,8,10,0,0.00,0,1,1,"In recent years, the empirical sciences have confronted a crisis of confidence following high-profile replication projects reporting disappointing results. The 'replication crisis' has led to widespread introspection and calls for reform across various fields. At the heart of this movement is the fundamental belief that reproducing, replicating, and interrogating published claims is essential to science. To facilitate this, many in the scientific community have emphasized the importance of openness and transparency in research, well-aligned incentives for researchers, robust peer review, and improved methods for meta-analyses over scientific corpora. Our work identifies with this movement, and in particular, highlights opportunities for emerging technologies to contribute to novel solutions in this space. This research proposes a coherent research agenda exploring artificial intelligence (AI) and hybrid human-AI technologies to support the evaluation of the replicability of scientific findings and understanding of science as a set of social and cultural processes. We discuss work developing and testing a new class of AI algorithms, specifically, artificial prediction markets populated by algorithmic traders who buy and sell contracts representing the outcomes of replication studies of published research outcomes. Given the critical role of human intuition and experience in making it more trustworthy, we explore ways in which artificial prediction markets can be modified to include human participants, so-called hybrid prediction markets. We study these hybrid systems both in simulation and in pilot testing with human participants in a real-world scenario. In a series of studies, we engage in extensive surveys and interviews with researchers to better understand the usefulness and design requirements for both AI and hybrid human-AI technologies in the scientific workflow. We solicit feedback on artificial and hybrid markets for estimating the replicability of published claims and focus on the role of transparency and explainability in evaluating scientific integrity. We aim to situate these concerns within social and cultural contexts, making explicit the norms and incentives driving current research ecosystems.","https://ojs.aaai.org/index.php/AIES/article/download/31894/34061",""
0,"Michael Robillard","The Ethics of Weaponized AI",2022,"Oxford Handbook of Digital Ethics","Oxford University Press","https://doi.org/10.1093/oxfordhb/9780198857815.013.29","",551,"2025-02-04 16:55:17","book-chapter","10.1093/oxfordhb/9780198857815.013.29","","",,,631,652,0,0.00,0,1,3,"Abstract: This chapter presents an overview of some of the major ethical arguments for and against the use of autonomous weapons systems (AWS). More specifically, this chapter looks at the set of contingent arguments as well as the set of in principle arguments for and against their use. After summarizing these various views, the chapter argues that AWS do not pose new or novel ethical problems. If we think an AWS makes actual decisions in the ‘strong AI’ sense, then by virtue of being a decision-maker, that entity would therefore have rights and interests worthy of our moral concern. If we, however, think an AWS does not make actual decisions, but is instead just an institutional proxy for the collective set of human decisions comprising it, then we ought to treat an AWS, both morally and metaphysically, like we would treat any other collective action problem.","https://academic.oup.com/edited-volume/37078/chapter/337809808",""
0,"Katherine B. Snyder, R. Austin Stewart, Catherine J. Hunter","Ethical considerations for the application of artificial intelligence in pediatric surgery",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00525-3","",555,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00525-3","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00525-3.pdf",""
0,"Suvrat Arora, Aditi Srivastava","A cross-lingual syntactic investigation of gender bias and stereotyping in GPT-4o: English vs Hindi",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00565-9","",556,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00565-9","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00565-9.pdf",""
0,"Vyoma Gajjar","The Future of AI Governance: Navigating the Challenges of Generative AI",2024,"International Journal of Science and Research (IJSR)","International Journal of Science and Research","https://doi.org/10.21275/sr24927012739","",557,"2025-02-04 16:55:17","journal-article","10.21275/sr24927012739","2319-7064","",13,9,1686,1687,0,0.00,0,1,1,"","",""
0,"Suzana Alpsancar, Heike M. Buhl, Tobias Matzner, Ingrid Scharlau","Explanation needs and ethical demands: unpacking the instrumental value of XAI",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00622-3","",558,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00622-3","2730-5953","",,,,,0,0.00,0,4,1,"Abstract: The call for XAI rests on a normative claim: ‘Good AI is explainable AI’ or even the stronger claim: ‘Only explainable AI is good AI.’ However, this valorization runs the risk of being overgeneralized because explanations are not per se useful, appropriate, or demanded. Explainability should not be seen as a value in itself but as a means to certain ends. In this paper, we put the valorization of explainability into question, which is discursively connected to the idea of ‘users’ needs’ and the will to design and develop ethically aligned AI systems. By making the instrumental character of the value of explainability explicit, we address two key issues that necessitate more theoretical attention: (i) to analyze the link between explainability and its presumed purpose; and (ii) to clarify the conceptions of these presumed purposes, namely users’ needs and ethical principles XAI is meant to promote. From a philosophical and from a psychological perspective, we constructively criticize the undertheorized and undercomplex way of talking about ‘users’ needs’ and ethical demands. We plea to carefully differentiate the value of explainable AI in social contexts and signal further need for research.","https://link.springer.com/content/pdf/10.1007/s43681-024-00622-3.pdf",""
0,"Bryan Lavender, Sami Abuhaimed, Sandip Sen","Positive and negative explanation effects in human–agent teams",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00396-0","",562,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00396-0","2730-5953","",4,1,47,56,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00396-0.pdf",""
0,"Haleh Asgarinia","Publisher Correction: Convergence of the source control and actual access accounts of privacy",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00286-5","",563,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00286-5","2730-5953","",4,2,499,499,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00286-5.pdf",""
0,"Dan Hendrycks","Overview of Catastrophic AI Risks",2024,"Introduction to AI Safety, Ethics, and Society","CRC Press","https://doi.org/10.1201/9781003530336-1","",564,"2025-02-04 16:55:17","book-chapter","10.1201/9781003530336-1","","",,,3,50,0,0.00,0,1,1,"","",""
0,"Paula Boddington","Philosophy for AI Ethics: Metaethics, Metaphysics, and More",2023,"Artificial Intelligence: Foundations, Theory, and Algorithms","Springer Nature Singapore","https://doi.org/10.1007/978-981-19-9382-4_7","",567,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-19-9382-4_7","2365-3051","",,,277,317,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-981-19-9382-4_7",""
0,"Laurence José","Generative AI, Algorithms, and Ethics",2024,"Public Relations and the Rise of AI","Routledge","https://doi.org/10.4324/9781032671482-18","",568,"2025-02-04 16:55:17","book-chapter","10.4324/9781032671482-18","","",,,227,241,0,0.00,0,1,1,"","",""
0,"Philipp Hartmann","AI in the Crafts",2023,"Work and AI 2030","Springer Fachmedien Wiesbaden","https://doi.org/10.1007/978-3-658-40232-7_25","",569,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-658-40232-7_25","","",,,219,226,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-3-658-40232-7_25",""
0,"Katharina Simbeck","Publisher Correction: They shall be fair, transparent, and robust: auditing learning analytics systems",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00301-9","",572,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00301-9","2730-5953","",4,2,573,573,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00301-9.pdf",""
0,"Hendrik Kempt, Jan-Christoph Heilinger","The Ethics of (Generative) AI",2024,"Critical AI","Duke University Press","https://doi.org/10.1215/2834703x-11205175","",573,"2025-02-04 16:55:17","journal-article","10.1215/2834703x-11205175","2834-703X","",2,1,,,0,0.00,0,2,1,"Abstract: The clamor for AI-based applications involving generative models for text and images has fueled wild speculation about the risks and opportunities for society and humanity at large. The potential “existential” threat as a precursor to artificial general intelligence has provoked wide-ranging debates in the public, politics, and the corporate world involving technologists and ethicists from a range of academic disciplines. This thinkpiece proposes a metaperspective to reflect critically and constructively upon the current state of the field of AI ethics, arguing that scholars working in the domain of ethics should focalize conceptual, substantive, and procedural issues as integral elements of an ethical assessment of given technologies and their applications. It suggests that the ethics of generative AI is conceptually still underexplored and overly propagating technological fixes to problems of all kinds (technosolutionism). Procedurally, it needs to be clarified who can, who ought to, and who ultimately will be considered and heard as an expert on AI ethics, a question of relevance for the trust in, and reliance on, AI.","https://read.dukeupress.edu/critical-ai/article/doi/10.1215/2834703X-11205175/390845/The-Ethics-of-Generative-AI",""
0,"","AI Ethics in Practice: A Literature Review on AI Professional's perception and attitude towards Ethical and Governance principles of AI.",2024,"European Economic Letters","Science Research Society","https://doi.org/10.52783/eel.v14i4.2232","",574,"2025-02-04 16:55:17","journal-article","10.52783/eel.v14i4.2232","","",,,,,0,0.00,0,0,1,"","",""
0,"Shameem Farouk, Aeron Zentner","Responsible AI, Data Governance, and Ethics",2025,"AI and Machine Learning","SAGE Publications, Inc.","https://doi.org/10.4135/9781071982716","",575,"2025-02-04 16:55:17","other","10.4135/9781071982716","","",,,,,0,0.00,0,2,1,"","",""
0,"","Trends and Standardization of Artificial Intelligence (AI) Ethics Regulations",2024,"The Korean Society for Artificial Intelligence Ethics","The Korean Society for Artificial Intelligence Ethics","https://doi.org/10.59728/jaie.2024.3.2.6","",576,"2025-02-04 16:55:17","journal-article","10.59728/jaie.2024.3.2.6","2982-4923","",3,2,6,33,0,0.00,0,0,1,"With the development of AI technology, ethical, legal, and social problems are emerging. In particular, examples of abuse of Generative AI include fake news, deepfakes, automatic spam and phishing, and copyright in-fringement, and ethical regulations are needed. Globally, these problems are responded to through AI ethics guidelines and AI Ethics Committee, among which the European Union is implementing safety and accountabil-ity and ethical evaluation through AI Act. In addition, AI ethics standardiza-tion is necessary to strengthen global competitiveness, secure social trust, and minimize negative effects. To this end, the domestic AI Ethics Forum promotes the ethical use of AI technology through discussion of ethical is-sues, guideline development, education, and international cooperation ac-tivities. In this paper, we examine the overall status of artificial intelligence ethics regulation trends and standardization, which can be expected to have effects such as reliability, safety assurance, innovation promotion, and increased social acceptance through standardization activities.","",""
0,"Shameem Farouk, Aeron Zentner","Responsible AI, Data Governance, and Ethics",2025,"AI and Machine Learning","SAGE Publications, Inc.","https://doi.org/10.4135/9781071982556","",577,"2025-02-04 16:55:17","other","10.4135/9781071982556","","",,,,,0,0.00,0,2,1,"","",""
0,"Princy Pappachan, Massoud Moslehpour, Ritika Bansal, Mosiur Rahaman","Transparency and Accountability",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch006","",579,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch006","2327-0411","",,,178,211,0,0.00,0,4,1,"The rapid growth and application of AI has ushered in ground-breaking technologies like LLMs. However, these innovations also bring significant challenges related to transparency and accountability, especially considering the complex neural network architectures and vast training datasets. This chapter thus explores the journey of AI from rule-based systems to the current ML and deep neural network, identifying the black box problem that plagues the decision-making process in LLMs. The chapter introduces strategies for enhancing transparency using explainable AI (XAI) frameworks to address these issues, offering practical solutions to quantify and improve transparency. Accountability is also emphasized through a detailed protocol for assigning responsibility across AI development phases, reinforced by ethical auditing and reporting methodologies. Mathematical equations and frameworks are also presented to compute transparency scores and accountability measures, providing organizations with structured, actionable guidelines for building transparent, fair, and ethical AI systems.","https://www.igi-global.com/viewtitle.aspx?TitleId=354396",""
0,"Hongwei Sheng, Xin Shen, Heming Du, Hu Zhang, Zi Huang, Xin Yu","AI empowered Auslan learning for parents of deaf children and children of deaf adults",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00457-y","",583,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00457-y","2730-5953","",4,4,877,887,0,0.00,0,6,1,"Abstract: Communication poses a challenge for the deaf and hearing loss community. This difficulty is even more pronounced in the families of Children of Deaf Adults (CODAs) and Parents of Deaf Children (PODCs). To help these families overcome this challenge, we design an AI-empowered interactive bi-directional Australian Sign Language (i.e., Auslan) dictionary application to facilitate communication within a household. Technically, our APP can not only look up sign gestures for the given English words but also translate isolated Auslan gestures into English. Through an inviting user interface and experience design, we can further improve engagement within the CODA and PODC families while enabling Auslan education at home. The positive user experience underscores the success of our APP not only in leveraging AI to revolutionise Auslan education but also in promoting cross-generational language acquisition and communication.","https://link.springer.com/content/pdf/10.1007/s43681-024-00457-y.pdf",""
0,"C. Karthikeyan","AI (Artificial Intelligence) Integration for Integrity Ethics and Privacy in AI-Driven Organizations",2024,"Advances in Business Strategy and Competitive Advantage","IGI Global","https://doi.org/10.4018/979-8-3693-8442-8.ch003","",587,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-8442-8.ch003","2327-3429","",,,51,76,0,0.00,0,1,1,"As India rapidly advances in artificial intelligence (AI), the need for robust ethical frameworks and comprehensive data privacy measures has become increasingly critical. This paper explores the current landscape of AI ethics and privacy concerns within India, highlighting key developments such as the implementation of the Personal Data Protection Bill (PDPB) and various state-level initiatives like the Karnataka AI Policy 2023. It examines how these efforts align with global standards and address emerging challenges in the digital age. The chapter also contrasts India's approach with those of other developing nations, showcasing the unique hurdles and strategies employed. The disparateness and the importance of strengthening regulatory frameworks, promoting ethical AI practices, enhancing public awareness, and fostering international collaboration.","https://www.igi-global.com/viewtitle.aspx?TitleId=364225",""
0,"Min-Sun Kim","Ethics Beyond Ethics: AI, Power, and Colonialism",2024,"Handbook on the Ethics of Artificial Intelligence","Edward Elgar Publishing","https://doi.org/10.4337/9781803926728.00021","",588,"2025-02-04 16:55:17","book-chapter","10.4337/9781803926728.00021","","",,,232,245,0,0.00,0,1,1,"","https://www.elgaronline.com/view/book/9781803926728/9781803926728.xml",""
0,"","Chapter 13: Challenges of AI",2024,"Using AI in Marketing","De Gruyter","https://doi.org/10.1515/9781501519765-017","",589,"2025-02-04 16:55:17","book-chapter","10.1515/9781501519765-017","","",,,121,126,0,0.00,0,0,1,"","https://www.degruyter.com/document/doi/10.1515/9781501519765-017/xml",""
0,"C. H. Maia, P. Ariel, S. Nunes","Adding human values on the deepfake: co-designing fact-checking solutions to combat misinformation",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00619-y","",591,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00619-y","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: The proliferation of misinformation poses a significant challenge to societies, and fact-checking emerges as a critical tool to combat this issue. In this work, we conduct an innovation impact assessment to question the use of technology to combat misinformation, specifically examining the ethical implications of this choice. To address this, we organized a workshop using the value sensitive design (VSD) methodology to explore questions in this context. The workshop introduced participants to the VSD framework, enabling them to critically assess whether specific scenarios align with human values, norms, and requirements. Real-world scenarios were discussed, including approaches implemented by legitimate news outlets and using 3D virtual characters by a Brazilian television employing deep learning. Participants analyzed how technology impacts journalism values, norms, and practices, focusing on aligning synthetic media technologies with automated fact-checking dissemination. In conclusion, the authors prepared recommendations from valuable insights into the complex ethical considerations surrounding synthetic media technologies for automatic fact-checking dissemination. It also facilitated cross-border discussions, with 11 participants from seven countries engaging in fruitful dialogue on this vital topic. The study proposed evaluation criteria for AI-generated content in this diversity, including privacy protection, inclusiveness, transparency, beauty standards conformity, engagement, meaningfulness, and effortlessness.","https://link.springer.com/content/pdf/10.1007/s43681-024-00619-y.pdf",""
0,"Rawan AlMakinah, Mahsa Goodarzi, Betul Tok, M. Abdullah Canbaz","Mapping artificial intelligence bias: a network-based framework for analysis and mitigation",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00609-0","",592,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00609-0","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00609-0.pdf",""
0,"June Jeon, Jaehyuk Park, Lanu Kim","The Ethics of Generative AI in Social-Scientific Research: A Qualitative Approach for Community-Based AI Ethics",2024,"SSRN Electronic Journal","Elsevier BV","https://doi.org/10.2139/ssrn.4703377","",593,"2025-02-04 16:55:17","journal-article","10.2139/ssrn.4703377","1556-5068","",,,,,0,0.00,0,3,1,"","",""
0,"Rashmi Nagpal, Rasoul Shahsavarifar, Vaibhav Goyal, Amar Gupta","Author Correction: Optimizing fairness and accuracy: a pareto optimal approach for decision-making",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00538-y","",596,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00538-y","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00538-y.pdf",""
0,"John Ratzan, Noushi Rahman","Measuring responsible artificial intelligence (RAI) in banking: a valid and reliable instrument",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00321-5","",598,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00321-5","2730-5953","",4,4,1279,1297,0,0.00,0,2,2,"Abstract: Widespread use of artificial intelligence (AI) and machine learning (ML) in the US banking industry raises red flags with regulators and social groups due to potential risk of data-driven algorithmic bias in credit lending decisions. The absence of a valid and reliable measure of responsible AI (RAI) has stunted the growth of organizational research on RAI (i.e., the organizational balancing act to optimize efficiency and equity). To address this void, we develop a novel measurement instrument to assess RAI maturity in firms. A review of the nascent literature reveals that there is a wide distribution of RAI capabilities. The RAI instrument that we advance is based on the exhaustive review of this dispersed literature. Analyses of data from large US banks show strong evidence of validity and reliability of the RAI maturity instrument.","https://link.springer.com/content/pdf/10.1007/s43681-023-00321-5.pdf",""
0,"James Steinhoff","AI Ethics as Subordinated Innovation Network",2023,"","Center for Open Science","https://doi.org/10.33767/osf.io/qsxj3","",599,"2025-02-04 16:55:17","posted-content","10.33767/osf.io/qsxj3","","",,,,,0,0.00,0,1,2,"<p>AI ethics is proposed, by the Big Tech companies which lead AI research and development, as the cure for diverse social problems posed by the commercialization of data-intensive technologies. It aims to reconcile capitalist AI production with ethics. However, AI ethics is itself now the subject of wide criticism; most notably, it is accused of being no more than “ethics washing” - a cynical means of dissimulation for Big Tech, while it continues its business operations unchanged. This paper aims to critically assess, and go beyond the ethics washing thesis. I argue that AI ethics is indeed ethics washing, but not only that. It has a more significant economic function for Big Tech. To make this argument I draw on the theory of intellectual monopoly capital. I argue that ethics washing is better understood as a subordinated innovation network: a dispersed network of contributors beyond Big Tech’s formal employment whose research is indirectly planned by Big Tech, which also appropriates its results. These results are not intended to render AI more ethical, but rather to advance the business processes of intellectual monopoly capitals. Because the parameters of AI ethics are indirectly set in advance by Big tech, the ostensible goal that AI ethics sets for itself-to resolve the contradiction between business and ethics-is in fact insoluble. I demonstrate this via an analysis of the latest trend in AI ethics: the operationalization of ethical principles.</p>","",""
0,"Ahmed Banafa","Generative AI: Types, Skills, Opportunities, and Challenges",2024,"Transformative AI","River Publishers","https://doi.org/10.1201/9781032669182-8","",603,"2025-02-04 16:55:17","book-chapter","10.1201/9781032669182-8","","",,,41,48,0,0.00,0,1,1,"","",""
0,"André Renz","AI in Education: Educational Technology and AI",2023,"Work and AI 2030","Springer Fachmedien Wiesbaden","https://doi.org/10.1007/978-3-658-40232-7_39","",605,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-658-40232-7_39","","",,,353,360,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-3-658-40232-7_39",""
0,"Kristina Šekrst, Jeremy McHugh, Jonathan Rodriguez Cefalù","AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development",2024,"","Qeios Ltd","https://doi.org/10.32388/6oe6ni","",606,"2025-02-04 16:55:17","posted-content","10.32388/6oe6ni","","",,,,,0,0.00,0,3,1,"This paper explores the development of an ethical guardrail framework for AI systems, emphasizing the importance of customizable guardrails that align with diverse user values and underlying ethics. We address the challenges of AI ethics by proposing a structure that integrates rules, policies, and AI assistants to ensure responsible AI behavior, while comparing the proposed framework to the existing state-of-the-art guardrails. By focusing on practical mechanisms for implementing ethical standards, we aim to enhance transparency, user autonomy, and continuous improvement in AI systems. Our approach accommodates ethical pluralism, offering a flexible and adaptable solution for the evolving landscape of AI governance. The paper concludes with strategies for resolving conflicts between ethical directives, underscoring the present and future need for robust, nuanced and context-aware AI systems.","https://www.qeios.com/read/6OE6NI/pdf",""
0,"Abhishek Mishra","Scalable AI Governance and Ethics",2024,"Scalable AI and Design Patterns","Apress","https://doi.org/10.1007/979-8-8688-0158-7_9","",607,"2025-02-04 16:55:17","book-chapter","10.1007/979-8-8688-0158-7_9","","",,,147,165,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/979-8-8688-0158-7_9",""
0,"Ashok K. Goel","Looking back, looking ahead: Humans, ethics, and AI",2022,"AI Magazine","Wiley","https://doi.org/10.1002/aaai.12052","",608,"2025-02-04 16:55:17","journal-article","10.1002/aaai.12052","0738-4602","",43,2,267,269,0,0.00,0,1,3,"","https://onlinelibrary.wiley.com/doi/pdf/10.1002/aaai.12052",""
0,"Yong Suk Lee","Optimizing whose engagement? Beliefs and protest participation of social media users in South Korea",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00322-4","",610,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00322-4","2730-5953","",4,4,1309,1321,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00322-4.pdf",""
0,"Christian Sieberichs, Simon Geerkens, Alexander Braun, Thomas Waschulzik","Correction: ECS: an interactive tool for data quality assurance",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00429-2","",611,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00429-2","2730-5953","",4,4,1583,1583,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00429-2.pdf",""
0,"Priyanka Mathur, Asha Mamraj Sharma","Robo-Advisors in Investment Management",2024,"Advances in Finance, Accounting, and Economics","IGI Global","https://doi.org/10.4018/979-8-3693-2185-0.ch006","",612,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-2185-0.ch006","2327-5677","",,,121,145,0,0.00,0,2,1,"The chapter on robo-advisors in investment management explores the multifaceted landscape of automated investment platforms, shedding light on the hurdles and apprehensions associated with their integration into the financial industry. Delving into issues such as the absence of human touch, regulatory compliance complexities, technological risks, algorithmic challenges, market volatility considerations, and the imperative need for client education, the chapter offers a comprehensive examination of the challenges and concerns that both investors and industry participants encounter in the realm of robo-advisors. By delineating these challenges, the chapter aims to contribute to a nuanced understanding of the dynamics shaping the adoption and evolution of robo-advisory services, robust risk management strategies to navigate market uncertainties, and the essential role of client education in fostering understanding and trust.","https://www.igi-global.com/viewtitle.aspx?TitleId=352614",""
0,"Willian Dihanster Gomes de Oliveira, Lilian Berton","A comparative study of pre-processing algorithms for fair classification in few labeled data context",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00601-8","",613,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00601-8","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00601-8.pdf",""
0,"Joseph Chapa","Military AI Ethics",2024,"Journal of Military Ethics","Informa UK Limited","https://doi.org/10.1080/15027570.2024.2439654","",616,"2025-02-04 16:55:17","journal-article","10.1080/15027570.2024.2439654","1502-7570","",23,3,306,321,0,0.00,0,1,1,"","https://www.tandfonline.com/doi/pdf/10.1080/15027570.2024.2439654",""
0,"Luke Haliburton, Jan Leusmann, Robin Welsch, Sinksar Ghebremedhin, Petros Isaakidis, Albrecht Schmidt, Sven Mayer","Uncovering labeler bias in machine learning annotation tasks",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00572-w","",618,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00572-w","2730-5953","",,,,,0,0.00,0,7,1,"Abstract: As artificial intelligence becomes increasingly pervasive, it is essential that we understand the implications of bias in machine learning. Many developers rely on crowd workers to generate and annotate datasets for machine learning applications. However, this step risks embedding training data with labeler bias, leading to biased decision-making in systems trained on these datasets. To characterize labeler bias, we created a face dataset and conducted two studies where labelers of different ethnicity and sex completed annotation tasks. In the first study, labelers annotated subjective characteristics of faces. In the second, they annotated images using bounding boxes. Our results demonstrate that labeler demographics significantly impact both subjective and accuracy-based annotations, indicating that collecting a diverse set of labelers may not be enough to solve the problem. We discuss the consequences of these findings for current machine learning practices to create fair and unbiased systems.","https://link.springer.com/content/pdf/10.1007/s43681-024-00572-w.pdf",""
0,"Jesper Ryberg","Artificial intelligence and criminal justice: How to use algorithmic sentencing support in real life (and ethically non-ideal) penal systems?",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00655-8","",619,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00655-8","2730-5953","",,,,,0,0.00,0,1,1,"Abstract: The use of artificial intelligence as an instrument to assist judges in determining sentences in criminal cases is attracting increasing theoretical attention. While many theorists have argued that there may be important advantages to introducing algorithmic sentencing support in criminal cases, almost no one has considered how such systems should actually be implemented. The purpose of this chapter is to fill this void. First, it is argued that current penal practice is non-ideal in the sense that it is dominated by overpunishment of offenders (the overpunishment assumption), and that algorithmic sentencing support systems are unlikely to be introduced in a way that appears to disturb the existing penal order (the preservation assumption). Second, a model called the “Restricted Application Model” is presented for how such algorithms might be used by judges within a framework characterized by the two outlined assumptions. Third, three objections to the model are considered and ultimately rejected. Thus, the model serves as a first attempt at outlining a procedure for the use of sentencing advisory systems by judges within real-life, and ethically non-ideal, penal systems.","https://link.springer.com/content/pdf/10.1007/s43681-024-00655-8.pdf",""
0,"Ali-Reza Bhojani","Between Fear and Hope:",2023,"The Promise and Peril of AI and IA","ATF Press","https://doi.org/10.2307/jj.24873339.19","",621,"2025-02-04 16:55:17","book-chapter","10.2307/jj.24873339.19","","",,,245,256,0,0.00,0,1,2,"","",""
0,"Hubert Etienne, Brent Mittelstadt, Rob Reich, John Basl, Jeff Behrends, Dominique Lestel, Chloé Bakalar, Geoff Keeling, Giada Pistilli, Marta Cantero Gamito","Exploring the mutations of society in the era of generative AI",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00632-1","",622,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00632-1","2730-5953","",,,,,0,0.00,0,10,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00632-1.pdf",""
0,"Kubilay Muhammed Sunnetci, Ahmet Alkan, Faruk Enes Oguz, Mahmut Nedim Ekersular","Performance and model behavior analysis from different perspectives of Bing Chat",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00540-4","",626,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00540-4","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00540-4.pdf",""
0,"María Carolina Jiménez","The Opacity of Automated Decision-Making Systems (ADMS) and its Challenges for Political Legitimacy in a Democracy",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3539563","",629,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3539563","","",,,901,901,0,0.00,0,1,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3539563",""
0,"","AI - Ethical and Legal Challenges [Working Title]",2024,"","IntechOpen","https://doi.org/10.5772/intechopen.1002134","",630,"2025-02-04 16:55:17","edited-book","10.5772/intechopen.1002134","","",,,,,0,0.00,0,0,1,"","",""
0,"Elena Falletti","Surfing reality, hype, and propaganda: an empirical comparative analysis on predictive software in criminal justice",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00447-0","",631,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00447-0","2730-5953","",4,3,819,831,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00447-0.pdf",""
0,"Jeff J.H. Kim, Adith V. Srivatsa, George R. Nahass, Timur Rusanov, Soonmyung Hwang, Soohyun Kim, Itay Solomon, Tae Ha Lee, Shrinidhi Kadkol, Olusola Ajilore, Yang Dai","Generative AI can effectively manipulate data",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00546-y","",635,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00546-y","2730-5953","",,,,,0,0.00,0,11,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00546-y.pdf",""
0,"Arlette Danielle Román Almánzar, David Joachim Grüning, Laura Marie Edinger-Schons","‘It wasn’t me’: the impact of social responsibility and social dominance attitudes on AI programmers’ moral imagination (intention to correct bias)",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00516-4","",637,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00516-4","2730-5953","",,,,,0,0.00,0,3,1,"Abstract: A plethora of research has shed light on AI’s perpetuation of biases, and the primary focus has been on technological fixes or biased data. However, there is deafening silence regarding the key role of programmers in mitigating bias in AI. A significant gap exists in the understanding of how a programmer’s personal characteristics may influence their professional design choices. This study addresses this gap by exploring the link between programmers’ sense of social responsibility and their moral imagination in AI, i.e., intentions to correct bias in AI, particularly against marginalized populations. Furthermore, it is unexplored how a programmer’s preference for hierarchy between groups, social dominance orientation-egalitarianism (SDO-E), influences this relationship. We conducted a between-subject online experiment with 263 programmers based in the United States. They were randomly assigned to conditions that mimic narratives about agency reflected in technology determinism (low responsibility) and technology instrumentalism (high responsibility). The findings reveal that high social responsibility significantly boosts programmers’ moral imagination concerning their intentions to correct bias in AI, and it is especially effective for high SDO-E programmers. In contrast, low SDO-E programmers exhibit consistently high levels of moral imagination in AI, regardless of the condition, as they are highly empathetic, allowing the perspective-taking needed for moral imagination, and are naturally motivated to equalize groups. This study underscores the need to cultivate social responsibility among programmers to enhance fairness and ethics in the development of artificial intelligence. The findings have important theoretical and practical implications for AI ethics, algorithmic fairness, etc.","https://link.springer.com/content/pdf/10.1007/s43681-024-00516-4.pdf",""
0,"Gleb Papyshev, Keith Jin Deng Chan","Fugazi regulation for AI: strategic tolerance for ethics washing",2024,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-02084-x","",638,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-02084-x","0951-5666","",,,,,0,0.00,0,2,1,"Abstract: Regulation theory offers a unique perspective on the institutional aspects of digital capitalism’s accumulation regime. However, a gap exists in examining the associated mode of regulation. Based on the analysis of AI ethics washing phenomenon, we suggest the state is delicately balancing between fueling innovation and reducing uncertainty in emerging technologies. This balance leads to a unique mode of regulation, ""Fugazi regulation,"" characterized by vaguely defined, non-enforceable moral principles with no specific implementation mechanisms. We propose a microeconomic model that rationalizes this approach and shows that it is justifiable when the government struggles to differentiate between benign and harmful technology use due to capacity constraints. The potential for private companies to adopt ethical practices under Fugazi regulation supports the government’s preference for this method. This regulation mode is particularly attractive to the government during technology’s early development stages, marked by governmental optimism and uncertainty about the technology. Implications for greenwashing are also derived from the analysis.","https://link.springer.com/content/pdf/10.1007/s00146-024-02084-x.pdf",""
0,"Basheer Riskhan, Halawati Abd Jalil Saufan, Jazuli Bello Ladan, Md Amin Ullah Sheikh, Khalid Hussain, Manzoor Hussain","Reshaping Secure Coding Through Generative AI Approach to Minimizing Programming Challenges",2024,"Advances in Information Security, Privacy, and Ethics","IGI Global","https://doi.org/10.4018/979-8-3693-5415-5.ch008","",639,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-5415-5.ch008","1948-9730","",,,265,280,0,0.00,0,6,1,"A combination of technical expertise, creativity, and problem-solving skills is needed to succeed in the complicated and demanding profession of programming. As a result, there are several challenges that programmers may run against when creating software or computer systems. The literature on how to optimize and reduce the problems and difficulties in computer programming is reviewed in this chapter. The issue has a global scope and keeps becoming worse on a local scale. Even though there are numerous instructional tools available to support the teaching and learning of computer programming, the issue still exists. Computer introduction courses had high failure and dropout rates even from the beginning. This situation's justification includes the student's inability to solve problems. To overcome the challenges of learning computer programming, these two factors must be taken into account concurrently. This chapter will find out the ways to minimize these challenges.","https://www.igi-global.com/viewtitle.aspx?TitleId=356776",""
0,"Kirsten Martin","Ethics, AI, Research, and Corporations",2022,"Ethics of Data and Analytics","Auerbach Publications","https://doi.org/10.1201/9781003278290-63","",641,"2025-02-04 16:55:17","book-chapter","10.1201/9781003278290-63","","",,,429,433,0,0.00,0,1,3,"","",""
0,"","Acknowledgments",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00010-x","",643,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00010-x","","",,,,,0,0.00,0,0,1,"","https://api.elsevier.com/content/article/PII:B978044318851000010X",""
0,"","Copyright",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00024-x","",644,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00024-x","","",,,,,0,0.00,0,0,1,"","https://api.elsevier.com/content/article/PII:B978044318851000024X",""
0,"","The ethics dimension",2022,"AI on Trial","Bloomsbury Professional","https://doi.org/10.5040/9781526513588.chapter-004","",649,"2025-02-04 16:55:17","book-chapter","10.5040/9781526513588.chapter-004","","",,,,,0,0.00,0,0,3,"","",""
0,"Dan Hendrycks","Introduction to AI Safety, Ethics, and Society",2024,"","CRC Press","https://doi.org/10.1201/9781003530336","",650,"2025-02-04 16:55:17","monograph","10.1201/9781003530336","","",,,,,0,0.00,0,1,1,"","",""
0,"Bauke Wielinga","Complex equality and the abstractness of statistical fairness: using social goods to analyze a CV scanner and a welfare fraud detector",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00384-4","",653,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00384-4","2730-5953","",,,,,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00384-4.pdf",""
0,"Jayant Verma, D. Lakshmi","Advancements in Ransomware Detection and Prevention Techniques",2024,"Advances in Finance, Accounting, and Economics","IGI Global","https://doi.org/10.4018/979-8-3693-2185-0.ch009","",659,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-2185-0.ch009","2327-5677","",,,192,223,0,0.00,0,2,1,"In today's world of ubiquitous sensors and intelligent devices, cyber incidents and crime have peaked. Ransomware poses a danger to the security of a computer system. Ransomware attacks have significantly increased over the past ten years. Inevitably, this has become the talk of the town quite extortionate due to considerable consequential damages and obstruction in sectors such as healthcare, insurance, business, and education. Automatic detection and prevention of ransomware attacks is a crucial aspect of cybersecurity. Various malware detection methods have still been unturned as new parts of malware emerge. In the last two decades, several machine-learning algorithms and behavior-based techniques have been developed to identify ransomware anomalies. This chapter provides a long-term understanding of ransomware and discusses current methods and advancements in ransomware detection and the phases of a ransomware attack. The authors also highlighted the brief history of ransomware from 1989 when the first ransomware was discovered to the recent year 2023.","https://www.igi-global.com/viewtitle.aspx?TitleId=352617",""
0,"Mahmoud Kamal Abouraia","Artificial Intelligence in Finance",2024,"Advances in Finance, Accounting, and Economics","IGI Global","https://doi.org/10.4018/979-8-3693-2185-0.ch002","",660,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-2185-0.ch002","2327-5677","",,,17,34,0,0.00,0,1,1,"This chapter provides a comprehensive overview of the opportunities and risks associated with the integration of artificial intelligence (AI) in the domains of banking, investments, and microfinance. Through a detailed analysis of various data sets, case studies, and industry reports, the research highlights the significant impact of AI on enhancing customer satisfaction, improving investment portfolio performance, and promoting financial inclusion for underserved communities. The findings underscore the transformative potential of AI in driving operational efficiency, mitigating risks, and fostering innovation within the financial sector. However, the discussions also underscore the challenges related to data security, ethical implications, regulatory compliance, and workforce transitions that accompany AI integration. The abstract emphasizes the importance of implementing robust governance frameworks, ethical guidelines, and continuous skill development initiatives to ensure responsible AI deployment and sustainable growth in the financial industry.","https://www.igi-global.com/viewtitle.aspx?TitleId=352610",""
0,"Lakshmanan Sethu Sankaranarayanan","The global AI framework: navigating challenges and societal impacts",2024,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-02070-3","",661,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-02070-3","0951-5666","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s00146-024-02070-3.pdf",""
0,"","Challenges of Socially Responsible AI",2023,"Socially Responsible AI","WORLD SCIENTIFIC","https://doi.org/10.1142/9789811266638_0004","",663,"2025-02-04 16:55:17","book-chapter","10.1142/9789811266638_0004","","",,,123,159,0,0.00,0,0,2,"","",""
0,"Tarnveer Singh","The Emergence of AI Ethics",2024,"Artificial Intelligence and Ethics","CRC Press","https://doi.org/10.1201/9781003502708-31","",668,"2025-02-04 16:55:17","book-chapter","10.1201/9781003502708-31","","",,,219,222,0,0.00,0,1,1,"","",""
0,"Jeff J. H. Kim, Adith V. Srivatsa, George R. Nahass, Timur Rusanov, Soonmyung Hwang, Soohyun Kim, Itay Solomon, Tae Ha Lee, Shrinidhi Kadkol, Olusola Ajilore, Yang Dai","Correction: Generative AI can effectively manipulate data",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00630-3","",671,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00630-3","2730-5953","",,,,,0,0.00,0,11,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00630-3.pdf",""
0,"Jeff J. H. Kim, Adith V. Srivatsa, George R. Nahass, Timur Rusanov, Soonmyung Hwang, Soohyun Kim, Itay Solomon, Tae Ha Lee, Shrinidhi Kadkol, Olusola Ajilore, Yang Dai","Publisher Correction: Generative AI can effectively manipulate data",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00579-3","",672,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00579-3","2730-5953","",,,,,0,0.00,0,11,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00579-3.pdf",""
0,"Junga Ko, Aeri Song","Youth perceptions of AI ethics: a Q methodology approach",2024,"Ethics &amp; Behavior","Informa UK Limited","https://doi.org/10.1080/10508422.2024.2396582","",674,"2025-02-04 16:55:17","journal-article","10.1080/10508422.2024.2396582","1050-8422","",,,1,18,0,0.00,0,2,1,"","https://www.tandfonline.com/doi/pdf/10.1080/10508422.2024.2396582",""
0,"Zichong Wang, Zhibo Chu, Thang Viet Doan, Shiwen Ni, Min Yang, Wenbin Zhang","History, development, and principles of large language models: an introductory survey",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00583-7","",675,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00583-7","2730-5953","",,,,,0,0.00,0,6,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00583-7.pdf",""
0,"Hubert Etienne, Florian Cova","The more they think, the less they want: studying people’s attitudes about autonomous vehicles could also contribute to shaping them",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00385-3","",676,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00385-3","2730-5953","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00385-3.pdf",""
0,"Fatemeh Habibi, Sadaf Sedaghatshoar, Tahereh Attar, Marzieh Shokoohi, Arash Kiani, Ali Naderi Malek","Revolutionizing education and therapy for students with autism spectrum disorder: a scoping review of AI-driven tools, technologies, and ethical implications",2025,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00608-1","",680,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00608-1","2730-5953","",,,,,0,0.00,0,6,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00608-1.pdf",""
0,"Katja Thieme, Mary Ann S. Saunders, Laila Ferreira","From language to algorithm: trans and non-binary identities in research on facial and gender recognition",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00375-5","",681,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00375-5","2730-5953","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00375-5.pdf",""
0,"Umair Rehman, Muhammad Umair Shah, Farkhund Iqbal, Ramsha Fatima","Comparative analysis of moral decision-making and trust dynamics: human reasoning vs. ChatGPT-3 narratives",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00605-4","",683,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00605-4","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00605-4.pdf",""
0,"Paula Boddington","AI Ethics and Ethical AI",2024,"The Cambridge Companion to Religion and Artificial Intelligence","Cambridge University Press","https://doi.org/10.1017/9781009031721.012","",687,"2025-02-04 16:55:17","book-chapter","10.1017/9781009031721.012","","",,,165,181,0,0.00,0,1,1,"","",""
0,"Roberta Spallone","Representation Challenges: Searching for New Frontiers of AR and AI Research",2022,"Representation Challenges","FrancoAngeli srl","https://doi.org/10.3280/oa-845-c190","",688,"2025-02-04 16:55:17","book-chapter","10.3280/oa-845-c190","","",,,,,0,0.00,0,1,3,"","",""
0,"Mikhail Khorkov","AI AND NEW PERSPECTIVES IN THE HISTORY OF PHILOSOPHY",2024,"Recasting Bioethics, Nueroethics, and AI Ethics","Editora Fundação Fênix","https://doi.org/10.36592/9786554602006-11","",689,"2025-02-04 16:55:17","book-chapter","10.36592/9786554602006-11","","",,,225,237,0,0.00,0,1,1,"","",""
0,"Jeffrey White","Augmenting morality through ethics education: the ACTWith model",2024,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-01864-9","",690,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-01864-9","0951-5666","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s00146-024-01864-9.pdf",""
0,"Azeem Khan, Noor Jhanjhi, Dayang H. T. B. A. Haji Hamid, Haji Abdul Hafidz B. Haji Omar, Fathi Amsaad, Sobia Wassan","Future Trends and Challenges in Cybersecurity and Generative AI",2024,"Advances in Information Security, Privacy, and Ethics","IGI Global","https://doi.org/10.4018/979-8-3693-5415-5.ch014","",691,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-5415-5.ch014","1948-9730","",,,491,522,0,0.00,0,6,1,"The chapter presents a comprehensive exploration of the changing dynamics at the intersection between the rapidly growing landscape of the interconnectivity of various devices—the internet of things—and the innovations piloted by advancements in generative artificial intelligence. In the following background-focused analysis, the significance of the enactment of new levels of security details in this fast-growing and virulently expansive landscape is emphasized, with generative AI ultimately serving as the highlight. The conversation consequently shifts to threats. This includes a detailed depiction of new cybersecurity threats rooted in advancements in AI, featuring AI malicious actors and incidents, such as the increasingly popular phenomenon of ransomware-as-a-service as mirror illustrations of the dynamic and multifaceted character of these threats. The class further proceeds to more in-depth detail about the most contemporary generative AI platforms such as generative adversarial networks, variational autoencoders, and reinforcement learning—all relevant in identifying emerging solutions to advance strategies in cybersecurity. The conversation simultaneously conducts an opportunity and threat analysis of the merger between these platforms and cybersecurity with regard to ethics, regulations, and overall adversarial touchpoints and tactics. The chapter concludes with a call for unity in discourse and action between the relevant industry, academia, and government stakeholders as a summary of the essential cross-disciplinary aspect that must drive the narrative in confronting and overcoming the threats to and from generative AI research. Having presented the narrative structure, this chapter has allowed a comprehensive coverage of the major issues and opportunities at the heart of the cybersecurity-generative AI combination. Additionally, it has provided a forum to call for collaborative and fortified efforts regarding the securing and defending of the uncertainties that the rapidly changing and more unpredictable digital landscape has in store for the world.","https://www.igi-global.com/viewtitle.aspx?TitleId=356782",""
0,"Catriona Campbell","Current State of AI Ethics",2022,"AI by Design","Chapman and Hall/CRC","https://doi.org/10.1201/9781003267003-5","",692,"2025-02-04 16:55:17","book-chapter","10.1201/9781003267003-5","","",,,59,70,0,0.00,0,1,3,"","",""
0,"Klaus Haller","Ethics, Regulations, and Explainability",2022,"Managing AI in the Enterprise","Apress","https://doi.org/10.1007/978-1-4842-7824-6_4","",693,"2025-02-04 16:55:17","book-chapter","10.1007/978-1-4842-7824-6_4","","",,,85,105,0,0.00,0,1,3,"","https://link.springer.com/content/pdf/10.1007/978-1-4842-7824-6_4",""
0,"David S. Watson, Jakob Mökander, Luciano Floridi","Competing narratives in AI ethics: a defense of sociotechnical pragmatism",2024,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-02128-2","",694,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-02128-2","0951-5666","",,,,,0,0.00,0,3,1,"Abstract: Several competing narratives drive the contemporary AI ethics discourse. At the two extremes are","https://link.springer.com/content/pdf/10.1007/s00146-024-02128-2.pdf",""
0,"","Preemptive Bioethics and Syncretist Approach for Ethical Challenges in AI and Robotics",2023,"Advances in Medical Education, Research, and Ethics","IGI Global","https://doi.org/10.4018/978-1-6684-4808-3.ch002","",696,"2025-02-04 16:55:17","book-chapter","10.4018/978-1-6684-4808-3.ch002","2475-6601","",,,24,34,0,0.00,0,0,2,"This chapter explores the main views of bioethics and their applications in the context of displacement and emerging technologies. It includes ethical implications and outcomes before deploying technologies like AI through preemptive bioethics. The author discusses the four ethical decision-making approaches: legalist, antinomian, situationist, and syncretist. Syncretist approach is related with proactive ethical considerations, integration of perspectives, and collaboration between humans and AI. Emphasizing proactive considerations and collaboration between humans and AI systems, the chapter delves into the complexity of bioethical issues from a mathematical perspective. The chapter concludes by assessing the future impact of bioinformatic privacy and the need for comprehensive frameworks and interdisciplinary approaches.","https://www.igi-global.com/viewtitle.aspx?TitleId=325088",""
0,"Wenbin Zhang","AI fairness in practice: Paradigm, challenges, and prospects",2024,"AI Magazine","Wiley","https://doi.org/10.1002/aaai.12189","",697,"2025-02-04 16:55:17","journal-article","10.1002/aaai.12189","0738-4602","",45,3,386,395,0,0.00,0,1,1,"Abstract: Understanding and correcting algorithmic bias in artificial intelligence (AI) has become increasingly important, leading to a surge in research on AI fairness within both the AI community and broader society. Traditionally, this research operates within the constrained supervised learning paradigm, assuming the presence of class labels, independent and identically distributed (IID) data, and batch‐based learning necessitating the simultaneous availability of all training data. However, in practice, class labels may be absent due to censoring, data is often represented using non‐IID graph structures that capture connections among individual units, and data can arrive and evolve over time. These prevalent real‐world data representations limit the applicability of existing fairness literature, which typically addresses fairness in static and tabular supervised learning settings. This paper reviews recent advances in AI fairness aimed at bridging these gaps for practical deployment in real‐world scenarios. Additionally, opportunities are envisioned by highlighting the limitations and significant potential for real applications.","https://onlinelibrary.wiley.com/doi/pdf/10.1002/aaai.12189",""
0,"Ishana Shastri, Shomik Jain, Barbara Engelhardt, Ashia Wilson","Automating Transparency Mechanisms in the Judicial System Using LLMs: Opportunities and Challenges",2024,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i1.31729","",700,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i1.31729","3065-8365","",7,,1357,1367,0,0.00,0,4,1,"Bringing more transparency to the judicial system for the purposes of increasing accountability often demands extensive effort from auditors who must meticulously sift through numerous disorganized legal case files to detect patterns of bias and errors. For example, the high-profile investigation into the Curtis Flowers case took seven reporters a full year to assemble evidence about the prosecutor's history of selecting racially biased juries. LLMs have the potential to automate and scale these transparency pipelines, especially given their demonstrated capabilities to extract information from unstructured documents. We discuss the opportunities and challenges of using LLMs to provide transparency in two important court processes: jury selection in criminal trials and housing eviction cases.","https://ojs.aaai.org/index.php/AIES/article/download/31729/33896",""
0,"Ishana Shastri, Shomik Jain, Barbara Engelhardt, Ashia Wilson","Automating Transparency Mechanisms in the Judicial System Using LLMs: Opportunities and Challenges",2024,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i1.31729","",702,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i1.31729","3065-8365","",7,,1357,1367,0,0.00,0,4,1,"Bringing more transparency to the judicial system for the purposes of increasing accountability often demands extensive effort from auditors who must meticulously sift through numerous disorganized legal case files to detect patterns of bias and errors. For example, the high-profile investigation into the Curtis Flowers case took seven reporters a full year to assemble evidence about the prosecutor's history of selecting racially biased juries. LLMs have the potential to automate and scale these transparency pipelines, especially given their demonstrated capabilities to extract information from unstructured documents. We discuss the opportunities and challenges of using LLMs to provide transparency in two important court processes: jury selection in criminal trials and housing eviction cases.","https://ojs.aaai.org/index.php/AIES/article/download/31729/33896",""
0,"","Preface",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00016-0","",703,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00016-0","","",,,,,0,0.00,0,0,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000160",""
0,"Nythamar Oliveira, Jair Tauchen","Recasting Bioethics, Nueroethics, and AI Ethics",2024,"","Editora Fundação Fênix","https://doi.org/10.36592/9786554602006","",704,"2025-02-04 16:55:17","edited-book","10.36592/9786554602006","","",,,,,0,0.00,0,2,1,"","",""
0,"Georg Starke","Introduction: Navigating ethics at the intersection of AI and neuroscience",2024,"Developments in Neuroethics and Bioethics","Elsevier","https://doi.org/10.1016/s2589-2959(24)00041-9","",705,"2025-02-04 16:55:17","book-chapter","10.1016/s2589-2959(24)00041-9","2589-2959","",,,,,0,0.00,0,1,1,"","https://api.elsevier.com/content/article/PII:S2589295924000419",""
0,"Alexander Kriebitz, Caitlin Corrigan, Auxane Boch, Katherine D. Evans","Decoding the EU AI Act in the context of ethics and fundamental rights",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00014","",709,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00014","","",,,123,152,0,0.00,0,4,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Rose Luckin, Karine George, Mutlu Cukurova","Educational challenges and AI",2022,"AI for School Teachers","CRC Press","https://doi.org/10.1201/9781003193173-2","",710,"2025-02-04 16:55:17","book-chapter","10.1201/9781003193173-2","","",,,17,32,0,0.00,0,3,3,"","",""
0,"Matthias Schindler, Frederik Schmihing","Technology Serves People: Democratising Analytics and AI in the BMW Production System",2023,"CSR, Sustainability, Ethics &amp; Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-09245-9_7","",711,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09245-9_7","2196-7075","",,,159,182,0,0.00,0,2,2,"","https://link.springer.com/content/pdf/10.1007/978-3-031-09245-9_7",""
0,"Naresh Tiwari","Transformative AI in Biomedicine Analysis",2024,"Advances in Information Security, Privacy, and Ethics","IGI Global","https://doi.org/10.4018/979-8-3693-9311-6.ch006","",712,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-9311-6.ch006","1948-9730","",,,167,262,0,0.00,0,1,1,"This chapter provides a comprehensive review of artificial intelligence (AI) applications in biomedicine, highlighting the transformative impact on various domains, from basic research to clinical practice. The author explores AI's role in medical imaging and diagnostics, drug discovery and development, genomics and precision medicine, and healthcare management and delivery. Key advancements, such as deep learning for image analysis, virtual screening for drug design, and AI-driven patient stratification, are discussed. The chapter also addresses challenges surrounding AI implementation, including data access, bias, scalability, transparency, privacy, and regulatory uncertainties. Potential solutions and policy options to address these challenges and enhance AI's benefits are proposed. The author emphasizes the importance of collaboration between AI experts and healthcare stakeholders, as well as the need for responsible AI development practices. Future directions highlight the potential for AI to transform healthcare and improve patient outcomes and need for responsible AI.","https://www.igi-global.com/viewtitle.aspx?TitleId=361302",""
0,"Ahmed Banafa","Generative AI: Types, Skills, Opportunities and Challenges",2024,"Introduction to Artificial Intelligence (AI)","River Publishers","https://doi.org/10.1201/9781003499527-7","",713,"2025-02-04 16:55:17","book-chapter","10.1201/9781003499527-7","","",,,37,43,0,0.00,0,1,1,"","",""
0,"Eliott Py, Elies Gherbi, Nelson Fernandez Pinto, Martin Gonzalez, Hatem Hajri","Real-time weather monitoring and desnowification through image purification",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00418-5","",715,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00418-5","2730-5953","",4,1,75,82,0,0.00,0,5,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00418-5.pdf",""
0,"Michal Pruski","Ethics Framework for Predictive Clinical Ai Model Updating",2023,"","Elsevier BV","https://doi.org/10.2139/ssrn.4385658","",717,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.4385658","","",,,,,0,0.00,0,1,2,"","",""
0,"","Contents",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00025-1","",718,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00025-1","","",,,,,0,0.00,0,0,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000251",""
0,"","Index",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00027-5","",719,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00027-5","","",,,383,397,0,0.00,0,0,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000275",""
0,"Jisoo Park","Appeared in AI educational books for lower grades AI content element analysis and critical review",2024,"The Korean Society for Artificial Intelligence Ethics","The Korean Society for Artificial Intelligence Ethics","https://doi.org/10.59728/jaie.2024.3.1.9","",724,"2025-02-04 16:55:17","journal-article","10.59728/jaie.2024.3.1.9","","",3,1,9,23,0,0.00,0,1,1,"In this study, we attempted to analyze the AI literacy content elements found in artificial intelligence textbooks, focusing on educational books for lower grades, and seek a direction for developing artificial intelligence educational books for lower grades through critical consideration. As a result of this study, it was found that there was some bias in the proportion of common areas of elementary school artificial intelligence education subjects shown in artificial intelligence literacy education books for lower grades. Elementary schools are places where basic artificial intelligence literacy education must be provided to school-age students entering the era of artificial intelligence. Students in lower grades must also receive appropriate artificial intelligence literacy education tailored to their grade level of development. Therefore, even if it is a somewhat difficult and unfamiliar concept or theory without bias in each area, teaching and learning materials that take into account the developmental characteristics of lower grades should be developed and actively reflected in the elementary school artificial intelligence education content system.","",""
0,"Paul Scherz","Principles and Virtues in AI Ethics",2024,"Journal of Military Ethics","Informa UK Limited","https://doi.org/10.1080/15027570.2024.2440195","",725,"2025-02-04 16:55:17","journal-article","10.1080/15027570.2024.2440195","1502-7570","",23,3,251,263,0,0.00,0,1,1,"","https://www.tandfonline.com/doi/pdf/10.1080/15027570.2024.2440195",""
0,"Lavanya Singh","Automated Kantian Ethics",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3539527","",726,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3539527","","",,,915,915,0,0.00,0,1,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3539527",""
0,"Steven Gouveia","THE ETHICS OF ARTIFICIAL INTELLIGENCE IN MEDICINE: PRELIMINARY REMARKS",2024,"Recasting Bioethics, Nueroethics, and AI Ethics","Editora Fundação Fênix","https://doi.org/10.36592/9786554602006-13","",727,"2025-02-04 16:55:17","book-chapter","10.36592/9786554602006-13","","",,,249,270,0,0.00,0,1,1,"","",""
0,"Eduardo Navas","AI Ethics, Aesthetics, Art and Artistry",2024,"Handbook on the Ethics of Artificial Intelligence","Edward Elgar Publishing","https://doi.org/10.4337/9781803926728.00017","",728,"2025-02-04 16:55:17","book-chapter","10.4337/9781803926728.00017","","",,,173,186,0,0.00,0,1,1,"","https://www.elgaronline.com/view/book/9781803926728/9781803926728.xml",""
0,"Paul Kofman","Scoring the Ethics of AI Robo-Advice: Why We Need Gateways and Ratings",2024,"Journal of Business Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s10551-024-05753-5","",729,"2025-02-04 16:55:17","journal-article","10.1007/s10551-024-05753-5","0167-4544","",,,,,0,0.00,0,1,1,"Abstract: Unlike the many services already transformed by artificial intelligence (","https://link.springer.com/content/pdf/10.1007/s10551-024-05753-5.pdf",""
0,"Rahman Sharifzadeh","ChatGPT as Co-Author? AI and Research Ethics",2024,"ETHICS IN PROGRESS","Adam Mickiewicz University Poznan","https://doi.org/10.14746/eip.2024.1.8","",731,"2025-02-04 16:55:17","journal-article","10.14746/eip.2024.1.8","2084-9257","",15,1,155,173,0,0.00,0,1,1,"Should ChatGPT be viewed merely as a supportive tool for writers, or does it qualify as a co-author? As ChatGPT and similar language models are likely to become more prevalent in assisting with academic writing and research, it seems that we will face with two possibilities: an increase in ghostwriting that could finally undermine the integrity of the knowledge system, or the need to theoretical preparation to recognize the role of non-human contributors. Drawing on Actor-Network Theory, this article examines the question of whether this Chatbot meets, in principle, the requirements for co-authorship. Answering this question in affirmative, it delves into philosophical discussions concerning the agency, moral agency, and moral accountability of such technological entities.","https://pressto.amu.edu.pl/index.php/eip/article/download/40098/36310",""
0,"Jessica White, Katrina Stevens","Ethics of digital education",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00019","",733,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00019","","",,,252,289,0,0.00,0,2,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Oshri Bar-Gil, Tom Ron, Ofir Czerniak","Ai for the People? Embedding Ai Ethics in Hr and People Analytics Projects",2023,"","Elsevier BV","https://doi.org/10.2139/ssrn.4491697","",736,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.4491697","","",,,,,0,0.00,0,3,2,"","",""
0,"Enzo Fenoglio, Emre Kazim","AI explainability, interpretability, and transparency",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00010","",739,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00010","","",,,66,94,0,0.00,0,2,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"","AN OVERVIEW OF AI AND GENERATIVE AI – SETTING THE STAGE",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.9","",741,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.9","","",,,47,74,0,0.00,0,0,1,"","",""
0,"Zachary R. Calo","AI, medicine and Christian ethics",2024,"Research Handbook on Health, AI and the Law","Edward Elgar Publishing","https://doi.org/10.4337/9781802205657.00021","",742,"2025-02-04 16:55:17","book-chapter","10.4337/9781802205657.00021","","",,,219,233,0,0.00,0,1,1,"","https://www.elgaronline.com/downloadpdf/book/9781802205657/9781802205657.xml",""
0,"Roberto Cerina, Élise Rouméas","The democratic ethics of artificially intelligent polling",2025,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-02150-4","",743,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-02150-4","0951-5666","",,,,,0,0.00,0,2,1,"Abstract: This paper examines the democratic ethics of artificially intelligent polls. Driven by machine learning, AI electoral polls have the potential to generate predictions with an unprecedented level of granularity. We argue that their predictive power is potentially desirable for electoral democracy. We do so by critically engaging with four objections: (1) the privacy objection, which focuses on the potential harm of the collection, storage, and publication of granular data about voting preferences; (2) the autonomy objection, which argues that polls are an obstacle to independently formed judgments; (3) the tactical voting objection, which argues that voting strategically on the basis of polls is troublesome; and finally (4) the manipulation objection, according to which malicious actors could systematically bias predictions to alter voting behaviours.","https://link.springer.com/content/pdf/10.1007/s00146-024-02150-4.pdf",""
0,"Markus Rüther","The meaningfulness gap in AI ethics: a guide on how to think through a complex challenge",2024,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-01993-1","",745,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-01993-1","0951-5666","",,,,,0,0.00,0,1,1,"Abstract: Technological outsourcing is increasingly prevalent, with AI systems taking over many tasks once performed by humans. This shift has led to various discussions within AI ethics. A question that was largely ignored until recently, but is now increasingly being discussed, concerns the meaningfulness of such a lifestyle. The literature largely features skeptical views, raising several challenges. Many of these challenges can be grouped under what I identify as the “meaningfulness gap”. Although this gap is widely acknowledged, there is a notable absence of systematic exploration in the literature. This paper aims to fill this void by offering a detailed, step-by-step guide for systematically exploring the different instances of the meaningfulness gap and aids in navigating their complexities. More specifically, it proposes differentiating the gaps according to their realms and objects, normative nature, scope, and severity. To make these areas manageable, the paper takes several taxonomies and distinctions on board. Finally, the guide is summarized, and some skeptical replies are anticipated and countered by clarificatory remarks.","https://link.springer.com/content/pdf/10.1007/s00146-024-01993-1.pdf",""
0,"Vincent Bebien, Odile Bellenguez, Gilles Coppin, Anna Ma-Wyatt, Rachel Stephens","Ethical decision-making in human-automation collaboration: a case study of the nurse rostering problem",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00459-w","",746,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00459-w","2730-5953","",,,,,0,0.00,0,5,1,"Abstract: As artificial intelligence (AI) is increasingly present in different aspects of society and its harmful impacts are more visible, concrete methods to help design ethical AI systems and limit currently encountered risks must be developed. Taking the example of a well-known Operations Research problem, the Nurse Rostering Problem (NRP), this paper presents a way to help close the gap between abstract principles and on-the-ground applications with two different steps. We first propose a normative step that uses dedicated scientific knowledge to provide new rules for an NRP model, with the aim of improving nurses’ well-being. However, this step alone may be insufficient to comprehensively deal with all key ethical issues, particularly autonomy and explicability. Therefore, as a complementary second step, we introduce an interactive process that integrates a human decision-maker in the loop and allows practical ethics to be applied. Using input from stakeholders to enrich a mathematical model may help compensate for flaws in automated tools.","https://link.springer.com/content/pdf/10.1007/s43681-024-00459-w.pdf",""
0,"Kristina Fong","From Paper to Practice: Utilizing the ASEAN Guide on Artificial Intelligence (AI) Governance and Ethics",2024,"From Paper to Practice","ISEAS–Yusof Ishak Institute Singapore","https://doi.org/10.1355/9789815203684-002","",747,"2025-02-04 16:55:17","book-chapter","10.1355/9789815203684-002","","",,,,,0,0.00,0,1,1,"","https://www.degruyter.com/document/doi/10.1355/9789815203684-002/xml",""
0,"Masoud Ghalambor","Ethical Challenges in Applying New Technologies in Orthopedic Surgery",2022,"Future of Business and Finance","Springer International Publishing","https://doi.org/10.1007/978-3-030-99838-7_6","",752,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-030-99838-7_6","2662-2467","",,,107,118,0,0.00,0,1,3,"","https://link.springer.com/content/pdf/10.1007/978-3-030-99838-7_6",""
0,"Marina Kaneti","What can AI see? The image of the ‘migrant’ in the era of AI post-visualization",2023,"Journal of Global Ethics","Informa UK Limited","https://doi.org/10.1080/17449626.2023.2279229","",753,"2025-02-04 16:55:17","journal-article","10.1080/17449626.2023.2279229","1744-9626","",19,3,307,322,0,0.00,0,1,2,"","https://www.tandfonline.com/doi/pdf/10.1080/17449626.2023.2279229",""
0,"In Jae Lee","Research Ethics Issues and Researcher’s Responsibilities in AI Using Research",2024,"Journal of Moral &amp; Ethics Education","The Korean Society for Moral and Ethics Education","https://doi.org/10.18338/kojmee.2024..82.245","",756,"2025-02-04 16:55:17","journal-article","10.18338/kojmee.2024..82.245","1598-8708","",82,,245,265,0,0.00,0,1,1,"","",""
0,"Marc Steen, Joachim de Greeff, Maaike de Boer, Cor Veenman","Ethical aspects of ChatGPT: An approach to discuss and evaluate key requirements from different ethical perspectives",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00571-x","",757,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00571-x","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00571-x.pdf",""
0,"Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner","AI Hazard Management: A Framework for the Systematic Management of Root Causes for AI Risks",2024,"Frontiers of Artificial Intelligence, Ethics and Multidisciplinary Applications","Springer Nature Singapore","https://doi.org/10.1007/978-981-99-9836-4_27","",758,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-99-9836-4_27","2731-8125","",,,359,375,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/978-981-99-9836-4_27",""
0,"Joshua Smith","Faith, Technology, and the Ethics of AI",2024,"Handbook on the Ethics of Artificial Intelligence","Edward Elgar Publishing","https://doi.org/10.4337/9781803926728.00008","",760,"2025-02-04 16:55:17","book-chapter","10.4337/9781803926728.00008","","",,,37,48,0,0.00,0,1,1,"","https://www.elgaronline.com/view/book/9781803926728/9781803926728.xml",""
0,"","THE ROLE OF AI AND GENERATIVE AI IN MISINFORMATION AND DISINFORMATION",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.10","",763,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.10","","",,,75,109,0,0.00,0,0,1,"","",""
0,"","Ethics in AI: Bias, Fairness, and Accountability",2024,"resmilitaris","Institute for Advanced Studies","https://doi.org/10.48047/resmil.v10i1.9","",764,"2025-02-04 16:55:17","journal-article","10.48047/resmil.v10i1.9","","",10,1,,,0,0.00,0,0,1,"","",""
0,"","Title page",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00023-8","",765,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00023-8","","",,,,,0,0.00,0,0,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000238",""
0,"","Episode 20: Ethics of AI in Radiology and Medicine",2022,"Radiology: Artificial Intelligence","Radiological Society of North America (RSNA)","https://doi.org/10.1148/ryai.06172022.podcast","",766,"2025-02-04 16:55:17","dataset","10.1148/ryai.06172022.podcast","","",,,,,0,0.00,0,0,3,"","",""
0,"Rikard Rosenbacke, Åsa Melhus, Martin McKee, David Stuckler","AI and XAI second opinion: the danger of false confirmation in human–AI collaboration",2024,"Journal of Medical Ethics","BMJ","https://doi.org/10.1136/jme-2024-110074","",767,"2025-02-04 16:55:17","journal-article","10.1136/jme-2024-110074","0306-6800","",,,,,0,0.00,0,4,1,"Can AI substitute a human physician’s second opinion? Recently the","https://syndication.highwire.org/content/doi/10.1136/jme-2024-110074",""
0,"Kestutis Mosakas","Reflections on Killing Sophia by Thomas Telving",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00293-6","",768,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00293-6","2730-5953","",3,3,1025,1031,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00293-6.pdf",""
0,"Omer Nguena Timo, Tianqi Xiao, Florent Avellaneda, Yasir Malik, Stefan Bruda","Evaluating trustworthiness of decision tree learning algorithms based on equivalence checking",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00415-0","",771,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00415-0","2730-5953","",4,1,37,46,0,0.00,0,5,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00415-0.pdf",""
0,"Mariarosaria Taddeo, Alexander Blanchard, Christopher Thomas","From AI Ethics Principles to Practices: A Teleological Methodology to Apply AI Ethics Principles in The Defence Domain",2024,"Philosophy &amp; Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s13347-024-00710-6","",772,"2025-02-04 16:55:17","journal-article","10.1007/s13347-024-00710-6","2210-5433","",37,1,,,0,0.00,0,3,1,"Abstract: This article provides a methodology for the interpretation of AI ethics principles to specify ethical criteria for the development and deployment of AI systems in high-risk domains. The methodology consists of a three-step process deployed by an independent, multi-stakeholder ethics board to: (1) identify the appropriate level of abstraction for modelling the AI lifecycle; (2) interpret prescribed principles to extract specific requirements to be met at each step of the AI lifecycle; and (3) define the criteria to inform purpose- and context-specific balancing of the principles. The methodology presented in this article is designed to be agile, adaptable, and replicable, and when used as part of a pro-ethical institutional culture, will help to foster the ethical design, development, and deployment of AI systems. The application of the methodology is illustrated through reference to the UK Ministry of Defence AI ethics principles.","https://link.springer.com/content/pdf/10.1007/s13347-024-00710-6.pdf",""
0,"YouAI Han","AI in Healthcare Advancing Science and Ethics",2023,"","Center for Open Science","https://doi.org/10.31219/osf.io/zwtv6","",777,"2025-02-04 16:55:17","posted-content","10.31219/osf.io/zwtv6","","",,,,,0,0.00,0,1,2,"<p>AI in Healthcare Advancing Science and Ethics</p>","",""
0,"Thomas Søbirk Petersen, Kasper Lippert-Rasmussen","Need for speed? Why vehicles capable of driving faster than legal speed limits should be banned",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00290-9","",778,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00290-9","2730-5953","",4,2,529,536,0,0.00,0,2,2,"Abstract: Speeding is a major cause of avoidable deaths and serious injuries. In this article, we defend the view that, with few exceptions, vehicles should be required by law to have a limited intelligent speed assistant (LISA) fitted, making it impossible to exceed speed limits. Our core argument appeals to the four-element","https://link.springer.com/content/pdf/10.1007/s43681-023-00290-9.pdf",""
0,"Louis Chislett, Louis J. M. Aslett, Alisha R. Davies, Catalina A. Vallejos, James Liley","Ethical considerations of use of hold-out sets in clinical prediction model management",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00561-z","",779,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00561-z","2730-5953","",,,,,0,0.00,0,5,1,"Abstract: Clinical prediction models are statistical or machine learning models used to quantify the risk of a certain health outcome using patient data. These can then inform potential interventions on patients, causing an effect called performative prediction: predictions inform interventions which influence the outcome they were trying to predict, leading to a potential underestimation of risk in some patients if a model is updated on this data. One suggested resolution to this is the use of hold-out sets, in which a set of patients do not receive model derived risk scores, such that a model can be safely retrained. We present an overview of clinical and research ethics regarding potential implementation of hold-out sets for clinical prediction models in health settings. We focus on the ethical principles of beneficence, non-maleficence, autonomy and justice. We also discuss informed consent, clinical equipoise, and truth-telling. We present illustrative cases of potential hold-out set implementations and discuss statistical issues arising from different hold-out set sampling methods. We also discuss differences between hold-out sets and randomised control trials, in terms of ethics and statistical issues. Finally, we give practical recommendations for researchers interested in the use hold-out sets for clinical prediction models.","https://link.springer.com/content/pdf/10.1007/s43681-024-00561-z.pdf",""
0,"Arthur So","The implications of ethical perspectives in AI and autonomous systems",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00019-6","",780,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00019-6","","",,,135,152,0,0.00,0,1,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000196",""
0,"","WHAT’S NEXT IN AI?",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.19","",783,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.19","","",,,223,240,0,0.00,0,0,1,"","",""
0,"Bethany Joy Hills-Grois","Ethics and Regulation of AI in Precision Oncology",2024,"AI in Precision Oncology","Mary Ann Liebert Inc","https://doi.org/10.1089/aipo.2024.0040","",784,"2025-02-04 16:55:17","journal-article","10.1089/aipo.2024.0040","2993-091X","",1,5,244,245,0,0.00,0,1,1,"","https://www.liebertpub.com/doi/full-xml/10.1089/aipo.2024.0040",""
0,"","AI AND AUTONOMOUS THINGS",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.16","",789,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.16","","",,,177,189,0,0.00,0,0,1,"","",""
0,"","AI AND ONLINE DISINHIBITION",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.11","",790,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.11","","",,,110,118,0,0.00,0,0,1,"","",""
0,"Tyler Cook","A qualified defense of top-down approaches in machine ethics",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01820-z","",793,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01820-z","0951-5666","",,,,,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s00146-023-01820-z.pdf",""
0,"","AI AND JOB DISPLACEMENT",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.13","",794,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.13","","",,,133,145,0,0.00,0,0,1,"","",""
0,"Toju Duke","AI Ethics",2023,"Building Responsible AI Algorithms","Apress","https://doi.org/10.1007/978-1-4842-9306-5_10","",797,"2025-02-04 16:55:17","book-chapter","10.1007/978-1-4842-9306-5_10","","",,,137,147,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-1-4842-9306-5_10",""
0,"Ram Prasad Sonkar, Jay Singh","Contemporary Challenges to Maintain Ethics and Values in Organization",2023,"Ethics and Values in Organization: Contemporary Issues and Challenges","Anu Books, Meerut","https://doi.org/10.31995/book.ab274-m23.chapter5","",799,"2025-02-04 16:55:17","book-chapter","10.31995/book.ab274-m23.chapter5","","",,,66,75,0,0.00,0,2,2,"","",""
0,"","Societal Challenges",2024,"Regulating the Synthetic Society","Hart Publishing","https://doi.org/10.5040/9781509974979.ch-004","",800,"2025-02-04 16:55:17","other","10.5040/9781509974979.ch-004","","",,,59,90,0,0.00,0,0,1,"","",""
0,"Mudasir Ashraf","Generative AI: Challenges and the Road Ahead",2024,"International Journal of Science and Research (IJSR)","International Journal of Science and Research","https://doi.org/10.21275/sr241009154508","",801,"2025-02-04 16:55:17","journal-article","10.21275/sr241009154508","2319-7064","",13,10,716,725,0,0.00,0,1,1,"","",""
0,"Siraj Kariyilaparambu Kunjumuhammed","Adoption of Artificial Intelligence in Corporate Finance",2024,"Advances in Finance, Accounting, and Economics","IGI Global","https://doi.org/10.4018/979-8-3693-2185-0.ch001","",802,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-2185-0.ch001","2327-5677","",,,1,16,0,0.00,0,1,1,"The integration of Artificial Intelligence (AI) into corporate finance has brought transformative changes in decision making process, managing risks, and improving operational efficiency. Many tasks which were earlier driven by human intervention were automated using machine learning, natural language processing and other AI tools. The widespread integration, though intended to reduce human biases and bring greater fairness to decisions, is not without limitations. AI driven finance poses certain inherent risks resulting from biases in decision making that must be carefully addressed to effectively utilize AI tools and sustain it during complex decision making and risk management situations. The chapter is recommended for corporate finance professionals and researchers in the field to enhance awareness and contribute to the growing risk mitigation strategies in AI driven finance.","https://www.igi-global.com/viewtitle.aspx?TitleId=352609",""
0,"Aziz Mimoudi","AI, Personalized Education, and Challenges",2024,"International Conference on AI Research","Academic Conferences International Ltd","https://doi.org/10.34190/icair.4.1.3133","",806,"2025-02-04 16:55:17","journal-article","10.34190/icair.4.1.3133","3049-5628","",4,1,271,280,0,0.00,0,1,1,"Artificial Intelligence (AI) is gaining traction in education, with potential applications that range from personalized learning to automated administrative tasks. However, the integration of AI into educational systems is not without its challenges. This paper explores both the opportunities and obstacles that AI presents in the field of education, particularly through the use of Intelligent Tutoring Systems (ITS) and Adaptive Learning Management Systems (ALMS). These technologies aim to tailor learning experiences to individual students by analyzing data and adjusting content to suit their needs. While this personalized approach could enhance student engagement and comprehension, it relies on vast amounts of data, raising concerns about privacy and the potential misuse of personal information. Moreover, AI’s impact on education extends to supporting educators by providing insights into student performance and automating routine tasks. However, the effectiveness of AI systems in this regard remains questionable, particularly when considering the limitations in current AI models and the challenges of integrating them into existing educational frameworks. The risk of algorithmic bias is also a critical issue, as AI systems can inadvertently reinforce inequalities present in the data they are trained on, leading to unfair or discriminatory outcomes. Additionally, while AI promises to streamline certain aspects of education, there are concerns that over-reliance on technology could depersonalize the learning process. Human educators play a crucial role in fostering not only intellectual growth but also emotional and social development—elements that AI systems are currently unable to replicate. This paper argues that while AI holds significant promise in education, its deployment must be carefully managed, with attention to ethical considerations, equity in access, and the preservation of the human elements essential to effective learning. Addressing these challenges is key to ensuring that AI contributes meaningfully to education rather than exacerbating existing issues.","https://papers.academic-conferences.org/index.php/icair/article/download/3133/2918",""
0,"Nicole van Geel","EROS Ethical Robotic Systems. A Multi-level Framework for Integrating Ethics in Robotics and AI",2024,"Frontiers of Artificial Intelligence, Ethics and Multidisciplinary Applications","Springer Nature Singapore","https://doi.org/10.1007/978-981-99-9836-4_29","",807,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-99-9836-4_29","2731-8125","",,,391,405,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/978-981-99-9836-4_29",""
0,"Mirza Abdul Aleem Baig","The Future of AI in Higher Education: Opportunities &amp; Challenges Ahead",2024,"","Open Engineering Inc","https://doi.org/10.31224/3903","",810,"2025-02-04 16:55:17","posted-content","10.31224/3903","","",,,,,0,0.00,0,1,1,"The world has entered in an era of an unprecedented technological revolution. Artificial Intelligence (AI) is increasingly becoming more attainable and transformative force reshaping every sector and higher education is no exception. Progress in AI has generated new professions while de-skilling or re-skilling many others and its influence extends across diverse areas, including healthcare, economy, security, and notably, higher education. AI’s exceptional capacity to ingest and analyze extensive datasets, discern intricate patterns, and forecast outcomes underscores its potential to revolutionize conventional educational methodologies within the higher education sphere. Through an exploration of various AI applications such as personalized learning, student support services, predictive analytics, automated grading, content creation, and research assistance, the chapter highlights the potential for AI to revolutionize educational practices. By leveraging personalized learning algorithms and AI-driven tutoring systems, educational institutions can tailor learning experiences to individual student needs, enhancing engagement and comprehension. Predictive analytics enable early identification of at-risk students, facilitating targeted interventions to improve academic outcomes. In Pakistan context, where access to quality education is a pressing concern, AI can offer promising solutions to bridge existing gaps. However, the integration of AI in higher education is not without challenges. Ethical considerations, including data privacy and algorithmic bias, necessitate careful navigation to ensure equitable and transparent use of AI technologies. Through a balanced approach that prioritizes ethical guidelines and fosters collaboration between stakeholders, the evolving landscape of AI in higher education can be effectively navigated to optimize student success and institutional performance. Given this context, this chapter aims to respond to the following investigations: Q1: What are the key benefits of AI for the future of higher education? Q2: What are the key challenges of AI for the future of higher education?","",""
0,"Marc Steen, Joachim de Greeff, Maaike de Boer, Cor Veenman","Author Correction: Ethical aspects of ChatGPT: an approach to discuss and evaluate key requirements from different ethical perspectives",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00578-4","",811,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00578-4","2730-5953","",,,,,0,0.00,0,4,1,"","https://link.springer.com/content/pdf/10.1007/s43681-024-00578-4.pdf",""
0,"Ilaria Compagnoni","When iVR Meets AI: Practices and Challenges for Language Educators",2024,"Exploring AI in Applied Linguistics","Iowa State University Digital Press","https://doi.org/10.31274/isudp.2024.154.14","",813,"2025-02-04 16:55:17","book-chapter","10.31274/isudp.2024.154.14","","",,,243,261,0,0.00,0,1,1,"Increasing developments in educational technology have redefined language pedagogies through the use of digital tools enabling multi-user interactivity and content creation. Given the increasing use of AI in task-oriented language practices, there is a need to train language teachers to use AI in contextualized linguistic practices. These competencies can be supported by preparing language teachers to use AI with immersive Virtual Reality (iVR) for classroom-based language activities. However, the literature lacks inquiries on language activities grounded on AI and iVR-based collaborative group work. Attempting to bridge this gap, this study presents the results of interventions using the iVR platform Workrooms and ChatGPT conducted at the University of Arizona with language educators attending a professional development course on educational technologies. Data collected from observations of teachers’ activities and a post-activity questionnaire provides methodological suggestions for integrating VR and AI in language learning contexts. This contribution gives indications of the technological skill sets necessary to teach and learn languages with AI and iVR needed to socially and professionally interact in an increasingly digital world.","",""
0,"Pei-Yu Chen","AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents",2022,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3514094.3539531","",814,"2025-02-04 16:55:17","proceedings-article","10.1145/3514094.3539531","","",,,894,894,0,0.00,0,1,3,"","https://dl.acm.org/doi/pdf/10.1145/3514094.3539531",""
0,"Peter Smith Peter Smith, Laura Smith Peter Smith","Artificial Intelligence (AI) and Ethics",2024,"International Journal of Computer Auditing","Angle Publishing Co., Ltd.","https://doi.org/10.53106/256299802024120601006","",817,"2025-02-04 16:55:17","journal-article","10.53106/256299802024120601006","2562-9980","",6,1,69,76,0,0.00,0,2,1,"<p>AI is everywhere around us and affecting our lives in many ways, some of which we may not even be aware of. This paper discusses the advantages and disadvantages of AI and its impact upon society. The authors, Peter, and Laura are both disabled which means that, inevitably, they view AI and the ethical issues which it brings, through the lens of disabled people. However, they try as much as they can to discuss the ethical issues around AI in an unbiased manner. This commentary starts by introducing the concept of AI, the authors and their disabilities, the meanings of ethics and goes on to discuss the ethical issues which Peter and Laura see in their day-to-day lives. They then drawfromthe literature on AI and ethics to broaden the discussion to take account of current published work by others on the topic. Finally, they return to their own perspectives and conclude that AI offers many advantages to society and our future. However, they also warn of the dangers and ethical challenges which AI raises. It is hoped that this commentary frames a contribution to the field of AI and ethics which readers will find interesting, useful and, perhaps, challenging.</p> <p>&nbsp;</p>","",""
0,"Melanie Smallman","Multi Scale Ethics – Why we need a sociological approach to the ethics of AI in healthcare at different scales.",2022,"","Center for Open Science","https://doi.org/10.31235/osf.io/uj6my","",827,"2025-02-04 16:55:17","posted-content","10.31235/osf.io/uj6my","","",,,,,0,0.00,0,1,3,"<p>Around the world, we are seeing a significant growth in interest and investment in AI in healthcare. This has been coupled with rising concerns about the ethical implications of these technologies and an array of ethical guidelines for the use of AI and data in healthcare has arisen. Nevertheless, the question of if and how AI and data technologies can be ethical remains open to debate. This paper aims to contribute to this debate by considering the wide range of implications that have been attributed to these technologies and asking whether current ethical guidelines take these factors into account. In particular, the paper argues that current ethics guidelines for AI in healthcare effectively account for the four key issues identified in the ethics literature (transparency; fairness; responsibility and privacy), they have largely neglected wider issues relating to the way in which these technologies shape institutional and social arrangements. This, we argue, has given current ethics guidelines a strong focus on evaluating the impact of these technologies on the individual, while not accounting for the powerful social shaping effects of these technologies. To address this, the paper proposes a Multiscale Ethics Framework, which aims to help technology developers and ethical evaluations to consider the wider implications of these technologies.</p>","",""
0,"Jannik Zeiser","Owning Decisions: AI Decision-Support and the Attributability-Gap",2024,"Science and Engineering Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s11948-024-00485-1","",828,"2025-02-04 16:55:17","journal-article","10.1007/s11948-024-00485-1","1471-5546","",30,4,,,0,0.00,0,1,1,"Abstract: Artificial intelligence (AI) has long been recognised as a challenge to responsibility. Much of this discourse has been framed around robots, such as autonomous weapons or self-driving cars, where we arguably lack control over a machine’s behaviour and therefore struggle to identify an agent that can be held accountable. However, most of today’s AI is based on machine-learning technology that does not act on its own, but rather serves as a decision-support tool, automatically analysing data to help human agents make better decisions. I argue that decision-support tools pose a challenge to responsibility that goes beyond the familiar problem of finding someone to blame or punish for the behaviour of agent-like systems. Namely, they pose a problem for what we might call “","https://link.springer.com/content/pdf/10.1007/s11948-024-00485-1.pdf",""
0,"Suyesha Singh, Ruchi Joshi, Paridhi Jain, K. Abilash","Human AI: Ethics and broader impact for mental healthcare",2024,"Emotional AI and Human-AI Interactions in Social Networking","Elsevier","https://doi.org/10.1016/b978-0-443-19096-4.00005-5","",830,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-19096-4.00005-5","","",,,191,212,0,0.00,0,4,1,"","https://api.elsevier.com/content/article/PII:B9780443190964000055",""
0,"Enrico Panai","The latent space of data ethics",2023,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-023-01757-3","",833,"2025-02-04 16:55:17","journal-article","10.1007/s00146-023-01757-3","0951-5666","",39,6,2647,2665,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s00146-023-01757-3.pdf",""
0,"Andrew Edgar","Sport and AI",2023,"Sport, Ethics and Philosophy","Informa UK Limited","https://doi.org/10.1080/17511321.2023.2228118","",835,"2025-02-04 16:55:17","journal-article","10.1080/17511321.2023.2228118","1751-1321","",17,3,275,277,0,0.00,0,1,2,"","https://www.tandfonline.com/doi/pdf/10.1080/17511321.2023.2228118",""
0,"Kirsten Martin","Google Research: Who Is Responsible for Ethics of AI? *",2022,"Ethics of Data and Analytics","Auerbach Publications","https://doi.org/10.1201/9781003278290-64","",836,"2025-02-04 16:55:17","book-chapter","10.1201/9781003278290-64","","",,,434,446,0,0.00,0,1,3,"","",""
0,"","Incorporating Ethics into the AI Clinical Decision Support System Life Cycle",2024,"Encoding Bioethics","University of California Press","https://doi.org/10.2307/jj.14491770.11","",839,"2025-02-04 16:55:17","book-chapter","10.2307/jj.14491770.11","","",,,144,178,0,0.00,0,0,1,"","",""
0,"Anton Korinek","Economic Policy Challenges for the Age of AI",2024,"","National Bureau of Economic Research","https://doi.org/10.3386/w32980","",841,"2025-02-04 16:55:17","report","10.3386/w32980","","",,,,,0,0.00,0,1,1,"","",""
0,"","Challenges to U.S. National Security and Competitiveness Posed by AI",2023,"","RAND Corporation","https://doi.org/10.7249/cta2654-1","",842,"2025-02-04 16:55:17","edited-book","10.7249/cta2654-1","","",,,,,0,0.00,0,0,2,"","",""
0,"Kulbir Singh","AI and Healthcare: Opportunities and Challenges",2024,"","B P International","https://doi.org/10.9734/bpi/mono/978-81-972223-4-4","",843,"2025-02-04 16:55:17","monograph","10.9734/bpi/mono/978-81-972223-4-4","","",,,,,0,0.00,0,1,1,"","",""
0,"Oliver Bown","The challenges ahead for generative AI",2023,"","Monash University","https://doi.org/10.54377/aac6-c1f6","",845,"2025-02-04 16:55:17","report","10.54377/aac6-c1f6","","",,,,,0,0.00,0,1,2,"","",""
0,"Sábëlo Mhlambi, Simona Tiribelli","Correction: Decolonizing AI Ethics: Relational Autonomy as a Means to Counter AI Harms",2024,"Topoi","Springer Science and Business Media LLC","https://doi.org/10.1007/s11245-024-10078-z","",846,"2025-02-04 16:55:17","journal-article","10.1007/s11245-024-10078-z","0167-7411","",,,,,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/s11245-024-10078-z.pdf",""
0,"Pak-Hang Wong","Confucian ‘Trustworthy AI’: Diversifying a Keyword in the Ethics of AI and Governance",2023,"SSRN Electronic Journal","Elsevier BV","https://doi.org/10.2139/ssrn.4595723","",847,"2025-02-04 16:55:17","journal-article","10.2139/ssrn.4595723","1556-5068","",,,,,0,0.00,0,1,2,"","",""
0,"Herrick K. Lidstone","Navigating the Challenges of Generative AI",2024,"","Elsevier BV","https://doi.org/10.2139/ssrn.5004973","",849,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.5004973","","",,,,,0,0.00,0,1,1,"","",""
0,"","Chapter Six Incorporating Ethics into the AI Clinical Decision Support System Life Cycle",2024,"Encoding Bioethics","University of California Press","https://doi.org/10.1525/9780520397545-009","",850,"2025-02-04 16:55:17","book-chapter","10.1525/9780520397545-009","","",,,144,178,0,0.00,0,0,1,"","https://www.degruyter.com/document/doi/10.1525/9780520397545-009/xml",""
0,"Anna Schmitz","Towards formalizing and assessing AI fairness",2023,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society","ACM","https://doi.org/10.1145/3600211.3604762","",851,"2025-02-04 16:55:17","proceedings-article","10.1145/3600211.3604762","","",,,999,1001,0,0.00,0,1,2,"","https://dl.acm.org/doi/pdf/10.1145/3600211.3604762",""
0,"Kathryn Muyskens, Yonghui Ma, Michael Dunn","Can an AI-carebot be filial? Reflections from Confucian ethics",2024,"Nursing Ethics","SAGE Publications","https://doi.org/10.1177/09697330241238332","",854,"2025-02-04 16:55:17","journal-article","10.1177/09697330241238332","0969-7330","",31,6,999,1009,0,0.00,0,3,1,"This article discusses the application of artificially intelligent robots within eldercare and explores a series of ethical considerations, including the challenges that AI (Artificial Intelligence) technology poses to traditional Chinese Confucian filial piety. From the perspective of Confucian ethics, the paper argues that robots cannot adequately fulfill duties of care. Due to their detachment from personal relationships and interactions, the “emotions” of AI robots are merely performative reactions in different situations, rather than actual emotional abilities. No matter how “humanized” robots become, it is difficult to establish genuine empathy and a meaningful relationship with them for this reason. Even so, we acknowledge that AI robots are a significant tool in managing the demands of elder care and the growth of care poverty, and as such, we attempt to outline some parameters within which care robotics could be acceptable within a Confucian ethical system. Finally, the paper discusses the social impact and ethical considerations brought on by the interaction between humans and machines. It is observed that the relationship between humans and technology has always had both utopian and dystopian aspects, and robotic elder care is no exception. AI caregiver robots will likely become a part of elder care, and the transformation of these robots from “service providers” to “companions” seems inevitable. In light of this, the application of AI-augmented robotic elder care will also eventually change our understanding of interpersonal relationships and traditional requirements of filial piety.","https://journals.sagepub.com/doi/pdf/10.1177/09697330241238332",""
0,"Daniel Lim","AI Ethics",2023,"Philosophy through Computer Science","Routledge","https://doi.org/10.4324/9781003271284-17","",855,"2025-02-04 16:55:17","book-chapter","10.4324/9781003271284-17","","",,,237,251,0,0.00,0,1,2,"","",""
0,"Joost Mollen","LLMs beyond the lab: the ethics and epistemics of real-world AI research",2024,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-024-09819-w","",858,"2025-02-04 16:55:17","journal-article","10.1007/s10676-024-09819-w","1388-1957","",27,1,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s10676-024-09819-w.pdf",""
0,"","Leveraging AI and Technology to Address the Challenges of Underdeveloped Countries",2023,"Journal of Electrical Electronics Engineering","Opast Group LLC","https://doi.org/10.33140/jeee.02.03.04","",859,"2025-02-04 16:55:17","journal-article","10.33140/jeee.02.03.04","2834-4928","",2,3,,,0,0.00,0,0,2,"This article addresses the challenges of underdeveloped countries and discusses the potential of AI and technology to positively impact them. The article explores how these technologies can be used in areas such as healthcare, education, agriculture, and societal issues. The system presented in the article proposed an AI system architecture that can be used in future implementation. The article concludes by discussing the limitation of such A.I systems [1].","",""
0,"Daniel Barke","AI as a Driver of Hybrid Forms of Employment",2023,"Work and AI 2030","Springer Fachmedien Wiesbaden","https://doi.org/10.1007/978-3-658-40232-7_17","",860,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-658-40232-7_17","","",,,151,158,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-3-658-40232-7_17",""
0,"Bharati Ainapure","Computational-Based Edge AI Services and Challenges",2024,"Edge Computational Intelligence for AI-Enabled IoT Systems","CRC Press","https://doi.org/10.1201/9781032650722-4","",867,"2025-02-04 16:55:17","book-chapter","10.1201/9781032650722-4","","",,,46,66,0,0.00,0,1,1,"","",""
0,"Pranshu Lokhande","AI Content Filtering System Vulnerabilities: Consequences for Ethics and Safety",2024,"","Elsevier BV","https://doi.org/10.2139/ssrn.4971289","",868,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.4971289","","",,,,,0,0.00,0,1,1,"","",""
0,"Balkrishna Rasiklal Yadav","The Ethics of Understanding: Exploring Moral Implications of Explainable AI",2024,"International Journal of Science and Research (IJSR)","International Journal of Science and Research","https://doi.org/10.21275/sr24529122811","",869,"2025-02-04 16:55:17","journal-article","10.21275/sr24529122811","2319-7064","",13,6,1,7,0,0.00,0,1,1,"","",""
0,"Manas Sahoo","Ethics in AI – Critical Skills for the New World",2024,"ADIPEC","SPE","https://doi.org/10.2118/222249-ms","",870,"2025-02-04 16:55:17","proceedings-article","10.2118/222249-ms","","",,,,,0,0.00,0,1,1,"Artificial intelligence (AI) systems are increasingly being used in consumer as well and business applications. Companies are using them, Governments are using them, public security, medical researchers, you and I – everybody seems to be somehow touched. As it becomes more advanced and ubiquitous in our lives, it is critical that AI developers and users understand the ethical implications and challenges posed by this powerful technology. This paper explores the key ethical skills and considerations that are essential for navigating the new world of AI.","https://onepetro.org/SPEADIP/proceedings-pdf/doi/10.2118/222249-MS/3843148/spe-222249-ms.pdf",""
0,"Dan Hendrycks","Governance",2024,"Introduction to AI Safety, Ethics, and Society","CRC Press","https://doi.org/10.1201/9781003530336-8","",871,"2025-02-04 16:55:17","book-chapter","10.1201/9781003530336-8","","",,,446,494,0,0.00,0,1,1,"","",""
0,"Byeong Gi Lee","Challenges of Digital and AI Transformation",2024,"Understanding the Digital and AI Transformation","Springer Nature Singapore","https://doi.org/10.1007/978-981-96-0033-5_8","",872,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-96-0033-5_8","","",,,211,225,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/978-981-96-0033-5_8",""
0,"Jonas Bjerg","A Brief AI History",2024,"The Early-Career Professional’s Guide to Generative AI","Apress","https://doi.org/10.1007/979-8-8688-0456-4_2","",873,"2025-02-04 16:55:17","book-chapter","10.1007/979-8-8688-0456-4_2","","",,,13,19,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/979-8-8688-0456-4_2",""
0,"Lena Wrzesniowska","CAN AI MAKE A CASE?AI VS. LAWYER IN THE DUTCH LEGAL CONTEXT",2024,"International Journal of Law, Ethics, and Technology","La Nouvelle Jeunesse","https://doi.org/10.55574/erqq4677","",874,"2025-02-04 16:55:17","journal-article","10.55574/erqq4677","2769-7142","",2024,,,,0,0.00,0,1,1,"The integration of AI, specifically GPT-4, into the legal field is a subject of both potential promise and intricate challenges. This thesis delves into the transformational possibilities of AI within the Dutch legal context, examining not only the quality and persuasiveness of AI-generated legal argumentation but also its competence in information retrieval, as measured by models’ ability to spot relevant legal issues. An experiment was conducted with 25 legal professionals, using a real-world Dutch case with the purpose to assess GPT-4’s capabilities against that of a human lawyer. To enable GPT-4 to handle case documents, the author first performed so-called co-reference resolution to remove ambiguities. Given the token limitations of GPT-4, a so-called prompt reducer technique was used to compress the text, retaining essential information. The above methods produced a coherent and full case summary within GPT-4’s token constraints. This case summary, together with original lawyer’s letter (denoted as Text A) was fed to ChatGPT-4 to obtain its AI-written alternative (Text B). The study subjects were presented with a case summary as well as both texts and asked for their preferences. The outcome of the experiment is as follow: 80% of participants chose the AI’s composed legal document, demonstrating a strong preference for both its linguistic competencies as well its ability to spot relevant legal issues. This preference for GPT-4 writing is very consistent among genders, age groups and professionssurveyed. Contextualising these findings within the broader implications for legal practice, the thesis explores potential benefits including increased access to justice and transformation of certain legal procedures. Insights and recommendations are offered for legal professionals, considering the technological evolution and ethical considerations inherent in AI integration. Acknowledging the need for further exploration, the study recognises its own limitations and encourages replication to solidify the understanding of AI’s transformative role in the legal realm.","",""
0,"Stuart McLennan, Theresa Willem, Amelia Fiske","What the embedded ethics approach brings to AI-enhanced neuroscience",2024,"Developments in Neuroethics and Bioethics","Elsevier","https://doi.org/10.1016/bs.dnb.2024.02.010","",875,"2025-02-04 16:55:17","book-chapter","10.1016/bs.dnb.2024.02.010","2589-2959","",,,221,230,0,0.00,0,3,1,"","https://api.elsevier.com/content/article/PII:S2589295924000158",""
0,"Shahab D. Mohaghegh","Ethics of Artificial Intelligence (AI-Ethics) in Science and Engineering",2024,"Artificial Intelligence for Science and Engineering Applications","CRC Press","https://doi.org/10.1201/9781003369356-7","",878,"2025-02-04 16:55:17","book-chapter","10.1201/9781003369356-7","","",,,72,88,0,0.00,0,1,1,"","",""
0,"Jonas Bjerg","Navigating the AI Landscape",2024,"The Early-Career Professional’s Guide to Generative AI","Apress","https://doi.org/10.1007/979-8-8688-0456-4_7","",879,"2025-02-04 16:55:17","book-chapter","10.1007/979-8-8688-0456-4_7","","",,,105,113,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/979-8-8688-0456-4_7",""
0,"Sylvie Borau","Deception, Discrimination, and Objectification: Ethical Issues of Female AI Agents",2024,"Journal of Business Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s10551-024-05754-4","",880,"2025-02-04 16:55:17","journal-article","10.1007/s10551-024-05754-4","0167-4544","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s10551-024-05754-4.pdf",""
0,"Gloriana J. Monko, Mohamedi M. Mjahidi","From Bias to Balance: Navigating Gender Inclusion in AI",2024,"AI - Ethical and Legal Challenges [Working Title]","IntechOpen","https://doi.org/10.5772/intechopen.1007449","",881,"2025-02-04 16:55:17","book-chapter","10.5772/intechopen.1007449","","",,,,,0,0.00,0,2,1,"This chapter explores the intersection of Artificial Intelligence (AI) and gender, highlighting the potential of AI to revolutionize various sectors while also risking the perpetuation of existing gender biases. The focus is on the challenges and strategies for achieving gender inclusivity within AI systems. By examining the progress made by organizations in addressing gender bias, the chapter identifies key technical, ethical, legal, and social barriers and outlines approaches for integrating gender inclusivity throughout the AI lifecycle. Utilizing a narrative literature review supplemented by industry case studies, the chapter critically analyzes selected literature to address these issues. The findings underscore persistent challenges in identifying and mitigating gender bias in AI systems alongside complex ethical and legal implications. Nevertheless, notable advancements in gender-specific algorithm design and inclusive data practices are highlighted. The chapter concludes that achieving gender inclusivity in AI requires a coordinated effort across developers, researchers, and policymakers, offering actionable recommendations to ensure AI systems are fair, transparent, and equitable, thus contributing to a more just and inclusive society.","https://intech-files.s3.amazonaws.com/a043Y00000xzFKUQA2/a09Tc000000rc3xIAA/Final-From%20Bias%20to%20Balance%3A%20Navigating%20Gender%20Inclusion%20%20%282024-11-15%2019%3A22%3A54%29.pdf",""
0,"Willie Koen","Ethics of Transplantation",2022,"Challenges in Medical Ethics: the South African context","African Sun Media","https://doi.org/10.52779/9781991201959/05","",882,"2025-02-04 16:55:17","book-chapter","10.52779/9781991201959/05","","",,,79,97,0,0.00,0,1,3,"","",""
0,"Zhanxu Jiang","AI in Personalized Health Management: Practices and Challenges",2024,"Proceedings of the 1st International Conference on Engineering Management, Information Technology and Intelligence","SCITEPRESS - Science and Technology Publications","https://doi.org/10.5220/0012897900004508","",884,"2025-02-04 16:55:17","proceedings-article","10.5220/0012897900004508","","",,,46,51,0,0.00,0,1,1,"","",""
0,"","A DIGITAL WORLD AND ETHICS",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.7","",885,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.7","","",,,21,41,0,0.00,0,0,1,"","",""
0,"Lily Eva Frank, Michał Klincewicz","Uses and Abuses of AI Ethics",2024,"Handbook on the Ethics of Artificial Intelligence","Edward Elgar Publishing","https://doi.org/10.4337/9781803926728.00019","",887,"2025-02-04 16:55:17","book-chapter","10.4337/9781803926728.00019","","",,,205,217,0,0.00,0,2,1,"","https://www.elgaronline.com/view/book/9781803926728/9781803926728.xml",""
0,"Stefan Knupfer, Stefan Weigert","AI in the Health Market",2023,"Work and AI 2030","Springer Fachmedien Wiesbaden","https://doi.org/10.1007/978-3-658-40232-7_36","",888,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-658-40232-7_36","","",,,323,331,0,0.00,0,2,2,"","https://link.springer.com/content/pdf/10.1007/978-3-658-40232-7_36",""
0,"Huzeyfe Demirtas","AI responsibility gap: not new, inevitable, unproblematic",2024,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-024-09814-1","",890,"2025-02-04 16:55:17","journal-article","10.1007/s10676-024-09814-1","1388-1957","",27,1,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/s10676-024-09814-1.pdf",""
0,"Sherri Lynn Conklin, Gaurav Sett","ETHICS-2023 Session E4 - Tutorial: AI Safety, governance, and alignment tutorial",2023,"2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology (ETHICS)","IEEE","https://doi.org/10.1109/ethics57328.2023.10154970","",892,"2025-02-04 16:55:17","proceedings-article","10.1109/ethics57328.2023.10154970","","",,,1,2,0,0.00,0,2,2,"","http://xplorestaging.ieee.org/ielx7/10154894/10154907/10154970.pdf?arnumber=10154970",""
0,"María Bernardita Portales Velasco, Juan Pablo Beca Infante","Roles and Challenges for Clinical Ethics Committees and Clinical Ethics Consultation Systems",2023,"Bioethics","CRC Press","https://doi.org/10.1201/9781003269885-12","",894,"2025-02-04 16:55:17","book-chapter","10.1201/9781003269885-12","","",,,149,164,0,0.00,0,2,2,"","",""
0,"Zhiyi Liu, Yejie Zheng","AI Medical Treatment: Epidemic, Death and Love",2022,"AI Ethics and Governance","Springer Nature Singapore","https://doi.org/10.1007/978-981-19-2531-3_4","",895,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-19-2531-3_4","","",,,47,61,0,0.00,0,2,3,"","https://link.springer.com/content/pdf/10.1007/978-981-19-2531-3_4",""
0,"","Chapter 1: Focusing on AI Ethics and Social Impact",2024,"AI Horizons","De Gruyter","https://doi.org/10.1515/9781501518430-003","",896,"2025-02-04 16:55:17","book-chapter","10.1515/9781501518430-003","","",,,1,76,0,0.00,0,0,1,"","https://www.degruyter.com/document/doi/10.1515/9781501518430-003/xml",""
0,"Zhiyi Liu, Yejie Zheng","AI and Robot: Darwin and Rebellious Machine",2022,"AI Ethics and Governance","Springer Nature Singapore","https://doi.org/10.1007/978-981-19-2531-3_6","",897,"2025-02-04 16:55:17","book-chapter","10.1007/978-981-19-2531-3_6","","",,,79,93,0,0.00,0,2,3,"","https://link.springer.com/content/pdf/10.1007/978-981-19-2531-3_6",""
0,"Abigail Goldsteen, Ariel Farkash, Michael Hind","Assessing and implementing trustworthy AI across multiple dimensions",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00001-9","",898,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00001-9","","",,,229,257,0,0.00,0,3,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000019",""
0,"Susan Brokensha, Eduan Kotzé, Burgert A. Senekal","AI ethics in and for Africa",2023,"AI in and for Africa","Chapman and Hall/CRC","https://doi.org/10.1201/9781003276135-3","",901,"2025-02-04 16:55:17","book-chapter","10.1201/9781003276135-3","","",,,27,43,0,0.00,0,3,2,"","",""
0,"Lode Lauwaert, Bartek Chomanski","Ethics of AI",2025,"We, robots","Springer Nature Switzerland","https://doi.org/10.1007/978-3-031-77174-3_2","",902,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-77174-3_2","","",,,39,80,0,0.00,0,2,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-77174-3_2",""
0,"","2024 – DIGITAL TECHNOLOGY DISRUPTION AND AI",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.8","",903,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.8","","",,,42,46,0,0.00,0,0,1,"","",""
0,"Ellen Hohma","On the elements and implications of accountability for AI providers",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00008","",904,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00008","","",,,13,36,0,0.00,0,1,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Abigail Goldsteen, Ariel Farkash, Michael Hind","Assessing and implementing trustworthy AI across multiple dimensions",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00001-9","",905,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00001-9","","",,,229,257,0,0.00,0,3,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000019",""
0,"Ajaya Adhikari, Steven Vethman, Daan Vos, Marc Lenz, Ioana Cocu, Ioannis Tolios, Cor J. Veenman","Gender mobility in the labor market with skills-based matching models",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00410-5","",912,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00410-5","2730-5953","",4,1,163,167,0,0.00,0,7,1,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00410-5.pdf",""
0,"Heather Richards-Rissetto","Technological Challenges to Practicing 3D Ethics in Archaeology",2022,"Digital Heritage and Archaeology in Practice","University Press of Florida","https://doi.org/10.5744/florida/9780813069302.003.0009","",913,"2025-02-04 16:55:17","book-chapter","10.5744/florida/9780813069302.003.0009","","",,,163,193,0,0.00,0,1,3,"<p>Digital technologies require that we transform archaeological practice, and 3D technologies bring specific conundrums of data access and representation. Archaeologists can now collect an overwhelming number of x, y, z data points from the scale of objects to vast landscapes. But to what extent are these high resolution 3D data actually accessible, who can actually access them, and who should make decisions about their accessibility? Open data is essential to data sustainability and avoiding digital colonialism; however, there are technological and ethical challenges to sharing 3D archaeological data. While 3D raises many ethical questions related to practicing digital archaeology, the scope of this chapter is 3D ethics that primarily stem from technological affordances. It addresses three key questions: (1) How do we make archaeological 3D data accessible to wide and diverse (including the Global South) audiences; (2) What 3D data and versions of data (i.e., representations) can we ethically make accessible? Who gets to decide, and; (3) How can we represent uncertainty and interpretation in our 3D models/depictions? While technology offers some potential solutions to these challenges, it is essential that we focus on human-centered solutions that foster dialog among diverse communities. </p>","",""
0,"Michail Ploumis","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/7yiu7z","",914,"2025-02-04 16:55:17","peer-review","10.32388/7yiu7z","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/7YIU7Z/pdf",""
0,"David Banks","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/5o01yo","",915,"2025-02-04 16:55:17","peer-review","10.32388/5o01yo","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/5O01YO/pdf",""
0,"Jing Zhao","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/ty7ge3","",916,"2025-02-04 16:55:17","peer-review","10.32388/ty7ge3","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/TY7GE3/pdf",""
0,"Hironori Washizaki, Nobukazu Yoshioka","AI Security Continuum: Concept and Challenges",2024,"Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI","ACM","https://doi.org/10.1145/3644815.3644983","",918,"2025-02-04 16:55:17","proceedings-article","10.1145/3644815.3644983","","",,,269,270,0,0.00,0,2,1,"","https://dl.acm.org/doi/pdf/10.1145/3644815.3644983",""
0,"Jonas Bjerg","The Unexpected Evolution of AI",2024,"The Early-Career Professional’s Guide to Generative AI","Apress","https://doi.org/10.1007/979-8-8688-0456-4_4","",919,"2025-02-04 16:55:17","book-chapter","10.1007/979-8-8688-0456-4_4","","",,,57,74,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/979-8-8688-0456-4_4",""
0,"Rahulrajan Karthikeyan, Chieh Yi, Moses Boudourides","Criminal Justice in the Age of AI: Addressing Bias in Predictive Algorithms Used by Courts",2024,"The Ethics Gap in the Engineering of the Future","Emerald Publishing Limited","https://doi.org/10.1108/978-1-83797-635-520241003","",920,"2025-02-04 16:55:17","book-chapter","10.1108/978-1-83797-635-520241003","","",,,27,50,0,0.00,0,3,1,"","https://www.emerald.com/insight/content/doi/10.1108/978-1-83797-635-520241003/full/html",""
0,"Bor Luen Tang","Written by AI, reviewed by AI, and published by AI - the human editor as the ultimate gatekeeper in publication ethics",2024,"European Science Editing","Pensoft Publishers","https://doi.org/10.3897/ese.2024.e132192","",921,"2025-02-04 16:55:17","journal-article","10.3897/ese.2024.e132192","2518-3354","",50,,,,0,0.00,0,1,1,"An exercise in AI driven publishing","https://ese.arphahub.com/article/132192/download/pdf/",""
0,"Ambika N.","An Augmented Edge Architecture for AI-IoT Services Deployment in the Modern Era",2022,"Advances in Information Security, Privacy, and Ethics","IGI Global","https://doi.org/10.4018/978-1-6684-5250-9.ch015","",922,"2025-02-04 16:55:17","book-chapter","10.4018/978-1-6684-5250-9.ch015","1948-9730","",,,286,302,0,0.00,0,1,3,"The previous proposal gains prognostic and regulatory examination. It uses boundary-based AI procedures to accomplish its task. It analyzes its received transmission utilizing a set of amenities. It verifies the data packets and detects the inconsistency in them. It also encompasses choosing the appropriate procedure to evaluate the data stored in the cloud. Kubernetes cases plan handles Docker similes vigorously. The dominant point has a trustable and stable credential supply. The system aims to manage the information of various groups. The leading device has a control component that aims to supervise the well-being of the other instruments. Replica set maintains anticipated mock-up count. The endpoints component seeks to spot and watch the modifications to the approaches in the service. The proposal suggests increasing the reliability by 4.37%, availability by 2.74%, and speed by 3.28%.","https://www.igi-global.com/viewtitle.aspx?TitleId=312427",""
0,"Francesca Mazzi","Concerted Actions to Integrate Corporate Social Responsibility with AI in Business: Two Recommendations on Leadership and Public Policy",2023,"CSR, Sustainability, Ethics &amp; Governance","Springer International Publishing","https://doi.org/10.1007/978-3-031-09245-9_13","",923,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-09245-9_13","2196-7075","",,,251,266,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-3-031-09245-9_13",""
0,"","Tweetorial: FDA Review of Radiologic AI Algorithms: Process and Challenges",2024,"Radiology","Radiological Society of North America (RSNA)","https://doi.org/10.1148/radiol.230242.tweetorial","",924,"2025-02-04 16:55:17","dataset","10.1148/radiol.230242.tweetorial","","",,,,,0,0.00,0,0,1,"","",""
0,"Fiore Fontanarosa","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/9gg6ca","",925,"2025-02-04 16:55:17","peer-review","10.32388/9gg6ca","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/9GG6CA/pdf",""
0,"Jian Guan","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/tz2sfz","",926,"2025-02-04 16:55:17","peer-review","10.32388/tz2sfz","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/TZ2SFZ/pdf",""
0,"Ankit Kumar Jain, Pooja Kumari, Ritesh Gupta","Intrusion Detection and Analysis in IoT Devices Using Machine Learning Models",2024,"Advances in Computational Intelligence and Robotics","IGI Global","https://doi.org/10.4018/979-8-3693-3860-5.ch012","",928,"2025-02-04 16:55:17","book-chapter","10.4018/979-8-3693-3860-5.ch012","2327-0411","",,,384,409,0,0.00,0,3,1,"This study proposes utilizing deep learning and machine learning techniques to identify network anomalies. The IoT-23 dataset serves as the basis for the analysis. The proposed approach models are designed to classify network flows as benign or assign them to one of the 11 labels in the dataset, as well as to differentiate between malicious and benign connections. Performance and time costs of various models are compared to determine the optimal algorithm for maximum performance in minimal time. This comparison identifies the better performing model with the least overhead cost for deployment on IoT devices, ensuring the security and privacy of users by blocking malicious connections. The experimental results show that decision tree offers maximum efficiency and the lowest overhead cost, making it suitable for use in IoT devices.","https://www.igi-global.com/viewtitle.aspx?TitleId=354402",""
0,"Ahmad Alzahrani, Ying Zheng","Exploring AI Applications in Essay-Based Assignments: Affordances and Risks",2024,"AI - Ethical and Legal Challenges [Working Title]","IntechOpen","https://doi.org/10.5772/intechopen.1008230","",929,"2025-02-04 16:55:17","book-chapter","10.5772/intechopen.1008230","","",,,,,0,0.00,0,2,1,"This study examined the feasibility of employing artificial intelligence (AI) for feedback provision on essay-based assignments in a UK Higher Education setting. Although the critical role of feedback in enhancing students’ learning experiences is widely recognised, resource limitations and large student numbers often hinder its quality and timely delivery. Through in-depth interviews with four participants from a university in the UK, this research investigated AI applications in essay evaluation, utilising data from 12 AI-generated essays and their corresponding feedback. The aims of the study are to evaluate tutors’ abilities in discerning human and AI-generated essays, as well as evaluating the quality of AI-generated feedback from their perspectives. Findings showed that assessors could detect certain characteristics consistent with AI generation and noted ethical concerns regarding deviations from academic standards. Participants also acknowledged AI’s capacity for swift feedback delivery as compared to human. The results of this study help enhance our understanding of AI’s affordances and risks in assessment and feedback, particularly in the less explored university essay assignments.","https://intech-files.s3.amazonaws.com/a043Y00000xzFKUQA2/a09Tc0000010ARpIAM/Final-Exploring%20AI%20application%20in%20essay-based%20assignment%20%282024-12-04%2012%3A21%3A32%29.pdf",""
0,"","Generative AI In Health Care: Opportunities, Challenges, And Policy",2024,"Forefront Group","Health Affairs (Project Hope)","https://doi.org/10.1377/forefront.20231229.503206","",930,"2025-02-04 16:55:17","dataset","10.1377/forefront.20231229.503206","","",,,,,0,0.00,0,0,1,"","",""
0,"Jamal Barafi","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/ausych","",931,"2025-02-04 16:55:17","peer-review","10.32388/ausych","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/AUSYCH/pdf",""
0,"Ricard Martinez","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/lnb986","",932,"2025-02-04 16:55:17","peer-review","10.32388/lnb986","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/LNB986/pdf",""
0,"Emmanuel Lagarde","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/0hj4qz","",933,"2025-02-04 16:55:17","peer-review","10.32388/0hj4qz","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/0HJ4QZ/pdf",""
0,"Matt Baughman","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/jsmydw","",934,"2025-02-04 16:55:17","peer-review","10.32388/jsmydw","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/JSMYDW/pdf",""
0,"Daniel German","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/v50irr","",935,"2025-02-04 16:55:17","peer-review","10.32388/v50irr","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/V50IRR/pdf",""
0,"Jonas Bjerg","The Early-Career Professional’s Guide to Generative AI",2024,"","Apress","https://doi.org/10.1007/979-8-8688-0456-4","",936,"2025-02-04 16:55:17","book","10.1007/979-8-8688-0456-4","","",,,,,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/979-8-8688-0456-4.pdf",""
0,"Georgios Bouchagiar","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/fegn6z","",937,"2025-02-04 16:55:17","peer-review","10.32388/fegn6z","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/FEGN6Z/pdf",""
0,"Gaya Spolverato","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/iyinf6","",938,"2025-02-04 16:55:17","peer-review","10.32388/iyinf6","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/IYINF6/pdf",""
0,"Debaprasad Dutta","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/jhnxyn","",939,"2025-02-04 16:55:17","peer-review","10.32388/jhnxyn","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/JHNXYN/pdf",""
0,"Alexandra Bizoi","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/du08w3","",940,"2025-02-04 16:55:17","peer-review","10.32388/du08w3","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/DU08W3/pdf",""
0,"Loveleen Gaur","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/sobxbf","",941,"2025-02-04 16:55:17","peer-review","10.32388/sobxbf","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/SOBXBF/pdf",""
0,"Thomas Wingfield","Review of: ""AI in Court: Facing Today's Legal Challenges""",2024,"","Qeios Ltd","https://doi.org/10.32388/hw8fx1","",942,"2025-02-04 16:55:17","peer-review","10.32388/hw8fx1","","",,,,,0,0.00,0,1,1,"","https://www.qeios.com/read/HW8FX1/pdf",""
0,"","Using Citizen Focus Groups to Examine Attitudes Towards Emotional AI: Design, Diversity and Ethics",2023,"","SAGE Publications, Ltd.","https://doi.org/10.4135/9781529627848","",943,"2025-02-04 16:55:17","database","10.4135/9781529627848","","",,,,,0,0.00,0,0,2,"","",""
0,"Bhavin M Badiyani","Integration of AI in Accounting: Opportunities and Challenges",2023,"International Journal of Science and Research (IJSR)","International Journal of Science and Research","https://doi.org/10.21275/sr23727002822","",945,"2025-02-04 16:55:17","journal-article","10.21275/sr23727002822","2319-7064","",12,7,2026,2030,0,0.00,0,1,2,"","",""
0,"Edyta Bogucka, Marios Constantinides, Sanja Šćepanović, Daniele Quercia","Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts",2024,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i1.31627","",946,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i1.31627","3065-8365","",7,,168,180,0,0.00,0,4,1,"In the evolving landscape of AI regulation, it is crucial for companies to conduct impact assessments and document their compliance through comprehensive reports. However, current reports lack grounding in regulations and often focus on specific aspects like privacy in relation to AI systems, without addressing the real-world uses of these systems. Moreover, there is no systematic effort to design and evaluate these reports with both AI practitioners and AI compliance experts. To address this gap, we conducted an iterative co-design process with 14 AI practitioners and 6 AI compliance experts and proposed a template for impact assessment reports grounded in the EU AI Act, NIST's AI Risk Management Framework, and ISO 42001 AI Management System. We evaluated the template by producing an impact assessment report for an AI-based meeting companion at a major tech company. A user study with 8 AI practitioners from the same company and 5 AI compliance experts from industry and academia revealed that our template effectively provides necessary information for impact assessments and documents the broad impacts of AI systems. Participants envisioned using the template not only at the pre-deployment stage for compliance but also as a tool to guide the design stage of AI uses.","https://ojs.aaai.org/index.php/AIES/article/download/31627/33794",""
0,"Maximilian Kiener","Cyber-Risks and Medical Ethics",2024,"AI Morality","Oxford University PressOxford","https://doi.org/10.1093/oso/9780198876434.003.0003","",947,"2025-02-04 16:55:17","book-chapter","10.1093/oso/9780198876434.003.0003","","",,,18,29,0,0.00,0,1,1,"Abstract: With digital AI-powered medicine being on the rise, the risk of cyber-attacks has become a reality. ‘You have been hacked. Pay or we will shut down your system!’ AI systems are not just subject to cyber-risks like any other computer-based system, but to new risks and new types of attacks which have previously existed. This chapter argues that inevitably there will have to be a change in medical practice.","https://academic.oup.com/book/chapter-pdf/58408729/oso-9780198876434-chapter-3.pdf",""
0,"Liselotte Polderman","Governing (ir)responsibilities for future military AI systems",2023,"Ethics and Information Technology","Springer Science and Business Media LLC","https://doi.org/10.1007/s10676-023-09694-x","",948,"2025-02-04 16:55:17","journal-article","10.1007/s10676-023-09694-x","1388-1957","",25,1,,,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s10676-023-09694-x.pdf",""
0,"Vanessa Camilleri","Perspectives on the ethics of a VR-based empathy experience for educators",2024,"Ethics in Online AI-based Systems","Elsevier","https://doi.org/10.1016/b978-0-443-18851-0.00020-2","",951,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18851-0.00020-2","","",,,211,228,0,0.00,0,1,1,"","https://api.elsevier.com/content/article/PII:B9780443188510000202",""
0,"Giovanni Rubeis","Practices",2024,"The International Library of Ethics, Law and Technology","Springer International Publishing","https://doi.org/10.1007/978-3-031-55744-6_5","",953,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-55744-6_5","1875-0044","",,,91,149,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-55744-6_5",""
0,"Graham Dove","Session details: Design, Ethics and AI Explorations",2024,"Designing Interactive Systems Conference","ACM","https://doi.org/10.1145/3676231","",954,"2025-02-04 16:55:17","proceedings-article","10.1145/3676231","","",,,,,0,0.00,0,1,1,"","",""
0,"","Data Ethics",2023,"Data Quality","Wiley","https://doi.org/10.1002/9781394320547.ch12","",955,"2025-02-04 16:55:17","other","10.1002/9781394320547.ch12","","",,,223,236,0,0.00,0,0,2,"","https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781394320547.ch12",""
0,"Henrik Skaug Sætra","Descriptive, Normative, and Action AI ethics: The Case of X",2024,"","Elsevier BV","https://doi.org/10.2139/ssrn.4908338","",956,"2025-02-04 16:55:17","posted-content","10.2139/ssrn.4908338","","",,,,,0,0.00,0,1,1,"","",""
0,"Dan Hendrycks","Complex Systems",2024,"Introduction to AI Safety, Ethics, and Society","CRC Press","https://doi.org/10.1201/9781003530336-5","",957,"2025-02-04 16:55:17","book-chapter","10.1201/9781003530336-5","","",,,240,280,0,0.00,0,1,1,"","",""
0,"","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",2023,"","ACM","https://doi.org/10.1145/3600211","",959,"2025-02-04 16:55:17","proceedings","10.1145/3600211","","",,,,,0,0.00,0,0,2,"","https://dl.acm.org/doi/pdf/10.1145/3600211",""
0,"Dan Hendrycks","Safety Engineering",2024,"Introduction to AI Safety, Ethics, and Society","CRC Press","https://doi.org/10.1201/9781003530336-4","",960,"2025-02-04 16:55:17","book-chapter","10.1201/9781003530336-4","","",,,178,239,0,0.00,0,1,1,"","",""
0,"Dominique J. Monlezun","AI + health ethics",2023,"The Thinking Healthcare System","Elsevier","https://doi.org/10.1016/b978-0-443-18906-7.00004-0","",961,"2025-02-04 16:55:17","book-chapter","10.1016/b978-0-443-18906-7.00004-0","","",,,219,261,0,0.00,0,1,2,"","https://api.elsevier.com/content/article/PII:B9780443189067000040",""
0,"","WHAT THE ASEAN AI GUIDE GETS RIGHT",2024,"From Paper to Practice","ISEAS–Yusof Ishak Institute Singapore","https://doi.org/10.1355/9789815203684-006","",962,"2025-02-04 16:55:17","book-chapter","10.1355/9789815203684-006","","",,,6,10,0,0.00,0,0,1,"","https://www.degruyter.com/document/doi/10.1355/9789815203684-006/xml",""
0,"","AI REGULATION AND POLICY – 2024 AND BEYOND",2024,"Digital Ethics in the Age of AI","IT Governance Publishing","https://doi.org/10.2307/jj.22679824.18","",963,"2025-02-04 16:55:17","book-chapter","10.2307/jj.22679824.18","","",,,201,222,0,0.00,0,0,1,"","",""
0,"Prakhar Ganesh","Model Multiplicity for Responsible AI",2025,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i2.31896","",964,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i2.31896","3065-8365","",7,2,14,17,0,0.00,0,1,1,"Machine learning has experienced a remarkable rise, with highly sophisticated over-parameterized models leading the way. Consequently, these cutting-edge models find application across diverse domains. Their increasing deployment has sparked concerns about their real-world impact, studied under the umbrella of responsible AI. A crucial aspect of building responsible AI models is the idea of model multiplicity. If managed well, model multiplicity gives us the freedom to prioritize several metrics, including those associated with responsible AI, and select the best models to minimize harm. However, the existence of multiplicity also marks the unavoidable presence of arbitrariness in model selection that can impact individual-level decisions, necessitating a broader discussion on the role and expectations of AI decision makers in our society.","https://ojs.aaai.org/index.php/AIES/article/download/31896/34063",""
0,"Ida Skubis","The EU guidelines on ethics of robots and sex robots",2024,"The Elgar Companion to Applied AI Ethics","Edward Elgar Publishing","https://doi.org/10.4337/9781803928241.00023","",965,"2025-02-04 16:55:17","book-chapter","10.4337/9781803928241.00023","","",,,397,408,0,0.00,0,1,1,"","https://www.elgaronline.com/view/book/9781803928241/9781803928241.xml",""
0,"Joshua P. Davis","AI, Ethics, and Law",2022,"The Cambridge Handbook of Artificial Intelligence","Cambridge University Press","https://doi.org/10.1017/9781009072168.029","",966,"2025-02-04 16:55:17","book-chapter","10.1017/9781009072168.029","","",,,304,320,0,0.00,0,1,3,"","",""
0,"Giovanni Rubeis","Relationships",2024,"The International Library of Ethics, Law and Technology","Springer International Publishing","https://doi.org/10.1007/978-3-031-55744-6_6","",969,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-55744-6_6","1875-0044","",,,151,212,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-55744-6_6",""
0,"Giovanni Rubeis","Environments",2024,"The International Library of Ethics, Law and Technology","Springer International Publishing","https://doi.org/10.1007/978-3-031-55744-6_7","",970,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-031-55744-6_7","1875-0044","",,,213,245,0,0.00,0,1,1,"","https://link.springer.com/content/pdf/10.1007/978-3-031-55744-6_7",""
0,"Lisa Murphy","‘Designing’ Ethics into AI",2023,"AI in Clinical Medicine","Wiley","https://doi.org/10.1002/9781119790686.ch40","",971,"2025-02-04 16:55:17","other","10.1002/9781119790686.ch40","","",,,437,447,0,0.00,0,1,2,"","https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119790686.ch40",""
0,"Sebastián Lehuedé","An elemental ethics for artificial intelligence: water as resistance within AI’s value chain",2024,"AI &amp; SOCIETY","Springer Science and Business Media LLC","https://doi.org/10.1007/s00146-024-01922-2","",972,"2025-02-04 16:55:17","journal-article","10.1007/s00146-024-01922-2","0951-5666","",,,,,0,0.00,0,1,1,"Abstract: Research and activism have increasingly denounced the problematic environmental record of the infrastructure and value chain underpinning artificial intelligence (AI). Water-intensive data centres, polluting mineral extraction and e-waste dumping are incontrovertibly part of AI’s footprint. In this article, I turn to areas affected by AI-fuelled environmental harm and identify an ethics of resistance emerging from local activists, which I term ‘elemental ethics’. Elemental ethics interrogates the AI value chain’s problematic relationship with the elements that make up the world, critiques the undermining of local and ancestral approaches to nature and reveals the vital and quotidian harms engendered by so-called intelligent systems. While this ethics is emerging from grassroots and Indigenous groups, it echoes recent calls from environmental philosophy to reconnect with the environment via the elements. In empirical terms, this article looks at groups in Chile resisting a Google data centre project in Santiago and lithium extraction (used for rechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert. As I show, elemental ethics can complement top-down, utilitarian and quantitative approaches to AI ethics and sustainable AI as well as interrogate whose lived experience and well-being counts in debates on AI extinction.","https://link.springer.com/content/pdf/10.1007/s00146-024-01922-2.pdf",""
0,"Lukas Weidener, Michael Fischer","Role of Ethics in Developing AI-Based Applications in Medicine: Insights From Expert Interviews and Discussion of Implications (Preprint)",2023,"","JMIR Publications Inc.","https://doi.org/10.2196/preprints.51204","",973,"2025-02-04 16:55:17","posted-content","10.2196/preprints.51204","","",,,,,0,0.00,0,2,2,"<sec> <title>BACKGROUND</title> <p>The integration of artificial intelligence (AI)–based applications in the medical field has increased significantly, offering potential improvements in patient care and diagnostics. However, alongside these advancements, there is growing concern about ethical considerations, such as bias, informed consent, and trust in the development of these technologies.</p> </sec> <sec> <title>OBJECTIVE</title> <p>This study aims to assess the role of ethics in the development of AI-based applications in medicine. Furthermore, this study focuses on the potential consequences of neglecting ethical considerations in AI development, particularly their impact on patients and physicians.</p> </sec> <sec> <title>METHODS</title> <p>Qualitative content analysis was used to analyze the responses from expert interviews. Experts were selected based on their involvement in the research or practical development of AI-based applications in medicine for at least 5 years, leading to the inclusion of 7 experts in the study.</p> </sec> <sec> <title>RESULTS</title> <p>The analysis revealed 3 main categories and 7 subcategories reflecting a wide range of views on the role of ethics in AI development. This variance underscores the subjectivity and complexity of integrating ethics into the development of AI in medicine. Although some experts view ethics as fundamental, others prioritize performance and efficiency, with some perceiving ethics as potential obstacles to technological progress. This dichotomy of perspectives clearly emphasizes the subjectivity and complexity surrounding the role of ethics in AI development, reflecting the inherent multifaceted nature of this issue.</p> </sec> <sec> <title>CONCLUSIONS</title> <p>Despite the methodological limitations impacting the generalizability of the results, this study underscores the critical importance of consistent and integrated ethical considerations in AI development for medical applications. It advocates further research into effective strategies for ethical AI development, emphasizing the need for transparent and responsible practices, consideration of diverse data sources, physician training, and the establishment of comprehensive ethical and legal frameworks.</p> </sec>","",""
0,"Jonne Maas, Aarón Moreno Inglés","Beyond Participatory AI",2024,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","Association for the Advancement of Artificial Intelligence (AAAI)","https://doi.org/10.1609/aies.v7i1.31693","",974,"2025-02-04 16:55:17","journal-article","10.1609/aies.v7i1.31693","3065-8365","",7,,932,942,0,0.00,0,2,1,"The ‘participatory turn’ in AI design has received much attention in the literature. In this paper, we provide various arguments and proposals to move the discussion of participatory AI beyond its current state and towards stakeholder empowerment. The participatory AI literature points to Arnstein’s understanding of ‘citizen power’ as the right approach to participation. Although we agree with this general idea, we argue that there is a lack of depth in analyzing the legal, economic, and political arrangements required for a genuine redistribution of power to prioritize AI stakeholders. We highlight two domains on which the current discourse on participatory AI needs to expand. These are (1) the legal-institutional background that could provide ‘participation teeth’ for stakeholder empowerment and (2) the political economy of AI production that fosters such power asymmetries between AI developers and other stakeholders. We conclude by offering ways forward to explore alternative legal arrangements and ownership models for participatory AI.","https://ojs.aaai.org/index.php/AIES/article/download/31693/33860",""
0,"R Daren Erisman","AI for Pastors",2023,"The Promise and Peril of AI and IA","ATF Press","https://doi.org/10.2307/jj.24873339.24","",975,"2025-02-04 16:55:17","book-chapter","10.2307/jj.24873339.24","","",,,327,336,0,0.00,0,1,2,"","",""
0,"M. B. Unver, L. Roddeck","Ethics Governance of AI for the Legal Sector:",2024,"Journal of AI Law and Regulation","Lexxion Verlag","https://doi.org/10.21552/aire/2024/2/5","",976,"2025-02-04 16:55:17","journal-article","10.21552/aire/2024/2/5","2942-4380","",1,2,177,198,0,0.00,0,2,1,"","",""
0,"Fatemehsadat Hoseini","AI Ethics: A Call for Global Standards in Technology Development",2023,"AI and Tech in Behavioral and Social Sciences","KMAN Publication Incorporation","https://doi.org/10.61838/kman.aitech.1.4.1","",978,"2025-02-04 16:55:17","journal-article","10.61838/kman.aitech.1.4.1","","",1,4,1,3,0,0.00,0,1,2,"The development of global standards for ethical AI is not merely a technical issue but a moral imperative that demands immediate and concerted action from all stakeholders involved in AI development and deployment. To navigate the ethical complexities of AI and ensure its benefits are maximized while minimizing its risks, a comprehensive and inclusive approach to AI ethics is essential. This approach must prioritize transparency, accountability, safety, and fairness, and include diverse perspectives to create a truly global ethical framework for AI. As we stand at the crossroads of technological innovation and ethical responsibility, the call for global standards in AI development cannot be overstated. It is time for researchers, policymakers, industry leaders, and the global community to come together to forge a path that ensures AI technologies are developed and used in a manner that upholds the highest ethical standards, respects human rights, and promotes the well-being of society and the environment. In conclusion, the journey towards ethical AI is a collective endeavor that requires the wisdom, insight, and cooperation of the global community. By embracing the challenge of developing and implementing global ethical standards for AI, we can ensure that this transformative technology serves as a force for good, driving progress and innovation in ways that are responsible, ethical, and sustainable.","",""
0,"David Brenner","Foreword",2023,"The Promise and Peril of AI and IA","ATF Press","https://doi.org/10.2307/jj.24873339.3","",979,"2025-02-04 16:55:17","book-chapter","10.2307/jj.24873339.3","","",,,,,0,0.00,0,1,2,"","",""
0,"Martin Hollender, Chaojun Xu, Ruomu Tan","Engineering Challenges in Industrial AI",2024,"Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI","ACM","https://doi.org/10.1145/3644815.3644968","",981,"2025-02-04 16:55:17","proceedings-article","10.1145/3644815.3644968","","",,,41,42,0,0.00,0,3,1,"","https://dl.acm.org/doi/pdf/10.1145/3644815.3644968",""
0,"Peter Schlicht","AI in the Automotive Industry",2023,"Work and AI 2030","Springer Fachmedien Wiesbaden","https://doi.org/10.1007/978-3-658-40232-7_29","",982,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-658-40232-7_29","","",,,257,265,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/978-3-658-40232-7_29",""
0,"Thomas Hummel, Monika Rimmele","AI in the Clinical Treatment Path",2023,"Work and AI 2030","Springer Fachmedien Wiesbaden","https://doi.org/10.1007/978-3-658-40232-7_34","",984,"2025-02-04 16:55:17","book-chapter","10.1007/978-3-658-40232-7_34","","",,,305,312,0,0.00,0,2,2,"","https://link.springer.com/content/pdf/10.1007/978-3-658-40232-7_34",""
0,"Rajendraprasad Chittimalla","Challenges of Integrating AI into Managed File Transfer Solutions",2024,"International Journal of Science and Research (IJSR)","International Journal of Science and Research","https://doi.org/10.21275/sr24802041810","",985,"2025-02-04 16:55:17","journal-article","10.21275/sr24802041810","2319-7064","",13,8,137,140,0,0.00,0,1,1,"","",""
0,"Cheng Xu, Yanqi Sun, Haibo Zhou","Artificial Aesthetics and Ethical Ambiguity: Exploring Business Ethics in the Context of AI-driven Creativity",2024,"Journal of Business Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s10551-024-05837-2","",986,"2025-02-04 16:55:17","journal-article","10.1007/s10551-024-05837-2","0167-4544","",,,,,0,0.00,0,3,1,"","https://link.springer.com/content/pdf/10.1007/s10551-024-05837-2.pdf",""
0,"Eugène Loos, Jan Radicke","Using ChatGPT-3 as a writing tool: an educational assistant or a moral hazard? Current ChatGPT-3 media representations compared to Plato’s critical stance on writing in Phaedrus",2024,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-024-00470-1","",989,"2025-02-04 16:55:17","journal-article","10.1007/s43681-024-00470-1","2730-5953","",,,,,0,0.00,0,2,1,"Abstract: ChatGPT-3, based on a large language model created by OpenAI, capable of generating human-like text, has been open to the public since November 2022. Since 2023, ChatGPT-3 has become a much-discussed educational writing tool. We elaborate on what we mean by referring to ChatGPT-3 as an educational assistant and define moral hazard. Then, we put this writing tool, as an extension of human capabilities, in a historical perspective with an analysis of Plato’s critical stance on writing in","https://link.springer.com/content/pdf/10.1007/s43681-024-00470-1.pdf",""
0,"YoungHwan Kim, Kyeonglim Hwang","Analysis of Korean AI Ethics Research: A Focus on Network Text Analysis",2024,"Journal of Digital Contents Society","Digital Contents Society","https://doi.org/10.9728/dcs.2024.25.4.981","",991,"2025-02-04 16:55:17","journal-article","10.9728/dcs.2024.25.4.981","1598-2009","",25,4,981,990,0,0.00,0,2,1,"","",""
0,"","AI job postings mentioning keywords related to ethics, 2019-22",2023,"OECD Skills Outlook","OECD","https://doi.org/10.1787/c8d5f900-en","",993,"2025-02-04 16:55:17","other","10.1787/c8d5f900-en","2521-1072","",,,,,0,0.00,0,0,2,"","",""
0,"","How Does AI Pose Challenges for Leaders in Organizations? -A Conceptual Study",2023,"Journal of Educational &amp; Psychological Research","Opast Group LLC","https://doi.org/10.33140/jepr.05.03.03","",994,"2025-02-04 16:55:17","journal-article","10.33140/jepr.05.03.03","2690-0726","",5,3,,,0,0.00,0,0,2,"Artificial Intelligence (AI) and its application are becoming ubiquitous. This drastic change creates a plethora of challenges for leaders and leads to a need for more holistic leadership skills. This article aims to give an overview of the challenges of AI for leaders and how leaders or managers can address these challenges in more effective ways. A literature review was conducted. To know the challenges of leaders, an AI technologies practitioner was interviewed working in one of the top software companies. In this article, I discussed various ways in which these challenges can be addressed for leaders while working with AI. To conclude recommendations and strategies are suggested so that AI leaders can enable innovation and collaboration between humans and AI, and change the workforce with a new set of skills that are unique to humans. Also, a few guiding principles may help business leaders sail across the changes in the era of AI, and future implications are discussed.","",""
0,"Laura Arbelaez Ossa, Stephen R. Milford, Michael Rost, Anja K. Leist, David M. Shaw, Bernice S. Elger","AI Through Ethical Lenses: A Discourse Analysis of Guidelines for AI in Healthcare",2024,"Science and Engineering Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s11948-024-00486-0","",996,"2025-02-04 16:55:17","journal-article","10.1007/s11948-024-00486-0","1471-5546","",30,3,,,0,0.00,0,6,1,"Abstract: While the technologies that enable Artificial Intelligence (AI) continue to advance rapidly, there are increasing promises regarding AI’s beneficial outputs and concerns about the challenges of human–computer interaction in healthcare. To address these concerns, institutions have increasingly resorted to publishing AI guidelines for healthcare, aiming to align AI with ethical practices. However, guidelines as a form of written language can be analyzed to recognize the reciprocal links between its textual communication and underlying societal ideas. From this perspective, we conducted a discourse analysis to understand how these guidelines construct, articulate, and frame ethics for AI in healthcare. We included eight guidelines and identified three prevalent and interwoven discourses: (1) AI is unavoidable and desirable; (2) AI needs to be guided with (some forms of) principles (3) trust in AI is instrumental and primary. These discourses signal an over-spillage of technical ideals to AI ethics, such as over-optimism and resulting hyper-criticism. This research provides insights into the underlying ideas present in AI guidelines and how guidelines influence the practice and alignment of AI with ethical, legal, and societal values expected to shape AI in healthcare.","https://link.springer.com/content/pdf/10.1007/s11948-024-00486-0.pdf",""
0,"Kareem Othman","Correction to: Understanding how moral decisions are affected by accidents of autonomous vehicles, prior knowledge, and perspective-taking: a continental analysis of a global survey",2023,"AI and Ethics","Springer Science and Business Media LLC","https://doi.org/10.1007/s43681-023-00346-w","",1000,"2025-02-04 16:55:17","journal-article","10.1007/s43681-023-00346-w","2730-5953","",4,4,1491,1491,0,0.00,0,1,2,"","https://link.springer.com/content/pdf/10.1007/s43681-023-00346-w.pdf",""
